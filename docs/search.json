[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rによるデータ解析のための前処理",
    "section": "",
    "text": "Warning\n\n\n\n本リポジトリで扱っているRコードは全般的に古い状態にあります。 具体的には、おおよそ2017年時点で書かれたもののため、dplyrやtidyrなどの一部の関数で 現在推奨されていない関数や処理をそのまま使っている箇所があります。 これらの箇所については、順次最新の書き方に修正していきますが、その間、古いコードが混ざります。"
  },
  {
    "objectID": "index.html#前処理とr",
    "href": "index.html#前処理とr",
    "title": "Rによるデータ解析のための前処理",
    "section": "前処理とR",
    "text": "前処理とR\n\nコンピュータサイエンスの分野に「Garbage in, garbage out(ガラクタを入れればガラクタが出てくる)」という格言がある。これは入力に用いるデータの質が悪い場合に、得られる結果の価値も低くなることを述べている。この言葉は、コンピュータサイエンスだけでなくデータ分析でも言える事柄である。例えばデータの中には入力の誤りや重複が含まれることがある。これらは非意図的に記録された値であり、存在に気づかずに分析を行ってしまうのは危険である。また、記録から漏れた欠損値や他の値と著しく異なる外れ値が含まれる場合、分析結果はこれらの値の影響を受ける。こうしたデータは「ガラクタ」であり、価値の低い結果を生み出す原因となる。こうしたデータは、分析目的やデータ全体を見ながら適切に片付ける必要がある。\nデータに含まれる「ガラクタ」を処理するとともに、データそのものを綺麗に整える必要がある場合もある。コンピュータの処理は予測される入力値が与えられることにより処理が実行される（簡単な算術演算を行う時に、文字と数値が混在していては出力がエラーになる。予測された入力を元に正しい出力を行う）ため、データは目的の処理を行うために適した形でなければならない。これはデータ分析を行う上で最低限クリアしていなければならない課題である。このような一連の作業は「前処理」や「データクレンジング」と呼ばれ、データ分析を行う際には、大きな役割を担っている。\n高価な家具が良質な木材と丁寧な処理によって出来上がるように、データ分析では前処理を通して可能な限り素材となるデータの質を高める必要がある。一方、前処理では不適切なデータが生み出された原因の追求や、データに対して繰り返しの修正を加えるため、本来の目的である解析作業より手間と時間がかかりやすい。したがって、前処理の手順を確立することは作業の効率化につながり、良い分析成果を得ることに繋がるだろう。\nデータ解析環境であるRは、かつては主に解析やデータのグラフ化に利用され、前処理の作業はAwkなどのコマンドラインツールやその他のプログラミング言語に委ねられることが多かった。またRでデータに対する処理を行なうとしても、apply()やtapply()といったapply族の関数が使い回されていたが、apply族の関数の利用方法にはやや癖があり、初心者には決して易しくなかった。\nこれに対し、Rでも2012年以降 plyr、reshape2、tidyr といった直感的に使いやすく、データ処理に必要な関数を備えたパッケージが台頭してきた。2014年にはplyrの後継であるdplyrパッケージが開発された。dplyrパッケージでは、統一的なコードで記述できるようになっており、より柔軟なデータ操作が可能になった。またdplyrでは、リレーショナルデータベースを扱う関数が用意されており、データの取得から操作までを通して実行する環境が整備されている。このため現在ではdplyrの関数を前処理や分析結果の整理に用いる方法が広まっている。そして2016年、Hadley Wichkamにより、データ取得およびデータ操作、可視化や統計モデリングの処理を調和的に利用可能にするtidyverseの概念が提唱された。ここにきてRによる前処理から分析結果の実行までの流れは成熟の兆しをみせている。\n\n\n\n\n\nRにおける前処理で利用されるパッケージや概念の変遷\n\n\n一方、利用するデータを正確にRで扱えるようにするというのも前処理に欠かせない作業である。これまでデータ分析者はCSVやExcelといったファイルやリレーショナルデータベースのデータ、すなわち表形式のデータを主に扱ってきた。しかし今ではXMLやJSONといった階層構造をもつデータ形式や、NoSQLと呼ばれるデータベースの普及により、これらのデータソースからも柔軟にデータを取得することが求められている。加えて、インターネットを利用して提供されるウェブサイトの情報やウェブAPIを使ってデータを収集する機会も増えている。オープンソースであるRでは、こうしたデータソースの変化にも対応したパッケージの開発が盛んに行われている。Rは高機能なデータ開発環境としてますます多くのユーザーを惹き付けている。\n\n本書では「前処理のためのデータの収集と前処理」をモダンなRの手法で実現する方法を解説する。データの収集と前処理に共通してRを用いることには2つの利点があると筆者は考える。まず分析基盤の統合とRに対する熟練度の向上である。データ解析はRで、前処理は別のツールで行う場合、異なるツールを利用するためにどうしても思考プロセスを切り替える必要が生じてくる。データの収集と前処理がRで実行可能になれば、こうした思考の分断を防ぎ、作業に集中できるだろう。つまり、データの収集と前処理、そして解析をシームレスに実行できるということである。本書を通じて、データ分析に伴なう前処理の負担が少しでも軽減されれば筆者の幸いとするところである。"
  },
  {
    "objectID": "index.html#本書について",
    "href": "index.html#本書について",
    "title": "Rによるデータ解析のための前処理",
    "section": "本書について",
    "text": "本書について\n本書の構成\n本書は、基礎編と実践編に分かれ、9の章から構成される。基礎編では、データ分析を行う上で留意すべき事項の確認から、Rへのデータの読み込み、基本となるデータ操作方法を学ぶ。1章は、Rにおけるデータの扱いの基礎や本書やRでのデータ分析には欠かせないデータフレームについて説明している。2章は、Rへデータを取り込む方法として、テキストファイル、Excelファイル、データベース、ウェブといった主要なデータ取得先を題材として紹介する。続く3章ではtidyverseの思想に沿ったモダンなデータ操作の導入を行う。この章は実践編の各章で重要な事項となる。\n実践編では基礎編の内容を踏まえた上で、前処理に必要な処理をデータ型ごとに章を分けて議論する。具体的には文字列と因子について4章および5章で取り上げる。また日付・時間データについて6章で、外れ値や欠損値の処理方法を7章で述べる。8章では、分析を行うのに不適切なデータを整然とした形に修正する方法とデータを自由自在に変形させる方法を学ぶ。続く9章は、これまで扱ってきた内容を振り返りながら、いくつかの例題を提示していく。これらを踏まえ、最終章では実行したデータ操作の再現性確保に必要なデータ管理について議論する。各章は、データ分析で必要となる前処理の時系列を例に沿って構成されており、その意味で、章の順番通りに読み進めることで、データ分析の手順について理解を深めることができるであろう。ただし、各章の内容的にはそれぞれ完結しており、必要な章だけを重点的に読むことも可能なように書かれている。\n\n\n本書の構成\n\n\n\n本書に掲載されたRコードの実行環境は次のとおりである。\n\n\n\nR version 4.1.2 (2021-11-01)\nmacOS Monterey 12.3.1\n\n本書の表記法\n本書では、Rのパッケージや関数について、次のように表記している。パッケージはpackage、関数はfunction()としたが、パッケージ間の関数名の衝突を避ける必要がある場合は package::function() と表記することもある。引数は argument のように斜体としている。またRの関数では引数名を省略して実引数を渡すことが可能であるが、複数の引数を取る関数については読者の理解を助けるために引数名を明示した。論理値については TRUE / FALSE とし、省略形の T や F は使っていない。同様に空白値 NULLや欠損値 NAも斜体で表記する。データフレームやベクトルなどのオブジェクトについては、x のように太字で記し、またデータの 変数 は斜体としている。"
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  データを扱うための下準備",
    "section": "",
    "text": "この章は、Rプログラミングの基礎と、近年注目を集めているtidyverseの枠組みを紹介する。すでにRに慣れている読者であれば、本章前半のR基本操作は読み飛ばしても問題がない。一方、後半はtidyverseの導入として、tidyデータという概念を解説し、この枠組みでのデータ読み込み、データ操作、効率的な処理ついて触れる。加えて、tidyverseの根幹にあるtibbleと呼ばれる拡張クラスや、パイプ処理と呼ばれるコード記述について説明する。"
  },
  {
    "objectID": "ch1.html#rプログラミングの基礎",
    "href": "ch1.html#rプログラミングの基礎",
    "title": "1  データを扱うための下準備",
    "section": "1.1 Rプログラミングの基礎",
    "text": "1.1 Rプログラミングの基礎\nここではオブジェクトの扱い、特にデータ型ついて説明しよう。\n\n1.1.1 データ型: データの種類\nプログラミング言語では、データは型で区別され、型ごとに適切な処理が定義されている。Rで頻繁に利用されるデータ型として、論理値 (logical)、数値 (整数型 integerおよび 倍精度小数点型 double)、文字型 (character)の4種類がある。これらは他の要素に分解することのできない原始的な(atomic)データ型である。\n\n# c()はベクトルを生成する関数\n# 以下は3つの数値要素からなるベクトルを生成\nc(3, 1, 6)\n\n[1] 3 1 6\n\n\nベクトルはc()によって生成されるが、ベクトル内では、要素のデータ型は統一されるという制約がある。もしベクトル内部で複数のデータ型を混在させようとすると、Rはいずれかのデータ型に強制変換を行う。変換はもっとも自由度の高いデータ型で統一される。例えば数値と文字では文字のほうが自由度が高い。文字は数値型として扱えないが、数値の方は文字列として保存することが可能である（必要があればRではなどを使って数値に変換できる）。\n\nx <- c(123, \"十二三\")\nclass(x)\n\n[1] \"character\"\n\n\nここで class() はデータの型を出力する関数である。 データ型の変換はこのように、データ型の扱いやすさによって決定する。データ型の変換の優先順位は柔軟性の低い型から論理型、整数型、倍精度小数点型、文字列の順となる。\nRではオブジェクトに関数を適用する際に自動的に型変換が行われることがある。例えば論理値のTRUEおよびFALSEは、合計を求める関数を適用すると自動的に数値に変換されて処理される。\n\n1L + TRUE\n\n[1] 2\n\n# FALSEは数値の1以下である \n# (正しいのでTRUEが返却される)\nFALSE <= 1\n\n[1] TRUE\n\n# 論理値のTRUE, FALSEは数値として処理され、それぞれ1と0として計算される\nsum(c(TRUE, FALSE, 1, TRUE))\n\n[1] 3\n\n\n一方でas.numeric()やas.character()といった関数を実行することで、任意のタイミングでデータ型を変換をすることも可能である。これらの関数の多くは as.と変換後のデータ名をつなげた名前で定義されている。\n\n# 明示的に型変換を行う\nas.numeric(c(TRUE, FALSE))\n\n[1] 1 0\n\nas.character(as.numeric(TRUE))\n\n[1] \"1\"\n\n\n\n\n1.1.2 Rではすべてがオブジェクトである\nRでは文字や数値、関数など、すべてがオブジェクトである。10や”abc”という数値や文字（リテラルという）、関数c()もまたオブジェクトである。通常オブジェクトには名前がつけられる。cは要素を結合する関数オブジェクトに付けられた名前である。また以下の例ではオブジェクト（リテラル）である3と5を加算した結果のオブジェクトを x という名前で保存している。\n\n# 処理の結果を x として保存する\nx <- 3 + 5\n# 保存されたオブジェクトを出力する\nx\n\n[1] 8\n\n# オブジェクトに対して処理を加える\nx + 8\n\n[1] 16\n\n# xの値を上書きする\nx <- \"こんにちは\"\nx\n\n[1] \"こんにちは\"\n\n\nx はオブジェクトである 8 に付けられた名前であるが、簡単のため x もオブジェクトと呼ぶことが多い。ユーザが作成したオブジェクトは作業スペースに保存される。作業スペース中のオブジェクトの一覧は ls() で確認できる。\nオブジェクトに名前をつける操作を代入と呼ぶ。代入には <- を利用する。この記号は代入演算子と呼ばれる。オブジェクトには自由に名前をつけることができるが、数値で始まる名前をつけることはできない(1x などは不可）。また予約語と呼ばれる、R内部で利用されるオブジェクト名を使うことはできない。既存のオブジェクトに代入を行うとその中身は上書される。\n\n\n1.1.3 クラス\n数値や文字といった基本的(atomic)なデータ型とは別に、日付や時間のような複雑な情報を表現するデータもある。これらはRでクラスとして定義されている。 クラスは基本データ型と並んでRを扱う上では大事な概念であるので、ここで簡単に説明しておこう。以下では現在の時間を表すオブジェクトを生成している。\n\ny <- Sys.time()\nclass(y)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nRのオブジェクトの方を確認するには class()を使う。Sys.time()で生成されたオブジェクト yに適用してみると POSIXct, POSIXt と出力される。これはオブジェクトy がクラスPOSIXctのインスタンス（クラスを実体化したオブジェクトのことをインスタンスと表現することもある）であり、そして POSIXct クラスは POSIXt という親クラスとして継承している（共通する特性の引き継いでいる）ことを意味している。本項ではクラスの継承関係を説明する余裕はないので、興味ある読者は〇〇を参照されたいが、ここでyを実行してみると以下の出力を得られる。\n\ny \n\n[1] \"2022-05-02 16:49:29 JST\"\n\n\n出力では日付と時刻が年-月-日 時:分:秒 タイムゾーンの形式で表示される。しかしながらyの実態は文字列ではない。yからクラス属性を外すと以下のように出力される。\n\nunclass(y)\n\n[1] 1651477769\n\n\nyの実態は1970年1月1日からの経過時間を表す整数値である。ところがPOSIXctクラスにはこの整数を時間として表現し直すための関数（メソッドともいう）が実装されている。 Rのコンソールにオブジェクト名を入力して実行すると、Rは自動的に print 関数を呼び出すが、この際オブジェクトのクラスに適合する関数を探し出して実行する。 POSIXctクラスのオブジェクトである y の場合は print.POSIXct() が呼び出されるが、この関数では整数値が日付のフォーマットに変換して上で画面に表示されるのである。\nオブジェクト yの実態は整数値であるため、加算などの数値演算を行うことができる。\n\ny + 1\n\n[1] \"2022-05-02 16:49:30 JST\"\n\n\n同様にSys.Date()は日付を表すクラスオブジェクト（インスタンス）を出力する。\n\nz <- Sys.Date()\nclass(z)\n\n[1] \"Date\"\n\nz\n\n[1] \"2022-05-02\"\n\n\nここで z は Date クラスのオブジェクト（インスタンス）であり、デフォルトでは実行時点での日付を1970年1月1日からの経過日数として保存する。 Dateクラスでは出力用の関数として print.Date()が定義されているため、コンソールには日付フォーマットで表示される。 しかしながら実態は整数値であるため、POSIXctクラスのオブジェクトと同様、算術演算を適用できる。\n\nunclass(z)\n\n[1] 19114\n\nz - 1\n\n[1] \"2022-05-01\"\n\n\nちなみに print.Date() は以下のように定義されている（コンソールに関数名を入力すると表示される）。\n\nprint.Date\n\nfunction (x, max = NULL, ...) \n{\n    if (is.null(max)) \n        max <- getOption(\"max.print\", 9999L)\n    if (max < length(x)) {\n        print(format(x[seq_len(max)]), max = max + 1, ...)\n        cat(\" [ reached 'max' / getOption(\\\"max.print\\\") -- omitted\", \n            length(x) - max, \"entries ]\\n\")\n    }\n    else if (length(x)) \n        print(format(x), max = max, ...)\n    else cat(class(x)[1L], \"of length 0\\n\")\n    invisible(x)\n}\n<bytecode: 0x1031631d8>\n<environment: namespace:base>\n\n\nこの定義から、yオブジェクトの実態である整数値を日付にフォーマットし直して出力することが分かる（正確には内部でさらに format.Date()などを呼び出している）。\nつまりDateや POSIXct, POSIXt の実態は整数なのだが、日付や時間として直感的な操作（整数としての足し算引き算など）ができるように工夫されたクラスなのである。 Rでオブジェクト名をコンソールで実行した場合、暗黙のうちにオブジェクごとに定義された print()が呼ばれる。クラスには専用に関数が定義されることが多い。\nRでは関数名とドットに続けてクラス名をつなげた関数は、指定されたクラスのオブジェクトを表示するためのメソッドとなる。この仕組みによるクラス設計をRではS3と呼ぶ。 summary() などもオブジェクトごとに異なる定義が実装されている（線形回帰を実行する lm クラスには出力を表示するための summary.lm() が実装されている）。 Rではクラス（オブジェクト）ごとに独自の関数が実行されうることは常に意識しておく必要がある。\nRはオブジェクト指向のプログラミング言語であり、C++やJava,Pythonと同様にユーザーが独自にクラスを定義して利用することもできる。 Rではクラスを定義する方法としてS3の他に、S4やReferance Classという仕組みが導入されている。これらの詳細に興味のある読者は〇〇を参照されたい。\nなお日付や時間オブジェクトの操作方法については別の章で解説する。\n\n1.1.3.1 データフレーム\nデータフレームは、Rにおいてデータを保存して操作するための基本クラスである。データフレームはデータベースやExcelファイルなどと共通の形式を持つ。つまりデータは長方形で記録され、行がレコード（測定対象）に対応し、列には対象のある性質について測定した結果が保存される。つまり列はデータ分析における変数に対応する。データフレームでは列ごとにデータ型は異なっていても構わないが、ある列に記録される型は統一されていなければならない。以下Rの標準データセットから、treesと mtcars を例にデータフレームの確認をしよう。treesはアメリカンブラックチェリーと呼ばれるバラ科の樹木の伐採時の幹の直径、重量および高さに関するデータ、mtcarsは1970年代に米国で発行された専門雑誌に掲載された自動車の仕様と性能をまとめたデータである。\nR組み込みのデータセット(datasetsパッケージのデータオブジェクト）は、Rの起動時に参照可能になっている。 インストール後に追加したパッケージに付属するデータは（library(パッケージ名)を実行することで参照可能になるが、data(データ名, package = \"パッケージ名\")としても参照できる。まれにdata()を明示的に呼ばなければデータを参照できないケースもある(kernlabパッケージのincomeデータセットなど）。\n\n# dplyrパッケージのデータを利用する\ndata(band_members, package = \"dplyr\")\n\nデータの概要は?演算子あるいはhelp()を利用して確認できる。\n\n# クラスの確認。data.frame\nclass(trees)\n\n[1] \"data.frame\"\n\n# ヘルプを確認\nhelp(trees)\n?mtcars\n\ntreesの中身をみるには、コンソールにtreesと打ち込むか、あるいはRStudioを使っているのであればView()の引数にデータを指定して実行する。\n\n# コンソールにデータセット名 (オブジェクト名) を\n# 与えて実行するとデータが出力される\ntrees\n# View関数でも同様\nView(trees)\n\nデータ全体ではなく、データの一部を確認したい場合にはhead()やtail()を使う。これらの関数はオブジェクトの先頭ないし末尾の値を出力する関数で、データフレームの場合先頭ないし末尾から6行を表示する。表示行数は引数nにより調整できる。\n\n# head, tailでは引数 n により出力する行数を調整できる\ntail(trees, n = 3)\n\nデータフレームのサイズ(行数や列数)を確認するのであれば、dim()を使うの便利である。\n\n# データセットの行数と列数を確認\ndim(trees)\n\n[1] 31  3\n\ndim(mtcars)\n\n[1] 32 11\n\n\n\n\n\n1.1.4 データフレームの行ないし列の指定\nデータフレームの行ないし列、あるいは一部を参照するには$や[、[[ を使う。 最初に[演算子をベクトルに適用してみよう。[にはベクトルの番号を整数値で指定する。これを添え字という。\n\n# 1から10までのベクトルを作成\n# :はfrom:toの形式で連続する値をベクトル化\nx <- 1:10\n# ベクトルの番号を指定\nx[1]\n\n[1] 1\n\nx[c(1, 3, 5)]\n\n[1] 1 3 5\n\nx[5:1]\n\n[1] 5 4 3 2 1\n\n\nでは[演算子をデータフレームに適用していこう。データフレームでは添え字として整数だけでなく列名も利用可能である。 データフレームでは参照する行と列を区別するためにカンマ (,) を間に挟む。すなわち[行,列]としてカンマの前に行番号、後に列番号（あるいは列名）をベクトルとして指定する。省略した場合、すべての行（なしいし列）を指定したことになる。行番号を省略し、特定の1列だけを指定した場合、返り値はベクトルに変換される。 ただし、[でカンマを使わずに整数値1つを指定した場合、該当する列がデータフレームの属性を保ったまま出力される。\n\n# 1行目を参照\ntrees[1, ]\n# 1列目を参照 (返り値はベクトルになる)\ntrees[, 1]\n# 行番号と列番号を指定\ntrees[1:2, 1:2]\n# 列を名前で指定\ntrees[c(1, 3), c(\"Height\", \"Volume\")]\n# カンマを加えない場合列番号を指定したことになる（出力はデータフレームのまま）\ntrees[1]\n# Girth列だけを抽出する\ntrees[\"Girth\"]\n# 複数の列を指定して抽出する\n# 1列目から3列目を指定\ntrees[1:3]\ntrees[c(\"Girth\", \"Height\")]\n\n次に[[と$演算子について説明しよう。前節の[演算子ではx[1:3]のようにベクトルを添え字として指定できたが、[[では単独の値しか指定できない。そして返り値はベクトルとなる。同様に$演算子はデータフレームの列名を指定する。この場合も返り値はベクトルになる。\n\n# 列番号による参照\ncolnames(mtcars)[[3]] # 3列目は disp\nmtcars[[3]]\n# $演算子による参照\n# データフレームではベクトルとして列の値を参照する\nmtcars$disp\nmtcars[[\"disp\"]]"
  },
  {
    "objectID": "ch1.html#tidyverse-モダンなrのフレームワーク",
    "href": "ch1.html#tidyverse-モダンなrのフレームワーク",
    "title": "1  データを扱うための下準備",
    "section": "1.2 tidyverse: モダンなRのフレームワーク",
    "text": "1.2 tidyverse: モダンなRのフレームワーク\n\n1.2.1 tidyデータ\n分析の対象となるデータが、Rで処理するのに適した形式であるとは限らない。むしろ何らかの整形を必要とする場合がほとんどだろう。 この際、データの形式の不備には大きく2つのケースがあるだろう。1つは、データの入力そのものがおかしいケースで、例えばデータの列数が途中からずれていてフォーマットそのものが崩れている場合や、文字化けが起こってしまって判読不能なケースである。この場合は、最初から入力し直すなど、根本的な修正を解こなさいと、そもそもRなどの統計解析ソフトで読み込むことが不可能だろう。\nもう1つは、形式的にデータは正しく記録されておりRなどで読み込むことが可能だが、必要な分析を行うにはデータの不統一やフォーマットを調整しなければならない場合である。 これには欠損値の表記方法が統一されていない（NAであったり空白であったり、9999などの大きな整数値であったり）、あるいはカテゴリの表記に揺れがある（「女性」、「女」、「F]）などの場合である。これらはRで読み込んで一括変換するなどの処理が必要だろう。こうした入力の不統一などを修正した後で、さらにデータのフォーマットを変換しなければならないこともある。 例えば、以下はある生物個体の体重を記録したフォーマットの例である。2つの形式で表現したデータである。\n\n\n\nid\ntime1\ntime2\ntime3\ntime4\n\n\n\n\na\n3.7\n3.9\n3.5\n3.4\n\n\nb\n4.4\n4.2\n4.5\n4.6\n\n\n\n\n\n\nid\nvar\nvalue\n\n\n\n\na\ntime1\n3.7\n\n\na\ntime2\n3.9\n\n\na\ntime3\n3.5\n\n\na\ntime4\n3.4\n\n\nb\ntime1\n4.4\n\n\nb\ntime2\n4.2\n\n\nb\ntime3\n4.5\n\n\nb\ntime4\n4.6\n\n\n\n最初の表では個体を行、記録した時間を独立した列にとっている。これに対して2つ目の表では時間を1列にまとめ、別の列に測定値をまとめている。 前者は反復データなどを記録するフォーマットとして珍しくないが、Rではこの形式そのままではデータを解析関数に適用できず、後者のように変換する必要があることが多い。\nHadley (2014)は、Rをはじめとしたコンピュータによるデータ操作や可視化、モデリングを行う上で処理を実行しやすい形式をtidyデータと呼んだ。tidyには「片付いた」、「綺麗な」という訳が当てられる。tidyの対義語はmessy 「散らかった」、「雑な」であり、tidyデータとはデータをプログラミングで扱いやすい形式に整理したものである。\ntidyデータの原則は以下にまとめられる。\n\n変数は独立した列として表現する\n観測対象は行として表現する\n観測値はそれぞれ独立させる\n\n\n\n\ntidyデータの設計\n\n\nこの原則からすると先の表1の形式はtidyではなく、messy（散らかった）データということになる。実際、表1の形式のままではRで分析を実行することはできない場合が多い。 著者の経験では、ほとんどのデータはHadleyのtidyデータの思想にそっていない。そのためRなどで効率的にデータ処理をするためには、これらのtidyでないデータを整形する必要がある。 Hadleyらはデータを効率的かつ一貫した方法でtidyな形式に整形するための技法をさまざまなパッケージにまとめている。 そしてこれらのパッケージ群のセットを tidyverse パッケージとして公開している。続く節ではtidyverse パッケージについて解説する。\n\n\n1.2.2 tidyverseパッケージ\nHadleyは2016年よりRでデータをtidyな形式に整形し、そして効率的なデータ操作を行うためのパッケージ群の開発に着手している。 現在、これらのパッケージ群はtidyverseという名称で統合されている。これは以下のパッケージで構成されている。\n\nggplot2パッケージ\ntibbleパッケージ\ntidyrパッケージ\nreadrパッケージ\npurrrパッケージ\ndplyrパッケージ\nstringrパッケージ\nforcatsパッケージ\n\n同時にtidyverseはRでのデータ分析プロセスを総称する概念でもある。ここでRでのデータ分析プロセスとは**(1)データ取得、(2)データ操作、(3)可視化、(4)統計モデリングの4つを指し、tidyverseの理想はこれらをシームレスに実現することにある。\nこのうちtidyverseにおけるデータ処理の理念をHadleyは以下の4点にまとめている (https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html)\n既存のデータ構造を利用する: パッケージで独自にデータ処理用の関数を実装するのではなく、Rのデータフレーム操作を踏襲すべきである。\n1つ1つの関数の処理を簡潔にし、パイプ演算子で連結可能にする: 1つの関数に複雑な処理を実装すべきでなく、処理ごとに小さな関数に分割すべきである。小さな関数はその構造が理解しやすいだけでなく、保守性も優れている。前処理ではこれらの関数を後述するパイプ演算子を使って連結させればよい。なおパイプ演算子を適用するには、それぞれの関数がデータを第一引数に指定する必要がある。\n関数型プログラミングを活用する: Rは関数型プログラミング言語をサポートしている。関数型プログラミングの技法を取り入れるとコードを簡潔にすることができる。\n人間にもわかりやすい設計であること: 処理を実行するのはプログラムであるがコードを書くのは人である。関数や引数は人が理解しやすいよう適切な長さの名前であるのが望ましい。特に関数の名前は処理内容が想像できるようにすべきだ。\ntidyverseパッケージ群は、基本的にこれらの原則が守られている。これによりユーザーが関数を習得しやすくなる。\nlibrary(tidyverse)を実行すると、tidyverseに含まれるパッケージのほとんどが一度に読み込まれる。Hadleyのイメージするデータ分析フローでは、これらのパッケージに含まれる関数を適宜パイプ演算子で連結させて処理をつづけていくので、個々のパッケージを必要になるたびに読み込むのは手間だろう。\n本書の基礎編ではtidyverse から特に以下のパッケージについて解説する。\n\n\n\nパッケージ\n主な機能\n本書で取り上げる章\n\n\n\n\ntibble\nデータフレームの拡張\n1\n\n\ntidyr\nデータ整形\n2\n\n\nreadr\nデータ読み込み・書き込み\n3\n\n\npurrr\n効率的な処理\n5\n\n\ndplyr\nデータ操作\n3, 6\n\n\nstringr\n文字列処理\n4, 6\n\n\nforcats\n因子型データ処理\n4, 6\n\n\n\nこの他にもtidyverseには多くのパッケージが含まれる。glueやhttr、haven、modelrなどであるが、これらについては本書では省略する。また、ggplot2パッケージによる作図をテーマとした書籍が豊富にあるため、詳細を知りたい読者は参考文献を当たって欲しい。\n\n\n1.2.3 tibble: ユーザの使い勝手を向上させたデーターフレーム拡張\ntidyverseでは、データ解析の基本となるデータフレームを拡張したtibbleが主に利用される。そこで最初に tbl_df クラスについて解説しよう\nまずは tbl_df の特徴を把握するため、データフレームと比較しよう。最初に通常のデータフレームを生成する。\n\n# 標準のdata.frame()とdplyr::data_frame()を比較する\nset.seed(21)\n\ndf_data_frame <- \n  data.frame(\n    v1        = c(\"value1\", \"value2\", NA, \"value4\"),\n    `var 2`   = 1:4,\n    v3        = rnorm(4, 1, 1),\n    `4th var` = letters[1:4])\n\ndf_data_frame\n\n      v1 var.2         v3 X4th.var\n1 value1     1  1.7930132        a\n2 value2     2  1.5222513        b\n3   <NA>     3  2.7462222        c\n4 value4     4 -0.2713361        d\n\n\n次にtibble() を使って同じ内容のオブジェクト df_tibble を生成する。\n\nlibrary(tibble)\nset.seed(21)\n\ndf_tibble <- \n  tibble(\n    v1        = c(\"value1\", \"value2\", NA, \"value4\"),\n    `var 2`   = 1:4,\n    v3        = rnorm(4, 1, 1),\n    `4th var` = letters[1:4])\n\ndf_tibble\n\n# A tibble: 4 × 4\n  v1     `var 2`     v3 `4th var`\n  <chr>    <int>  <dbl> <chr>    \n1 value1       1  1.79  a        \n2 value2       2  1.52  b        \n3 <NA>         3  2.75  c        \n4 value4       4 -0.271 d        \n\nclass(df_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ndf_tibble は tbl_df クラスのオブジェクトであるが、コンソールでの出力は従来のデータフレームと若干異なっている。以下、tibbleの特徴をあげる。\n\nデータフレームのソースと大きさ、変数のデータ型を表示する\n出力の際にデータフレーム全体を表示するのではなく、データフレームの一部を返す\n入力された値(文字列)をそのまま評価する\n\ndata.frame()で列要素として文字列を指定するとデフォルトでは因子化されたが、tibbleでは文字列のままとなる − tibbleではrow.namesは与えられない\n\n列名の空白や記号を入力の通りに評価する\n\ndata.frame() では列名が数値で始まっていたり、空白が使われているとXやドットに変換されたが、tibbleではそのまま列名として設定される（ただしクオートされる）\n\n変数の遅延評価を行える – 上の例にはないが v4 = v3 *10 のように他の列（変数）を=の右辺におくことができる\n要素としてリストクラスのオブジェクトを指定できる\n変数を参照する際、正確な名称を指定しない限りNULLを返す\n\ndata.frameでは列名は部分マッチが適用されるが(iris\\$Spe は iris$Species と解釈される）、tibbleでは完全に指定する必要がある。 \\end{itemize}\n\n\n\n\n1.2.4 パイプ処理\n従来のRでは、複数の関数を続けて適用する場合、一次オブジェクトを利用するか、あるいは関数を入れ子にすることが多かった。\n\n# 一時オブジェクトを作成する例\nex1_1 <- 1:4\nex1_2 <- mean(ex1_1)\nround(ex1_2, digits = 0)\n\n[1] 2\n\n# 関数を入れ子にする例\nround(mean(1:4), digits = 0)\n\n[1] 2\n\n\n一時変数を作成する方法は処理が逐次的に行われるため理解しやすいが、多数のオブジェクトがワークスペースに乱立すると混乱を招く恐れがないとはいえない。 一方、関数を入れ子にするとなんの処理をしているのかがわかりにくくなってしまう。 そのため、これらの方法で処理を繰り返していると、コードの見通しが悪くなってしまう。\nこの2つのコーディングと異なる発想で処理を連携させる方法がtidyverseで導入されているパイプ処理である。パイプ処理では %>% 演算子でそれぞれの処理を連結することができる。先のコードを %>% 演算子を使って書き直した例を示す。\n\n# %>% は library(tidyverse)を実行することで導入される\n1:4 %>% \n  mean() %>% # 最初の処理... 平均値を算出\n  round(digits = 0) # 2番目の処理... 小数点の丸め込み\n\n[1] 2\n\n\n%>% 演算子では、演算子の左側にあるオブジェクトが右側の関数の第一引数に暗黙のうちに渡される。したがってround()ではdigits引数だけを指定すればよい。\nパイプ処理を使うことで中間的なオブジェクトを作成する必要はなくなり、処理の順番とコードの記述を揃えることができる。 %>% 演算子の挙動が不明瞭な場合は、中間にprint()を使い、途中結果を出力すると段階的な処理結果が見られるので理解しやすくなるだろう。\n\n\n\nパイプ処理の実行\n\n\n\n1:4 %>% \n  # . の値として1:4が引き継がれているのを確認する\n  # なお本来、x = . の表記は不要である\n  mean(x = .) %>% \n  print() %>% # 途中の値を表示する \n  round()\n\n[1] 2.5\n\n\n[1] 2\n\n\nパイプ処理およびパイプ演算子の特徴をまとめると次のようになる。\n\n直前の%>%で示された内容は次の関数内の第一引数に渡され、ドット(.)によって表記できる\n\n第一引数のドットは省略が可能\n第一引数以外の引数に渡す場合には明示的にドットで表記する\n%>%は処理が完了するまで連続して利用できる\n\n以降本書では、積極的にパイプ処理を取り入れたコードを実行する。"
  },
  {
    "objectID": "ch1.html#まとめ",
    "href": "ch1.html#まとめ",
    "title": "1  データを扱うための下準備",
    "section": "1.3 まとめ",
    "text": "1.3 まとめ\n\nRで扱う主要なデータ型として、論理値、数値、文字型がある。これらはベクトルで処理することが多く、異なるデータ型が同一のベクトル内で与えられた時は、データ型の強制変換が行われる。\nRではすべてがオブジェクトである。オブジェクトにはクラス属性があり、適用可能な関数や出力結果が異なる。\nデータフレームは、表形式のデータ構造である。\nRにおけるデータ分析を統一的な枠組みで提供するtidyverseが存在する。tidyverseはデータ分析に必要となる処理を実行する複数のパッケージの集合パッケージとしても提供されている。\ntidyverseではdata.frame を拡張した tbl_df クラスが実装されている。\n段階的なRの処理を記述する方法として、パイプ演算子を利用したパイプ処理がある。パイプ処理による記述は従来の方法と比べるとユーザが解釈しやすく、記述が用意になるという利点がある。"
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  データ整形および操作: tidyr, dplyrパッケージの基礎",
    "section": "",
    "text": "前章において、Rでさまざまな処理が適用しやすいtidyデータについて述べたが、本章の前半で具体的に手持ちのデータをtidyな形式へ変換する方法を学ぶ。後半は、tidyに整理した後に実行するであろう、データの操作を紹介する。これらの方法はデータの種類に問わず、あらゆるデータで共通な、データの選択や抽出、加工、結合という処理を含んでおり、多くの場面で活用の機会がある。"
  },
  {
    "objectID": "ch2.html#データ自由自在",
    "href": "ch2.html#データ自由自在",
    "title": "2  データ整形および操作: tidyr, dplyrパッケージの基礎",
    "section": "2.1 データ自由自在",
    "text": "2.1 データ自由自在\nデータのあり方が多種多様であることは前章で述べた通りである。そこで、雑多なデータを規則的な形式に整形し、コンピュータによる処理を適用しやすくしたものをtidyデータと言う。ここではtidyデータの条件として定義されている形式へとのデータ整形に有効な関数を備えたRパッケージとしてtidyrを取り上げる。\n\nlibrary(tidyr)\n\n\n2.1.1 横長データと縦長データ\n同一の対象について反復して記録を行ったデータを反復測定データ、経時観測データと呼ぶ。反復測定データの例として、ある植物個体の高さについて2015年4月から2017年3月まで毎月1回の記録を行ったデータを示そう。\n\n# readrを使ったテキストファイルからのデータ読み込み方法は3章で解説する\ndf_wide <- \n  # 読み込み対象のファイルが置かれているパスをfile引数に指定して実行\n  readr::read_csv(file = \"data/plants.csv\",\n                  col_types = readr::cols(\n                    .default = readr::col_double(),\n                    sp = readr::col_character()\n))\n\n\n\n\nこの植物データに記録された項目は”高さ”のみである。このデータは、対象が定まっているために記録の時期や回数を列として扱い、横方向に長くなる傾向がある。このデータは対象の経年変化を捉えやすいが、記録頻度の高いデータでは、対象全体について把握するのが困難となる。\n一方でこのデータは次のように、変数として扱われている調査時期を変数として各項目として調査時期の値を、それに対応させる形で高さの変数とその計測値を記録した3変数のデータとして表現することも可能である。\nこのデータは調査の対象と回数が増えるほど縦に長くなる特徴がある。しかし、同時に複数の調査項目がある時には変数を追加し、同じファイルあるいはシートで管理できる。\n1つの変数の値を複数の変数に分けて記録する横長データに対し、縦長データは項目と値の関係を1つの列とその値として完結させている。生物サイズの例では、もともと変数名に使われていた調査時期をperiodという列として定義し、サイズについてはperiodとひもづく形で記述するようになっている。これは各変数は独立した1つの列、1行につき1つの観測値というtidyデータの原則に従っている。\n\n2.1.1.1 横長と縦長の相互変換\nそれではtidyrパッケージを使ってこのデータ変形を行ってみよう。植物データセットは現在、df_wideというオブジェクト名で保存されている。まずは今一度df_wideの列名を確認しよう。\n\nnames(df_wide)\n\n [1] \"sp\"     \"201504\" \"201505\" \"201506\" \"201507\" \"201508\" \"201509\" \"201510\"\n [9] \"201511\" \"201512\" \"201601\" \"201602\" \"201603\" \"201604\" \"201605\" \"201606\"\n[17] \"201607\" \"201608\" \"201609\" \"201610\" \"201611\" \"201612\" \"201701\" \"201702\"\n[25] \"201703\"\n\n\n一列目は”sp”という列で、個体を識別するための固有な値となっている。これは$演算子を使って列の項目をベクトル化させることで簡単に確認できる。\n\ndf_wide$sp\n\n[1] \"a_1\" \"a_2\" \"b_1\" \"b_2\" \"c_1\"\n\n\ndf_wideの残りの列は数値で表され、調査が行われた時期が西暦月の値（2015年4月の場合は201504）が使われている。各列に含まれる値が高さの観測値である。これらの列は本質的に共通である変数（高さ）を扱っているので、調査時期と高さの観測値の組み合わせからなる列にすることができる。この処理を実行するにはgather()を利用する。\ngather()は第一引数に与えたデータを対象として、キーバリューと呼ばれるkeyとvalueの2つの対となるデータ形式に整形する関数である。keyはvalueに記録される項目、valueにはkeyが扱う値が記録される。gather()ではkeyとvalueはそれぞれ第二、第三引数ので指定する。key引数は元のデータで変数として扱われていた名前が入る列で、value引数で指定した変数には、keyと対になる値が入る。またgather()を実行する時は、全ての変数名がvalueで指定した列に格納される候補となるので、変数として残す列は-を使ってgather()の対象から除外する必要がある。\n植物データに対して、調査時期をkey、高さの観測値をvalueとしたキーバリュー形式を適用すると次のようになる。なお個体識別の列である”sp”列は変数として残しておくことに注意である。キーバリューでは、キーとする項目の数が増えるほど縦長のデータになる特徴がある。\n\n# 横長から縦長データの変換\ndf_long <-\n  df_wide %>%\n  # key... 変数を元に作成される変数\n  # value... keyで対象となる変数の値を保存する変数\n  # 対象から除外する列は - を使って取り除く\n  gather(key = period, value = height, -sp)\n\nhead(df_long)\n\n# A tibble: 6 × 3\n  sp    period height\n  <chr> <chr>   <dbl>\n1 a_1   201504   147.\n2 a_2   201504   148.\n3 b_1   201504   159.\n4 b_2   201504   160.\n5 c_1   201504   175.\n6 a_1   201505   148.\n\n\n\n# sp列を除外しない場合、\n# spの値を含めてキーバリュー形式に変形する\ndf_wide %>%\n  gather(period, height) %>%\n  head()\n\n# A tibble: 6 × 2\n  period height\n  <chr>  <chr> \n1 sp     a_1   \n2 sp     a_2   \n3 sp     b_1   \n4 sp     b_2   \n5 sp     c_1   \n6 201504 147.34\n\n\n次に縦長になったデータを横長に戻す処理を実行しよう。spread()はgather()と対をなす関数で、key引数に指定された値を列名、value引数で指定された変数の値をその値として格納する。\n\n# 縦長から横長への再変換\ndf_wide2 <-\n  df_long %>%\n  spread(key = period, value = height)\n\nhead(df_wide2, 3)\n\n# A tibble: 3 × 25\n  sp    `201504` `201505` `201506` `201507` `201508` `201509` `201510` `201511`\n  <chr>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n1 a_1       147.     148.     149.     149.     149.     149.     149.     149.\n2 a_2       148.     148.     148.     149.     149.     149.     149.     150.\n3 b_1       159.     160.     160.     160.     160.     160.     160.     160.\n# … with 16 more variables: `201512` <dbl>, `201601` <dbl>, `201602` <dbl>,\n#   `201603` <dbl>, `201604` <dbl>, `201605` <dbl>, `201606` <dbl>,\n#   `201607` <dbl>, `201608` <dbl>, `201609` <dbl>, `201610` <dbl>,\n#   `201611` <dbl>, `201612` <dbl>, `201701` <dbl>, `201702` <dbl>,\n#   `201703` <dbl>\n\n\n\n\n\n2.1.2 複数列への分割\ntidyデータへの整形の例として次に、1つの列で分割可能な値を扱っている場合の処理としてseparate()およびextract()を使った例を取り上げる。\n植物データの”sp”列の値を見ると、「a_1」、「a_2」という規則性のある文字列になっている。これはa種の1番目の個体、a種の2番目の個体、…という具合に個体識別のために記録されているものである。一方で種aだけのデータや各種の1番目の個体について対象にした処理を加える際にはこの列は分割しておいた方が良い。このようなデータは複数列に分割するのがtidyデータとして適切である。ここでは引き続き、縦長になったdf_longを対象として、“sp”列の値を分割してみよう。\n\n# 同一の変数内に複数の要素をもつデータを2つの変数に分割する\ndf_long %>%\n  separate(\n    # 分割対象の変数\n    col = sp,\n    # 区切り文字\n    sep = \"_\",\n    # 値を保存する列を指定する\n    into = c(\"species\", \"number\"),\n    #\n    extra = \"warn\",\n    # removeは、col引数の値を残すかを定義する\n    remove = TRUE) %>%\n  head(3)\n\n# A tibble: 3 × 4\n  species number period height\n  <chr>   <chr>  <chr>   <dbl>\n1 a       1      201504   147.\n2 a       2      201504   148.\n3 b       1      201504   159.\n\n\n関数separateは引数colに分割対象とする変数名を指定し、into引数に分割したデータを格納する変数名を与えて実行することで、一つの列に含まれる値を複数の列へと分割する。分割の基準はsep引数で指定する。sepには既定値として正規表現の[^[:alnum:]]+によるパターンが与えられているが、任意の区切り文字やパターンを指定可能である。\nseparate()の仕様では、はじめにsepの値による値の分割が実施され、次にその値をinto列に代入するという流れになる。そのため、intoでの列数と分割後の要素数が一致しない場合、何らかの警告が表示される。また引数extraに既定の”warn”を指定していると、指定した列に含まれない、つまり以降の要素についてはデータから除去される。引数extraには”warn”と”drop”、“merge”の3つの値が指定できる。“warn”は既定値であり、要素と列の不一致に対する警告を表示する。“drop”では列に入りきらない値については警告も表示せずに切り落とす。“merge”を設定しておくと、切り落としは実行されず、列に入りきる状態でcolの値が分割される。代入先の変数の数が不足する場合や過剰な変数の指定についても同様に警告が表示される。この場合、要素が足りないものには欠損値が与えられる。\n変数分割の処理として次にextract()の例を示す。extract()は、1つの列にまとまっている変数を引数regexに指定した正規表現などの基準によって複数の列に分割する。この関数の利用例として、行の中に複数の値が記録されている場合や、特定の基準によって変数を分割するべき状況が考えられる。\n\ndf_plants_sep <-\n  df_long %>%\n  extract(col = sp,\n          # 分割後の変数名を与える\n          into = c(\"species\", \"number\"),\n          # 分割する文字列を指定する\n          regex = \"([[:alnum:]]+)_([[:alnum:]]+)\",\n          # 元の列は取り除く\n          remove = TRUE)\n\nhead(df_plants_sep, 3)\n\n# A tibble: 3 × 4\n  species number period height\n  <chr>   <chr>  <chr>   <dbl>\n1 a       1      201504   147.\n2 a       2      201504   148.\n3 b       1      201504   159.\n\n\nseparate()やextract()により分割した変数を一つの変数に結合するにはunite()を利用する。unite()ではcol引数に結合した変数を格納する変数名を与え、残りの引数に結合対象の列を指定する。既定値では全ての変数が結合対象となる。\nextract()を使って変数を分割したdf_plants_sepの列を再び一つの変数として結合した例を次に示す。species、numberの列の値をsepで指定した文字列”_“で結合し、新たにsp列として扱う、という処理である。\n\n# unite()の引数sepで変数間の値をつなげる文字列を指定する\ndf_plants_sep %>%\n  unite(\n    # 新しく作成する変数の名前を指定\n    col = sp,\n    # 区切り文字として結合時に利用する文字列\n    sep = \"_\",\n    # 対象のデータフレームに含まれる変数名が結合に用いられる\n    species, number) %>%\n  head(3)\n\n# A tibble: 3 × 3\n  sp    period height\n  <chr> <chr>   <dbl>\n1 a_1   201504   147.\n2 a_2   201504   148.\n3 b_1   201504   159.\n\n\nunite()にはremove引数が用意されており、既定ではTRUEが与えられている。これは結合に指定された元の変数を取り除くオプションである。\n\n\n2.1.3 複数行への分割\n一行に分割可能な値が記録されており、それを列ではなく行方向へ展開する場合に便利な関数としてseparate_rows()を紹介する。\n例えば次のような、idごとに複数のタグが記録され、それがカンマ区切りで一つの変数(tag)に記録されたデータがあるとする。各タグにはそれと対応してスコアが与えられているが、その値もまた別の列(score)にカンマ区切りで記録されている。これをデータフレームの変数名はそのままに、タグとスコアを一対の関係として一行ごとに記録したデータへと整形しよう。\n\ndf_wide_col <- \n  tibble(\n  id    = 1:3,\n  tag   = c(\"apple,banana,melon\", \"grape,orange\", \"peer,mango,pine apple\"),\n  score = c(\"0.7,0.5,0.4\", \"0.6,0.2\", \"0.7,0.4,0.3\")\n)\n\n# tag, scoreは対応関係にある\ndf_wide_col\n\n# A tibble: 3 × 3\n     id tag                   score      \n  <int> <chr>                 <chr>      \n1     1 apple,banana,melon    0.7,0.5,0.4\n2     2 grape,orange          0.6,0.2    \n3     3 peer,mango,pine apple 0.7,0.4,0.3\n\n\nseparate_rows()では、引数にデータフレームの変数名を与えて実行する。分割の対象となる区切り文字をsepで指定するが、この例では区切り文字は”,“となる。\n\ndf_wide_col %>% \n  separate_rows(tag, score, sep = \",\") %>% \n  head()\n\n# A tibble: 6 × 3\n     id tag    score\n  <int> <chr>  <chr>\n1     1 apple  0.7  \n2     1 banana 0.5  \n3     1 melon  0.4  \n4     2 grape  0.6  \n5     2 orange 0.2  \n6     3 peer   0.7  \n\n\n関数を実行すると、タグの項目とスコアが一行ずつ記録されたデータになったことが確認できるが、scoreが文字列のデータとなっていることに注意しよう。separate_rows()の処理は、元の変数のデータ型を引き継ぐため、分割後の変数も文字列のままである。そのため分割後のデータ型を見直すためのオプションとして、separate_rows()では引数convertが用意される。convert引数でTRUEを指定した際、変換可能な変数がある場合、以下の例の通り、データ型の自動変換が行われる。\n\ndf_wide_col %>% \n  separate_rows(tag, score, sep = \",\", convert = TRUE) %>% \n  head(3)\n\n# A tibble: 3 × 3\n     id tag    score\n  <int> <chr>  <dbl>\n1     1 apple    0.7\n2     1 banana   0.5\n3     1 melon    0.4\n\n\n\n\n2.1.4 入れ子データ\n文字列や因子型の変数は、グループとして共通の処理を施したり、グループごとの値を参照することがある。標準関数ではsplit()によるデータの分割が可能であるが、これは対象がデータフレームであっても返り値はリストである。そこで、データフレームの構造を維持したままデータをグループに分割するにはnest()が有効となる。\nnest()の実行例として、sp、period、heightの3列からなる次のデータを、調査個体ごとのデータとして扱えるようにする処理を示す。\n\nhead(df_long)\n\n# A tibble: 6 × 3\n  sp    period height\n  <chr> <chr>   <dbl>\n1 a_1   201504   147.\n2 a_2   201504   148.\n3 b_1   201504   159.\n4 b_2   201504   160.\n5 c_1   201504   175.\n6 a_1   201505   148.\n\ndf_nest <- \n  df_long %>% \n  nest(period, height)\n\nWarning: All elements of `...` must be named.\nDid you want `data = c(period, height)`?\n\ndf_nest\n\n# A tibble: 5 × 2\n  sp    data             \n  <chr> <list>           \n1 a_1   <tibble [24 × 2]>\n2 a_2   <tibble [24 × 2]>\n3 b_1   <tibble [24 × 2]>\n4 b_2   <tibble [24 × 2]>\n5 c_1   <tibble [24 × 2]>\n\n\nnest()はデータフレームを第一引数にとり、関数内で指定されたデータフレーム中の変数を別のデータフレームとして処理し、入れ子のように新たな変数として格納する。このような、大きさ(データフレームの場合は行数)が異なるデータを、データフレームの一変数として扱うデータ構造を本書では入れ子データ nested data frameと呼ぶ。\nnest()により、入れ子としてまとめられるデータは変数dataに格納される（これは引数keyで指定した名前に変更することができる）。入れ子のデータを確認するには、変数dataを参照する。\n\n# spがa_1の値かならるデータを格納する\ndf_nest$data[[1]]\n\n# A tibble: 24 × 2\n   period height\n   <chr>   <dbl>\n 1 201504   147.\n 2 201505   148.\n 3 201506   149.\n 4 201507   149.\n 5 201508   149.\n 6 201509   149.\n 7 201510   149.\n 8 201511   149.\n 9 201512   150.\n10 201601   150.\n# … with 14 more rows\n\n\n変数の指定方法は、既定では全ての変数が対象となっている。すなわち対象のデータフレーム中の変数名が与えられない場合はデータフレーム自体を入れ子にする。変数の指定方法は、入れ子にする変数名を直接与える他、列番号、コロン(“:”)を使った連続した変数の選択、マイナス記号(“-”)による除外などが可能である。\n\n# これらの処理は上記の処理と等しい\n\n# - で入れ子に含めない変数を与える\ndf_long %>% \n  nest(-sp)\n\n# 列番号による指定\ndf_long %>% \n  nest(2, 3)\n\ndf_long %>% \n  nest(period:height)\n\n\n# 変数名を指定しない場合は全ての変数を入れ子に含める\ndf_long %>% \n  nest()\n\nWarning: `...` must not be empty for ungrouped data frames.\nDid you want `data = everything()`?\n\n\n# A tibble: 1 × 1\n  data              \n  <list>            \n1 <tibble [120 × 3]>\n\n\n\n\n\n入れ子データのイメージ\n\n\n入れ子を解除するには、unnest()を使う。これにより入れ子データとする前のデータ構造が復元される。\n\ndf_nest %>% \n  unnest()\n\nunnest()にはいくつかのオプションが備わっている。例えばunnest()を実行した時に用いられる変数名は、入れ子に含まれていた元の変数名であるが、.sepで、nest()を実行した時の入れ子の変数名(.data引数で調整。既定値はdata)に続けて任意の文字列を含ませるようになる。\n\n# 入れ子データを展開した時の変数名を \"data_(元の変数名)\" とする\ndf_nest %>% \n  unnest(.sep = \"_\")\n\nまたunnest()は、データ型が等しい、リストの要素を一行ずつ展開する機能も備えている。例えば次のようにlist列に、行ごとに長さの異なるベクトルがリストとして格納されているデータに対して、1行に要素の1つの値を含んだデータへ変換する。\n\n# 長さの異なる要素をもつリストの列を含んだデータフレームを用意\n(df <- \n  tibble(\n    id   = 1:3,\n    list = list(\"a\" = 1,\n                \"b\" = 2:3,\n                \"c\" = 4:6)))\n\n# A tibble: 3 × 2\n     id list        \n  <int> <named list>\n1     1 <dbl [1]>   \n2     2 <int [2]>   \n3     3 <int [3]>   \n\n\n\n# .id引数は、展開されたデータを識別するのに有効である\ndf %>% \n  unnest(.id = \"list_id\")\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(list)`\n\n\nWarning: The `.id` argument of `unnest()` is deprecated as of tidyr 1.0.0.\nManually create column of names instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n# A tibble: 6 × 3\n     id  list list_id\n  <int> <dbl> <chr>  \n1     1     1 a      \n2     2     2 b      \n3     2     3 b      \n4     3     4 c      \n5     3     5 c      \n6     3     6 c"
  },
  {
    "objectID": "ch2.html#柔軟なデータ操作",
    "href": "ch2.html#柔軟なデータ操作",
    "title": "2  データ整形および操作: tidyr, dplyrパッケージの基礎",
    "section": "2.2 柔軟なデータ操作",
    "text": "2.2 柔軟なデータ操作\ntidyverseにはデータ操作のためのパッケージとしてdplyrが整備されている。dplyrパッケージは”A Grammar of Data Manipulation”を掲げ、データ操作を文法として捉え、複雑になりがちなデータ操作を簡潔に記述することを可能としている。dplyrの機能を見るため、以下のコードを実行し、パッケージとデータセットを利用可能なようにしておこう。データは前章で扱ったtreesデータとmtcarsデータである。\n\n# dplyrパッケージの読み込み\nlibrary(dplyr)\ndata(\"trees\")\ndata(\"mtcars\")\n\n\n2.2.1 データ操作における5つの工程\nデータに行う基本操作は、選択、抽出、並び替え、集計、加工の5工程に分けられる。データ分析の前処理として、これらの操作を組み合わせデータを加工したあとで、可視化やモデリング、分析関数を実施することになるだろう。\ndplyrパッケージではこれらのデータ操作のための処理を個別の関数として提供する。加工と選択を実行する関数はそれぞれmutate()およびtransmute()、select()であり、抽出はfilter()、summarise()による集計、並び替えはarrange()で実行される。\n選択 select: 必要な変数を参照・抽出する、不要な変数を削除する\n抽出 filter: 特定の条件に一致する値を取り出す\n並び替え arrange: 変数内の項目の順序を変更する\n集計 summarise: データ全体あるいはグループ別の代表値・統計量等の算出\n加工 mutate, transmute: 値の単位・尺度などを変更する、新たな列を追加する\n\n\n\nデータ操作をdplyrの関数で実施する例\n\n\n\nこれらのデータ操作関数の特徴は、基本的にはtidyverseの原則に従い、第一引数に対象のデータ、上記の5つの関数は全てデータフレームをとり、関数内部で処理内容を記述するという形式をとる。dplyrのフレームワークでは、関数名はデータに適用する「動詞」、その内容を「文法」として表現する。\n\n\n2.2.2 データを抽出する: filter\nデータ抽出とは、データからあるまとまりを取り出すことである。部分集合を抽出すると言い変えても良い。filter()は比較・論理演算子を用いた条件指定を行い、条件に該当するデータ (TRUEとなる値)を返す関数である。関数の第一引数には対象のデータをとり、以降に条件式を記述して実行する。\n条件式は、左辺に対象の変数を指定して不等号を挟んで右辺に比較する値を与える、という記述となる。treesの”Volume”列の値が52以上のデータを抽出するコードは次のようになる。\n\n# filter()では比較演算子を用いたデータ抽出を実行する\ntrees %>% filter(Volume >= 52.0)\n\n  Girth Height Volume\n1  17.3     81   55.4\n2  17.5     82   55.7\n3  17.9     80   58.3\n4  20.6     87   77.0\n\n\nこのコードの「文法」を読み解いてみよう。データ抽出を実行するfilter()に、treesをパイプ演算子で対象に与えている。続く第二引数に、具体的な条件を指定しているが、ここで注目すべきは”Volume”列の参照に$演算子を必要とせず、変数名を直接指定することである。dplyrを使ったデータ操作では、主な対象がデータ自身とその変数になるため、変数を参照する$は省略可能になっている。\n複数の条件を指定して抽出をする場合、カンマ (,)あるいは縦線 (|)により条件となる処理を区切る。ここでカンマは「かつ and」であり、縦線は「または or」である。次の実行例は「Heightが65より小さい、またはVolumeが52以上の行を抽出」する。\n\ntrees %>% filter(Height < 65 | Volume >= 52.0)\n\n  Girth Height Volume\n1   8.8     63   10.2\n2  13.8     64   24.9\n3  17.3     81   55.4\n4  17.5     82   55.7\n5  17.9     80   58.3\n6  20.6     87   77.0\n\n\n\n\n\n\n演算子\n効果\n\n\n\n\n>\n左辺が右辺より大きい\n\n\n>=\n左辺が右辺より大きいか等しい\n\n\n<\n左辺が右辺より小さい\n\n\n<=\n左辺が右辺より小さいか等しい\n\n\n==\n左辺と左辺が一致する\n\n\n!=\n左辺と左辺不一致\n\n\n|\n論理和 (2つの条件式のいずれかが真)\n\n\n&\n論理積 (2つの条件式が共に真)\n\n\n\n一つの変数に複数の条件を与える場合、|と同等の機能をもつ%in%演算子を利用することもできる。%in%演算子はmatch()関数の機能を演算子として提供するもので、左辺ベクトルから右辺のベクトルの値を抽出する。\n\n# 2つの処理結果は等しい\ntrees %>% filter(Height == 70 | Height == 81)\n# %in%演算子を使い、左辺に含まれる右辺のベクトルの値を抽出する\nmtcars %>% filter(cyl %in% c(6, 8))\n\nまた数値を対象にとり、範囲指定による抽出がbetween()を用いることで可能である。この関数は第一引数に対象の数値ベクトル、第二、第三引数で下限と上限を指定し、範囲に含まれるかを論理値で返す。この関数をfilter()に用いた例を以下に示す。\n\n# disp列の値が200から300までの値を抽出する\nmtcars %>% filter(between(disp, 200, 300))\n\n\n2.2.2.1 欠損値を含む変数に対するfilter()の働き\n欠損値を含んだ変数についてfilter()を実行した結果を確かめておこう。filter()では記述した条件に従う値を取り出すという処理が適用されるため、下記のコードを実行すると欠損値を含んだ行は除外される。\n\nd <- tibble(var = c(1, 2, NA, 4, 5))\n\nd %>% filter(var >= 2)\n\n# A tibble: 3 × 1\n    var\n  <dbl>\n1     2\n2     4\n3     5\n\n\n変数varが2以上の値の抽出に欠損値を含んだ行を加えるにはどうすれば良いだろうか。これには縦線による「または」の条件に欠損値である値を指定すると良い。\n\nd %>% filter(var >= 2 | is.na(var))\n\n# A tibble: 4 × 1\n    var\n  <dbl>\n1     2\n2    NA\n3     4\n4     5\n\n\n\n\n2.2.2.2 位置を利用した抽出\n特定の変数の値が大きいあるいは小さいものから何行か分の抽出を行いたい、という時にはtop_n()やnth()などの関数が役立つ。\ntop_n()は対象をデータフレームとし、引数で行数や基準となる変数を指定する(初期値では第一番目の変数が基準となる)。\n\nd <- tibble(var = 1:5)\n\n# var変数が大きな値から2行抽出する\nd %>% top_n(n = 2L, wt = var)\n\n# A tibble: 2 × 1\n    var\n  <int>\n1     4\n2     5\n\n# -演算子をつけて降順での抽出を行う\nd %>% top_n(-2L, var)\n\n# A tibble: 2 × 1\n    var\n  <int>\n1     1\n2     2\n\n\n一方、nth()やfirst()、last()はベクトルを対象にとり、それぞれ、任意の位置の値、先頭、末尾の値を抽出する。そのためfilter()と組み合わせて利用することも可能である。\n\nfirst(d$var)\n\n[1] 1\n\nnth(d$var, 2)\n\n[1] 2\n\n\n\nd %>% filter(var == first(var))\n\n# A tibble: 1 × 1\n    var\n  <int>\n1     1\n\nd %>% filter(var == last(var))\n\n# A tibble: 1 × 1\n    var\n  <int>\n1     5\n\nd %>% filter(var == nth(var, 2))\n\n# A tibble: 1 × 1\n    var\n  <int>\n1     2\n\n\n\n\n2.2.2.3 重複を除いた抽出\nデータから重複した行を取り除いたユニークなデータへ加工するには関数distinct()を用いる。引数に重複のある変数を指定することで、重複を除いた値を返す。一方で変数名を指定した実行では対象外の変数は初期値では削除されてしまうので、ユニークにする処理を行いつつ他の変数を残す場合は.keep_allでTRUEを指定する必要がある。またこの時、ユニークに扱う変数以外の値は、重複のある行の第一行目の値が選択される。\n\nd <- tibble(\n  var = c(1, 2, 1, 3, 4),\n  var2 = c(\"A\", \"B\", \"A\", \"A\", \"D\"),\n  var3 = c(1.0, 1.0, 1.0, 2.0, 2.3)\n)\n\n# データ全体から重複する行を除く\nd %>% distinct()\n\n# A tibble: 4 × 3\n    var var2   var3\n  <dbl> <chr> <dbl>\n1     1 A       1  \n2     2 B       1  \n3     3 A       2  \n4     4 D       2.3\n\n# 引数に指定した変数の重複を解消する\nd %>% distinct(var2)\n\n# A tibble: 3 × 1\n  var2 \n  <chr>\n1 A    \n2 B    \n3 D    \n\n# .keep_allを指定することで\n# ユニークな値として扱いつつ他の変数も残す\nd %>% distinct(var2, var3, .keep_all = TRUE)\n\n# A tibble: 4 × 3\n    var var2   var3\n  <dbl> <chr> <dbl>\n1     1 A       1  \n2     2 B       1  \n3     3 A       2  \n4     4 D       2.3\n\n\nまた、ベクトルに含まれるユニークな値の数を数えるn_distinct()もある。この関数はlength(unique(x))のラッパとして機能するが幾分か処理が高速であり、欠損値をカウントに含めないための引数na.rmが用意されている。\n\n# 欠損を含んだベクトルを生成\n# 3および欠損値 NAが重複\nx <- c(NA, 2, 3, 3, NA, 5)\nx %>% n_distinct() # NAを含めたユニークな要素の和\n\n[1] 4\n\nx %>% n_distinct(na.rm = TRUE) # 欠損を含めないユニークな要素数\n\n[1] 3\n\n\n\n\n2.2.2.4 選択した行を抽出する\nデータの抽出方法として、filter()のような条件指定による抽出とは別に、slice()を利用した位置を指定したデータ抽出がある。これはRで標準的に利用可能なhead()やtail()のようにデータの特定の行を抽出する関数であるが、抽出対象の位置が指定可能な点で異なっている。\n\n# 3行目のデータを抽出\ntrees %>% slice(3L)\n\n  Girth Height Volume\n1   8.8     63   10.2\n\n# 抽出対象の行数を指定する\ntrees %>% slice(c(1, 3, 6))\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.8     63   10.2\n3  10.8     83   19.7\n\n\n\n\n2.2.2.5 データの一部を無作為に抽出: sample_n / sample_frac\nデータからランダムに一部を取り出す時にsample_n()およびsample_frac()が役立つ。これらは引数sizeで指定した条件でいずれも無作為にデータフレームの特定行を抽出する関数であるが、その違いはsample_n()は具体的に行数を、sample_frac()はデータ全体の行数から抽出するデータの行数の割合を指定するということになる。なお無作為抽出の際に復元抽出を認めるreplaceオプションを用意している(既定値ではFALSE)。\n\n# mtcarsデータセットの行数を確認\nmtcars %>% nrow()\n\n[1] 32\n\n# 無作為抽出により5行分のデータを抽出\nmtcars %>% sample_n(5)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nFord Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nMaserati Bora  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n\n# 復元抽出による無作為抽出\n# Toyota Corolla の行が複数回抽出されている点に注意\nset.seed(1234)\nmtcars %>% sample_n(4, replace = TRUE)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n\n# sample_frac()ではデータセット全体の行数に対する割合を指定\nmtcars %>% sample_frac(0.1)\n\n                    mpg cyl  disp  hp drat   wt  qsec vs am gear carb\nHornet Sportabout  18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2\nMerc 450SE         16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4\n\n\n\n\n\n2.2.3 データフレームからの変数選択: select\nselect()はデータフレームから特定の列(変数)を選択するのに用いられる。選択対象は関数内部で指定された変数になる。対象の変数の列が並ぶ時は:を使って表記を省略できる。また対象の変数の前に記号-をつけることで選択から除外することもできる。\n\n# 変数名を与え、2列を選択する\ntrees %>%\n  select(Height, Volume) %>%\n  dim()\n\n[1] 31  2\n\n# 列番号による変数の選択\ntrees %>%\n  select(c(1, 3)) %>%\n  colnames()\n\n[1] \"Girth\"  \"Volume\"\n\n# 除外する列名は - で指定する\n# mtcarsデータセットのdispからwtまでの列を削除する\nmtcars %>%\n  select(-(disp:wt), 3) %>%\n  head()\n\n                   mpg cyl  qsec vs am gear carb disp\nMazda RX4         21.0   6 16.46  0  1    4    4  160\nMazda RX4 Wag     21.0   6 17.02  0  1    4    4  160\nDatsun 710        22.8   4 18.61  1  1    4    1  108\nHornet 4 Drive    21.4   6 19.44  1  0    3    1  258\nHornet Sportabout 18.7   8 17.02  0  0    3    2  360\nValiant           18.1   6 20.22  1  0    3    1  225\n\n# 選択した列の順に変数名が並び替えられる\ntrees %>%\n  select(Volume, Height) %>%\n  colnames()\n\n[1] \"Volume\" \"Height\"\n\n\n\n2.2.3.1 変数名の変更: rename\n関数selectでは関数内部で変更後の変数名 = 変更前の変数名と指定すると選択時に列名の変更ができる。ただしselect()は、変数を選択する関数であり、選択しなかった変数はデータフレームから除外される。そのため変数名の変更には関数renameを用いるか、あるいはeverything()を使い、全ての変数を選択すると良い。\n\n# 変更後 = 変更前、の形式で変数名を記述することで変数名の変更が行われる\n# select()では選択した変数名以外は取り除かれる\ntrees %>%\n  select(height = Height) %>%\n  colnames()\n\n[1] \"height\"\n\n# rename()では他の変数に影響を与えずに変数名の変更を行う\ntrees %>%\n  rename(height = Height) %>%\n  colnames()\n\n[1] \"Girth\"  \"height\" \"Volume\"\n\n# everything()により他の変数名を残す\n# ただし変数の並びが変更されることに注意\ntrees %>%\n  select(height = Height, everything()) %>%\n  colnames()\n\n[1] \"height\" \"Girth\"  \"Volume\"\n\n\n\n\n2.2.3.2 変数選択の補助関数\nselect_helpers関数群を利用すると効率的な変数選択が可能になる。これらの関数は、例えば先ほど紹介した「全ての変数を選択する」everything()をはじめとして、「XXXで始まる変数」、「XXXとマッチする」というようなパターンマッチや条件指定による選択を実行する。\n\nd <- tibble(\n  x_col = 7,\n  y_col = c(\"あ\", \"い\", \"う\"),\n  var_1 = letters[1:3],\n  var_2 = 9:7,\n  var_3 = c(0.1, NA, 0.4),\n  var_z = rnorm(3)\n)\n\n# データの変数名を確認\nd %>% colnames()\n\n[1] \"x_col\" \"y_col\" \"var_1\" \"var_2\" \"var_3\" \"var_z\"\n\n\n\n\n# 'var'で始まる変数を選択する\nd %>%\n  select(starts_with(\"var\")) %>%\n  colnames()\n\n[1] \"var_1\" \"var_2\" \"var_3\" \"var_z\"\n\n# 'col'で終わる変数を選択\nd %>%\n  select(ends_with(\"col\")) %>%\n  colnames()\n\n[1] \"x_col\" \"y_col\"\n\n# 'co'を含んだ変数を選択\nd %>%\n  select(contains(\"co\")) %>%\n  colnames()\n\n[1] \"x_col\" \"y_col\"\n\n# 正規表現を用いたマッチングによって変数を選択する\nd %>%\n  select(matches(\".ar_[0-9]\")) %>%\n  colnames()\n\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n# 任意の文字列と数値の組み合わせからなる変数を選択\nd %>%\n  select(num_range(\"var_\", 1:3)) %>%\n  colnames()\n\n[1] \"var_1\" \"var_2\" \"var_3\"\n\n# 任意の変数集合の中から該当する列を選択\nd %>%\n  select(one_of(c(\"var_1\", \"x_col\"))) %>%\n  colnames()\n\n[1] \"var_1\" \"x_col\"\n\n# すべての変数を選択\nd %>%\n  select(y_col, var_z, everything()) %>%\n  colnames()\n\n[1] \"y_col\" \"var_z\" \"x_col\" \"var_1\" \"var_2\" \"var_3\"\n\n\nselect_helpers関数群は、select()以外の関数、tidyrを始めたとしたtidyverseに含まれるパッケージでも利用可能である。またtidyselectというパッケージでの開発が進んでおり、将来的にはこちらのパッケージに移植される可能性がある。\n\n\n\n2.2.4 データの並び替え: arrange\nデータの並び替えは、単純に見た目を良くするためや、集計や加工のために順番を変更する、という理由で行われる。この処理は関数arrangeにより実行する。例えばtreesデータセットでは、Girth列の大きさによってデータが並べられているが、他の変数を基準として順序を入れ替えるという処理を行う時、次のようにする。\n\n# treesではGirthの大きさでデータが並ぶ\ntrees %>% head()\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n# Height列を基準とした昇順並び替えを行う\ntrees %>%\n  arrange(Height) %>%\n  head()\n\n  Girth Height Volume\n1   8.8     63   10.2\n2  13.8     64   24.9\n3   8.6     65   10.3\n4  11.0     66   15.6\n5  11.7     69   21.3\n6   8.3     70   10.3\n\n\narrange()はデータを任意の変数を基準に並び替える。この時の並びは昇順となるが、降順指定するにはdesc()を併用する。なお基準とする変数の値が数値型である場合は-をつけて降順の指定が可能である。\n\n# 降順に並び替えるにはdesc()や-演算子を用いる\ntrees %>%\n  arrange(desc(Girth)) %>%\n  head()\n\n  Girth Height Volume\n1  20.6     87   77.0\n2  18.0     80   51.5\n3  18.0     80   51.0\n4  17.9     80   58.3\n5  17.5     82   55.7\n6  17.3     81   55.4\n\n# 数値からなる変数では - で降順指定が可能\ntrees %>%\n  arrange(-Height, -Volume) %>%\n  head()\n\n  Girth Height Volume\n1  20.6     87   77.0\n2  13.3     86   27.4\n3  12.9     85   33.8\n4  10.8     83   19.7\n5  17.5     82   55.7\n6  17.3     81   55.4\n\n\n\n\n2.2.5 変数に対する処理: mutate / transmute\nデータフレームの既存の変数に処理を加えたり、新たな列を追加するにはmutate()およびtransmute()を利用する。これらの関数では、引数内で算術演算や任意の関数を指定し、データの変数をベクトルとして引数に渡すことが可能である。形式としては、mutate(データ, 変更後の変数 = 処理内容)という具合になる。次の例は「treesに、各行のGirth列に2.54を乗した値をgirth_cmという列を追加する」という処理である。\n\ntrees %>%\n  mutate(girth_cm = Girth * 2.54) %>%\n  head(3)\n\n  Girth Height Volume girth_cm\n1   8.3     70   10.3   21.082\n2   8.6     65   10.3   21.844\n3   8.8     63   10.2   22.352\n\n\nmutate()では、引数で指定した変数名を追加するが、対象のデータに存在する場合、変数の値を上書きする。\n\ntrees %>%\n  mutate(Height       = as.character(Height),\n         Volume_round = round(Volume, digits = 0)) %>%\n  head(3)\n\n  Girth Height Volume Volume_round\n1   8.3     70   10.3           10\n2   8.6     65   10.3           10\n3   8.8     63   10.2           10\n\n\nmutate()と似た機能をもつ関数としてtransmute()がある。その違いはmutate()では変更を加えなかった列も残るのに対し、transmute()では引数内部で指定した処理の結果からなるデータフレームを返す点にある。\n\n# transmute()では実行結果のみからなるデータフレームを返す\ntrees %>%\n  transmute(girth_cm = Girth * 2.54) %>%\n  head(3)\n\n  girth_cm\n1   21.082\n2   21.844\n3   22.352\n\n\n\n2.2.5.1 ベクトルを対象とした値の操作\ndplyrでは、ベクトルを対象にしたデータ操作の関数が備わっており、中にはベクトル内の欠損を処理する関数もある。いずれの関数もmutate()内外で利用できる。これらの関数はそれぞれ表XXに示すような差異がある。\n\n\n\n\n\n\n\n関数\n特徴\n\n\n\n\ncase_when()\nベクトルの各要素に対する挙動を指定。指定しない場合には欠損値として扱われる\n\n\nrecode()\nベクトル内の特定の値だけを変更する。変換後の型に注意\n\n\nif_else()\nifelse()の拡張。条件式とその結果に対する処理を定義する。欠損値に対する挙動も指定できる\n\n\nna_if()\n特定の値を欠損値へと変換する\n\n\ncoalesce()\n欠損値を特定の値へと変換する\n\n\n\n関数if_elseは条件式および、条件式について真・偽であった時の挙動をそれぞれ引数の値として指定して実行する。treesのHeight列の値をベクトルとして参照し、各要素が80より大きければ”large”、小さければ”small”を返す例を次に示す。\n\nif_else(condition = trees$Height > 80,\n        true = \"large\",\n        false = \"small\")\n\n [1] \"small\" \"small\" \"small\" \"small\" \"large\" \"large\" \"small\" \"small\" \"small\"\n[10] \"small\" \"small\" \"small\" \"small\" \"small\" \"small\" \"small\" \"large\" \"large\"\n[19] \"small\" \"small\" \"small\" \"small\" \"small\" \"small\" \"small\" \"large\" \"large\"\n[28] \"small\" \"small\" \"small\" \"large\"\n\n\nなおif_else()では変換に用いる値のデータ型は統一されている必要がある。特に欠損値として処理させる場合、NA_integerのようにデータ型についても指定しなければならない点に注意である。\n\n# 返り値を欠損値とする場合は\n# 真・偽の結果でデータ型を揃える必要がある\nx <- c(trees$Height, NA)\n\n\n# 整数と実数を区別するため、データ型の不一致によるエラーとなる\nif_else(condition = x > 80, 1L, 0)\n# Error: `false` must be type integer, not double\n\n\n# いずれの返り血も整数型であるので処理は実行される\nif_else(condition = x > 80, 1L, 0L)\n\n [1]  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0\n[26]  1  1  0  0  0  1 NA\n\n\n\n# 真であるときの返り値が整数値なので\n# 偽であるときの返り値も整数型の欠損値にする\nif_else(condition = x > 80, 1L, NA)\n# Error: `false` must be type integer, not logical\n\n\nif_else(condition = x > 80, 1L, NA_integer_)\n\n [1] NA NA NA NA  1  1 NA NA NA NA NA NA NA NA NA NA  1  1 NA NA NA NA NA NA NA\n[26]  1  1 NA NA NA  1 NA\n\n\n引数missingは欠損値への処理を指定するために利用できる。\n\nif_else(condition = x > 80, 1L, 0L, missing = -99L)\n\n [1]   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   1   1   0\n[20]   0   0   0   0   0   0   1   1   0   0   0   1 -99\n\n\nif_else()では、関数の中で指定できる条件は一つなので、より複数の条件を設定するには入れ子としてさらにif_else()を記述することが可能である。ただしこのような場合には次のcase_when()やrecode()のように複数の条件が指定可能な関数を利用するのが適切である。\ncase_when()は複数の条件分岐をベクトルに適用するのに用いられる。通常の関数とは引数の指定方法が異なっており、関数内部で条件式と、TRUEとなる場合の変更後の値をチルダ記号（~）で繋いでformulaとして記述する(「対象の値 == 条件式 ~ 変換後の値」)。条件文はカンマによって区切られるが、宣言の順に値の評価が実行される。また対象のベクトルが、記述したいずれかの条件に一致しない場合には欠損値として扱わるという点については注意が必要である。\n\n# treesデータセットのGirthの値に応じて指定した文字列を返す\ncase_when(\n  trees$Girth <= 12 ~ \"small\",\n  trees$Girth <= 13 ~ \"medium\",\n  trees$Girth > 13 ~ \"large\"\n)\n\n [1] \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"small\" \n [9] \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"small\"  \"medium\"\n[17] \"medium\" \"large\"  \"large\"  \"large\"  \"large\"  \"large\"  \"large\"  \"large\" \n[25] \"large\"  \"large\"  \"large\"  \"large\"  \"large\"  \"large\"  \"large\" \n\n\nベクトルの特定の値を変更したい時にはrecode()も利用できる。この関数も引数内で「対象の値 = 変換後の値」を記述する形式となるように、対象となる値を直接記述するのがcase_when()との違いである。また、変更後の値が指定されていない要素には元の値が採用されるが、引数.defaultに与えた値を利用することもできる。\n\n# mtcarsデータセットのcylの値を変更する\nrecode(mtcars$cyl, `4` = \"四\", `6` = \"六\", `8` = \"八\")\n\n [1] \"六\" \"六\" \"四\" \"六\" \"八\" \"六\" \"八\" \"四\" \"四\" \"六\" \"六\" \"八\" \"八\" \"八\" \"八\"\n[16] \"八\" \"八\" \"四\" \"四\" \"四\" \"四\" \"八\" \"八\" \"八\" \"八\" \"四\" \"四\" \"四\" \"八\" \"六\"\n[31] \"八\" \"四\"\n\n# 変更後の値を指定しない時の挙動 (.default)と、欠損値の変更を.missingにより指定する\nrecode(c(mtcars$cyl, NA),\n       `4` = \"四\",\n       `8` = \"八\",\n       .default = \"その他\",\n       .missing = \"なし\")\n\n [1] \"その他\" \"その他\" \"四\"     \"その他\" \"八\"     \"その他\" \"八\"     \"四\"    \n [9] \"四\"     \"その他\" \"その他\" \"八\"     \"八\"     \"八\"     \"八\"     \"八\"    \n[17] \"八\"     \"四\"     \"四\"     \"四\"     \"四\"     \"八\"     \"八\"     \"八\"    \n[25] \"八\"     \"四\"     \"四\"     \"四\"     \"八\"     \"その他\" \"八\"     \"四\"    \n[33] \"なし\"  \n\n\n\n# 文字列ベクトルに数値を混ぜることはできない\nrecode(mtcars$cyl, `4` = \"四\", `6` = \"六\",  `8` = 8.0)\n# Error: Vector 3 must be type character, not double\n\n欠損値を処理する関数na_if()はベクトル中の欠損値を対象に、y引数で指定された特定の値への変換する。coalesce()は対象と同じ長さのベクトルを与えることで、欠損値を同じ要素の位置の値で置換する。また、長さが異なるベクトルとして1つの値が与えられた時は、その値が繰り返し使われる。\n\nx <- c(1, \"\", 3, 4)\n# ベクトル中の \"\" を欠損値へ変換\nx %>% na_if(y = \"\")\n\n[1] \"1\" NA  \"3\" \"4\"\n\n\n\nx <- c(1, 2, NA, NA, 5)\nx %>% coalesce(2)\n\n[1] 1 2 2 2 5\n\n# 欠損値が含まれる3, 4番目の要素を\n# 3, 4へと置換\ncoalesce(x,\n         c(NA, NA, 3, 4, 5))\n\n[1] 1 2 3 4 5\n\n\n\n\n2.2.5.2 値を前後にずらす: lead / lag\n植物データのように行ごとに調査時期と数値のキーペアからなる反復測定データでは、1つ前の期間との差分を求めたり、2つの期間の合計値を求める、などの処理を実施することがある。この処理を実行するには、対象のベクトルの要素を前後にずらして参照する機能をもつlead()およびlag()を利用する。\n次の例は5つの要素をもつベクトルxにlead()を適用した物である。lead()、lag()は引数nで対象ベクトルの基準となる位置を前後に調整する。ベクトルを参照した時、元の長さ以上の位置にある要素にはNAが与えられるが、これはdefault引数により任意の値に変更可能である。\n\n\nx <- 1:5\n\n# n の既定値には1が与えられている\nx %>% lead()\n\n[1]  2  3  4  5 NA\n\n# 3つ後ろの要素を参照\nx %>% lead(3)\n\n[1]  4  5 NA NA NA\n\n# defaultによりNAの値を変更\nx %>% lead(3, default = 0)\n\n[1] 4 5 0 0 0\n\n\nこの関数を用いて植物データの各個体の高さ成長が調査時期でどれだけ増えたかを確認しよう。ここでは簡潔な結果を示すために一個体分のデータを用いることにする。\n\n\n# 植物データ (data/plants.csv)を\n# tidyrを用いて縦長にしたデータを利用する\n\n# a_1個体のみを抽出し (filter)、\n# periodの並びにする (arrange)\nd <-\n  df_long %>%\n  filter(sp == \"a_1\") %>%\n  arrange(period)\n\nd %>% head()\n\n# A tibble: 6 × 3\n  sp    period height\n  <chr> <chr>   <dbl>\n1 a_1   201504   147.\n2 a_1   201505   148.\n3 a_1   201506   149.\n4 a_1   201507   149.\n5 a_1   201508   149.\n6 a_1   201509   149.\n\nd$height[2] - d$height[1]\n\n[1] 0.16\n\n\nここで2015年4月と次の調査時期である2015年5月の値を見ると、1増えていることがわかる。これをlag()で算出しよう。\n\nd %>%\n  mutate(growth = height - lag(height))\n\n# A tibble: 24 × 4\n   sp    period height  growth\n   <chr> <chr>   <dbl>   <dbl>\n 1 a_1   201504   147. NA     \n 2 a_1   201505   148.  0.160 \n 3 a_1   201506   149.  1.17  \n 4 a_1   201507   149.  0.0800\n 5 a_1   201508   149.  0.220 \n 6 a_1   201509   149.  0.170 \n 7 a_1   201510   149.  0.160 \n 8 a_1   201511   149.  0.150 \n 9 a_1   201512   150.  0.110 \n10 a_1   201601   150.  0.0400\n# … with 14 more rows\n\n\n\n\n2.2.5.3 順位や割合、累積和を求める: ranking / cumall\nベクトル内の要素の順位や割合を求めるための関数群がrankingとして提供されている。これを用いて、次のデータフレームの変数varの値へ順位に関する関数を適用する例を紹介する。\n\nd <- tibble(\n  var = c(1:5, 3, NA)\n)\nmutate(d,\n       # 行の順番に番号を与える\n       row_number = row_number(),\n       # 指定の変数の昇順での順位を与える。\n       # 同位値がある場合、同位の数が順位に影響する\n       min_rank   = min_rank(var),\n       # 同位値の数とは関係なく、連続した順位が与えられる\n       dense_rank = dense_rank(var),\n       # 順位を0から1の範囲で示す\n       percent_rank = percent_rank(var),\n       # 順位を積み上げ、全体での割合を示す\n       cume_dist = cume_dist(var),\n       # データを引数nで指定した値で分割した時のグループ\n       ntile = ntile(var, n = 2))\n\n# A tibble: 7 × 7\n    var row_number min_rank dense_rank percent_rank cume_dist ntile\n  <dbl>      <int>    <int>      <int>        <dbl>     <dbl> <int>\n1     1          1        1          1          0       0.167     1\n2     2          2        2          2          0.2     0.333     1\n3     3          3        3          3          0.4     0.667     1\n4     4          4        5          4          0.8     0.833     2\n5     5          5        6          5          1       1         2\n6     3          6        3          3          0.4     0.667     2\n7    NA          7       NA         NA         NA      NA        NA\n\n\nNAの扱いについては、関数により処理が異なる点に注意である。なおこれらの関数内においても、降順並び替えのためのdesc()や-演算子が利用可能である。\n\n\n\n\n\n\n\n\n関数\n処理内容\n\n\n\n\nrow_number()\n行番号\n\n\nmin_rank()\n順位(同位値には同じ順位を与える。同位値の数を含めて次の順位が考慮される)\n\n\ndense_rank()\n順位(同位値には同じ順位を与え、以降の順には連続した順位が割り振られる)\n\n\npercent_rank()\n順位のパーセント(0から1の範囲)\n\n\ncume_dist()\n順位の積み上げ値に対する割合(累積比率)\n\n\nntile()\n指定した群数に分割\n\n\n\n\n\n\n2.2.6 集計値を求める: summarise\nsummarise()は、データ全体あるいはグループに要約統計量を算出するのに役立つ。\ntreesの3つの変数に対し、平均 mean()、最大値 max()、標準偏差 sd()を求める処理は以下のようになる。\n\n# 各変数の要約統計量を求める\ntrees %>% summarise(\n  mean_girth = mean(Girth),\n  max_height = max(Height),\n  sd_volume  = sd(Volume))\n\n  mean_girth max_height sd_volume\n1   13.24839         87  16.43785\n\n\nデータに共通の項目、すなわちグループがある場合、グループごとの集計値を求めるにはsummarise()とgroup_by()を組み合わせて利用する。group_by()は引数に与えた変数をグループとして扱い、以降の処理をグループ単位で適用する。返り値はデータフレームであるがgrouped_dfというクラスが与えられている。\n\n# mtcarsデータセットをcylの値ごとにグループ化する\ngrp_mtcars <-\n  mtcars %>%\n  group_by(cyl)\n\nclass(grp_mtcars)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n例えば、mtcarsの車種のシリンダ (気筒)数ごとの1ガロンあたりの平均走行距離を求めるには次のようなコードを実行する。グループに含まれる値は水準として扱われ、ここでの水準はcylに含まれる4、6、8の3水準である。\n\n# summarise()の処理は水準ごとに出力される\ngrp_mtcars %>%\n  summarise(mean_mpg = mean(mpg))\n\n# A tibble: 3 × 2\n    cyl mean_mpg\n  <dbl>    <dbl>\n1     4     26.7\n2     6     19.7\n3     8     15.1\n\n\n一つの変数を元にグループ化を実行した場合、summarise()やmutate()関数を適用した後はグループ化が解除されるが、複数の変数からグループを作成した場合、グループの解除はungroup()を使い明示的に実行する必要がある。\n\n# 複数の変数でグループを作る\n# cylのグループ化は継続する\nmtcars %>%\n  group_by(cyl, gear) %>%\n  summarise(mean_mpg = mean(mpg))\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 3\n# Groups:   cyl [3]\n    cyl  gear mean_mpg\n  <dbl> <dbl>    <dbl>\n1     4     3     21.5\n2     4     4     26.9\n3     4     5     28.2\n4     6     3     19.8\n5     6     4     19.8\n6     6     5     19.7\n7     8     3     15.0\n8     8     5     15.4\n\n# ungroup()でグループを解除する\nmtcars %>%\n  group_by(cyl, gear) %>%\n  summarise(mean_mpg = mean(mpg)) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 3\n    cyl  gear mean_mpg\n  <dbl> <dbl>    <dbl>\n1     4     3     21.5\n2     4     4     26.9\n3     4     5     28.2\n4     6     3     19.8\n5     6     4     19.8\n6     6     5     19.7\n7     8     3     15.0\n8     8     5     15.4\n\n\n\n2.2.6.1 グループのカウント\n要素数を数える関数としてn()が提供されている。これをグループ化したデータフレームに実行すると、各グループの数を求めるのに便利である。次の処理は、cyl列でグループ化したmatcarsの各cylの値がどれだけ含まれるかを数えた物である。\n\ngrp_mtcars %>%\n  summarise(n = n())\n\n# A tibble: 3 × 2\n    cyl     n\n  <dbl> <int>\n1     4    11\n2     6     7\n3     8    14\n\n\nまた、単にグループの数を求めるという処理の場合には上記の記述を簡略化させたtally()やcount()を用いると良い。tally()とcount()、前者ではあらかじめグループ化されたデータを渡す(summarise(n = n())のラッパー関数として動作する)が、後者では引数でグループ化する変数を指定する点が異なる。2つの関数には引数sortがあり、TRUEを指定することで降順並び替えを実行する。\n\n# tallyではgroup_by()によりグループ化をあらかじめ実行しておく\nmtcars %>%\n  group_by(cyl, gear) %>%\n  tally()\n\n# A tibble: 8 × 3\n# Groups:   cyl [3]\n    cyl  gear     n\n  <dbl> <dbl> <int>\n1     4     3     1\n2     4     4     8\n3     4     5     2\n4     6     3     2\n5     6     4     4\n6     6     5     1\n7     8     3    12\n8     8     5     2\n\n# count()では引数に含めた変数をグループ化してカウントを実施する\nmtcars %>%\n  count(cyl, gear)\n\n  cyl gear  n\n1   4    3  1\n2   4    4  8\n3   4    5  2\n4   6    3  2\n5   6    4  4\n6   6    5  1\n7   8    3 12\n8   8    5  2\n\nmtcars %>%\n  count(cyl, gear, sort = TRUE)\n\n  cyl gear  n\n1   8    3 12\n2   4    4  8\n3   6    4  4\n4   4    5  2\n5   6    3  2\n6   8    5  2\n7   4    3  1\n8   6    5  1\n\n\nなお、summarise()を含め、グループ化したデータのカウントを求める関数では、返り値がデータの集約結果となっているが、add_tally()またはadd_count()を使うことで、データフレームの他の変数に影響を及ぼさずにカウント値を算出し、列として追加することができる。\n\nmtcars %>%\n  group_by(cyl) %>%\n  add_tally() %>%\n  colnames()\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\" \"n\"   \n\n# sortの並び替えがデータフレームに対して有効になる\nmtcars %>%\n  add_count(cyl, sort = TRUE) %>%\n  head()\n\n   mpg cyl  disp  hp drat   wt  qsec vs am gear carb  n\n1 18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2 14\n2 14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4 14\n3 16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3 14\n4 17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3 14\n5 15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3 14\n6 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4 14\n\n\nグループに関する補助関数として、水準数や水準ごとの件数を出力する関数が用意されている。\n\n# グループとなっている変数を確認する\ngroups(grp_mtcars)\n\n[[1]]\ncyl\n\n# グループの水準数を数える\nn_groups(grp_mtcars)\n\n[1] 3\n\n# グループの水準に含まれる件数を出力\ngroup_size(grp_mtcars)\n\n[1] 11  7 14\n\n\n\n\n2.2.6.2 グループに対するsummarise以外のデータ操作\n\nグループ化されたデータフレームに対する処理はsummarise()以外でも適用できる。引き続き、mtcarsのcylについてグループ化したデータを対象に、グループ化されたデータフレームが対象となることで挙動の変化する関数の例を見ていこう。\nまずslice()でデータの任意の行のデータを抽出する際、グループ化されたデータが対象の場合、グループの各要素の位置からデータを返却する。cylは3つの要素をもつ変数なので、次の処理を実行すると、各グループの要素の値について出力を得る。\n\n# グループ化されていないため、\n# データ全体から1行を返す\nmtcars %>% slice(1L)\n\n          mpg cyl disp  hp drat   wt  qsec vs am gear carb\nMazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n\n# 各グループのデータの先頭から1行抽出。\ngrp_mtcars %>% slice(1L)\n\n# A tibble: 3 × 11\n# Groups:   cyl [3]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n2  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n3  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n\n\n\n# グループの要素から各1行返す\ngrp_mtcars %>% slice(n())\n\n# A tibble: 3 × 11\n# Groups:   cyl [3]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21.4     4   121   109  4.11  2.78  18.6     1     1     4     2\n2  19.7     6   145   175  3.62  2.77  15.5     0     1     5     6\n3  15       8   301   335  3.54  3.57  14.6     0     1     5     8\n\n# 各グループからサイズ数のデータを無作為に抽出\ngrp_mtcars %>% sample_n(2)\n\n# A tibble: 6 × 11\n# Groups:   cyl [3]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  21.5     4  120.    97  3.7   2.46  20.0     1     0     3     1\n2  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n3  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n4  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n5  15.8     8  351    264  4.22  3.17  14.5     0     1     5     4\n6  15.2     8  276.   180  3.07  3.78  18       0     0     3     3\n\n\n各グループの最大値あるいは最小値を求めるという処理は、次のようにarrange()と組み合わせることで実現可能になる。\n\n# wtが小さい順に各グループから2行ずつ抽出\ngrp_mtcars %>%\n  arrange(wt) %>%\n  slice(1:2L)\n\n# A tibble: 6 × 11\n# Groups:   cyl [3]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n2  30.4     4  75.7    52  4.93  1.62  18.5     1     1     4     2\n3  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n4  19.7     6 145     175  3.62  2.77  15.5     0     1     5     6\n5  15.8     8 351     264  4.22  3.17  14.5     0     1     5     4\n6  15.2     8 304     150  3.15  3.44  17.3     0     0     3     2\n\n\n同様にsample_n()やdistinct()などのデータ抽出に関する関数もグループ化の影響を受ける。\n\nmtcars %>% distinct(carb, .keep_all = TRUE) %>%\n  nrow()\n\n[1] 6\n\n# 各グループのcarbの値がユニークに処理される\ngrp_mtcars %>% distinct(carb, .keep_all = TRUE) %>%\n  nrow()\n\n[1] 9\n\n\nベクトルの値を前後にずらすlead-lagもグループ化されたデータが対象の際に挙動が変化する。これを利用して、1個体分の高さ成長量を求めた処理を植物データ全体に適用可能になる。\n\ndf_long %>%\n  arrange(sp, period) %>%\n  group_by(sp) %>%\n  mutate(growth = height - lag(height)) %>%\n  ungroup()\n\n# A tibble: 120 × 4\n   sp    period height  growth\n   <chr> <chr>   <dbl>   <dbl>\n 1 a_1   201504   147. NA     \n 2 a_1   201505   148.  0.160 \n 3 a_1   201506   149.  1.17  \n 4 a_1   201507   149.  0.0800\n 5 a_1   201508   149.  0.220 \n 6 a_1   201509   149.  0.170 \n 7 a_1   201510   149.  0.160 \n 8 a_1   201511   149.  0.150 \n 9 a_1   201512   150.  0.110 \n10 a_1   201601   150.  0.0400\n# … with 110 more rows\n\n\n\n\n\n2.2.7 複数列へのデータ操作関数の適用: _at / _if / _all\ndplyrを使ったデータ操作の関数群では、例えば「共通の接頭語をもつ変数」「文字列である変数」「すべての変数」というような複数の列を同時に処理するための効率的な方法が提供されている。*_at()、*_if()、*_all()である。これらの複数列への処理が可能な関数の原形は、データ操作の基本として本章で取り上げてきたmutate()、transmute()、select()、filter()、summarise()、group_by()、arrange()がある。\n例えば全ての変数の平均値をmean()を使って求めるという処理をsummarise()関数を使うと次のような記述となるが、summarise()の処理を全変数に適用するsummarise_all()を使うことで簡略化できる。\n\n# 各変数の平均値を求める\ntrees %>%\n  summarise(mean_girth = mean(Girth),\n            max_height = mean(Height),\n            sd_volume  = mean(Volume))\n\n  mean_girth max_height sd_volume\n1   13.24839         76  30.17097\n\n\n\n# summarise_allではデータフレームの全列に関数を適用する\ntrees %>%\n  summarise_all(.funs = mean)\n\n     Girth Height   Volume\n1 13.24839     76 30.17097\n\n\n\n# 文字列で指定しても良い\ntrees %>%\n  summarise_all(.funs = \"mean\")\n\n# 関数名を.funsに指定するとエラーになる\ntrees %>%\n  summarise_all(.funs = mean())\n# Error in mean.default() : argument \"x\" is missing, with no default\n\n*_all()では関数内の引数として、対象のデータに含まれる全ての列に適用する関数名を文字列で指定し、その関数に適用する引数を記述する。すなわち、先の例で各列に適用した関数はmean()であったがmean()の引数trimを有効にするには次のようにする必要がある。\n\n# mean()を全ての列に適用するが、\n# meanは文字として定義する\ntrees %>%\n  summarise_all(mean, trim = 2)\n\n  Girth Height Volume\n1  12.9     76   24.2\n\n\n一方でfuns()を使うことで関数呼び出しのリストを生成することも可能である。この場合、表現式はラムダ式で記述する。\n\n# 次の3つの実行結果は全て等しい\ntrees %>%\n  summarise_all(funs(mean(x = ., trim = 2)))\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nPlease use a list of either functions or lambdas: \n\n  # Simple named list: \n  list(mean = mean, median = median)\n\n  # Auto named with `tibble::lst()`: \n  tibble::lst(mean, median)\n\n  # Using lambdas\n  list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n  Girth Height Volume\n1  12.9     76   24.2\n\ntrees %>%\n  summarise_all(function(x) {\n  mean(x, trim = 2)\n})\n\n  Girth Height Volume\n1  12.9     76   24.2\n\ntrees %>%\n  summarise_all(funs(mean), trim = 2)\n\n  Girth Height Volume\n1  12.9     76   24.2\n\n\nいずれの実行結果も各列の変数名を保持しているが、変数名はfuns(変数名 = 適用する関数名)の形式で調整できる。\n\n# 処理後の列名を変更する\ntrees %>%\n  summarise_all(funs(heikin = mean), trim = 2)\n\n  Girth_heikin Height_heikin Volume_heikin\n1         12.9            76          24.2\n\n\n*_all()は、データフレームがもつすべての変数に対し任意の関数を適用するが、変数のデータ型が異なる場合には意図しない出力結果となることがあるので注意が必要である。例えば、先の例ではmean()をすべての変数に実行したが、文字列型の変数が含まれている場合には欠損値や警告が出力される。このような時には、より柔軟に関数の適用範囲を指定する*_if()や*_at()を利用すると良い。\n*_if()は、関数を適用する変数の条件を指定し、その条件に合うものに任意の関数を処理できるようになっている。また*_at()は、適用する変数を明示的あるいは補助関数 (変数選択の補助関数を参考)をvars()と組み合わせて指定すると良い。\n次の例はselect_at()、select_if()を利用して条件に従う変数を選択するものである。1番目はvars()を用いて任意の変数名をベクトルとして与え、一致する変数名の列を選択する、という処理になる。2番目の処理は、vars()内部で変数選択の補助関数の一種end_with()により条件指定を実行している。最後は関数を定義しているが、ここで引数xには各変数がベクトルで与えられる。この例では変数のベクトルの合計値が1000以下となる変数を選択するようにしている。ここで注意なのは、対象のデータに数値以外のデータ型の変数が含まれる場合、この処理はエラーになることである。\n\n# 選択する変数名を直接指定する\nmtcars %>%\n  select_at(vars(c(\"mpg\", \"cyl\", \"gear\"))) %>%\n  names()\n\n[1] \"mpg\"  \"cyl\"  \"gear\"\n\n# end_with()を使い、pで終わる変数を選択する\nmtcars %>%\n  select_at(vars(ends_with(\"p\"))) %>%\n  names()\n\n[1] \"disp\" \"hp\"  \n\n# 各列の合計値が1000以下となる変数を選択\ntrees %>%\n  select_if(function(x) sum(x) < 1000) %>%\n  names()\n\n[1] \"Girth\"  \"Volume\"\n\n\nsummarise_all()などの関数は、適用する関数の形式（*_all()、_at()、_if()）に応じて、第二引数以降で指定する引数が異なっている。特定の変数を対象とする*_atでは.vars、条件に従う変数を対象にする*_ifでは.predicateである。これらはいずれも変数の適用範囲を指定するのに用いられる。そして処理する関数を与える。データ操作の「文法」としては、対象のデータを指定し、その変数の適用範囲を決めたのちに処理する内容を述べる、という具合になる。\n\nmtcars %>%\n  select_at(vars(\"mpg\", \"vs\", \"am\")) %>%\n  # vsからamまでを論理値に変換\n  mutate_at(vars(vs:am), as.logical) %>%\n  head(10)\n\n                   mpg    vs    am\nMazda RX4         21.0 FALSE  TRUE\nMazda RX4 Wag     21.0 FALSE  TRUE\nDatsun 710        22.8  TRUE  TRUE\nHornet 4 Drive    21.4  TRUE FALSE\nHornet Sportabout 18.7 FALSE FALSE\nValiant           18.1  TRUE FALSE\nDuster 360        14.3 FALSE FALSE\nMerc 240D         24.4  TRUE FALSE\nMerc 230          22.8  TRUE FALSE\nMerc 280          19.2  TRUE FALSE\n\n\n\n\n2.2.8 データの結合\nこれまでdplyrのデータ操作関数の振る舞いについて紹介してきたが、次は2つのデータを結合する処理について見ていこう。データフレームの結合には、bind_rows()とbind_cols()を用いる方法と、join関数群を使う方法とがある。\n\n2.2.8.1 行・列方向による結合: bind_rows / bind_cols\nまずbind_rows()、bind_cols()だが、この関数は結合するデータの方向性、すなわち行を増やすのか、列を追加するのか、により使い分ける。bind_rows()では結合対象のデータの変数名が共通である場合にデータを追加する。共通でない変数名が含まれる場合、それらの列をもたない行については欠損値が与えられる。一方bind_cols()は列の結合を実施する関数であり、与えるデータの行数が一致していなければいけない。\ntreesを使って結合の例を示そう。まずtreesを10行ごとに分割したオブジェクトを作成し、これをbind_rows()で結合する。\n\ntrees_x <- trees[1:10, ]\ntrees_y <- trees[11:20, ]\ntrees_z <- trees[21:nrow(trees), ]\n\nbind_rows(trees_x, trees_y) %>%\n  dim()\n\n[1] 20  3\n\n\nbind_rows()やbind_cols()では、引数に与えたデータを対象に結合の処理を行うが、これらのデータ数に制約はない。つまり複数のデータを一度に結合する処理も可能なのである。\n\nbind_rows(trees_x, trees_y, trees_z) %>%\n  dim()\n\n[1] 31  3\n\n\n次にbind_cols()の例であるが、結合の方向が異なるだけでbind_rows()と差異はない。treesの列ごとに結合を行おう。\n\nbind_cols(trees[1], trees[2], trees[3]) %>% dim()\n\n[1] 31  3\n\n\n\n\n2.2.8.2 リレーショナルデータの結合: _join\n\n\n\n\ndplyrパッケージの*_join関数群を使ったデータフレームの結合は、2つのデータのいずれかに含まれる値を結合したり、共通する項目を除外したりと応用範囲の広い結合関数である。基本は引数x、yに共通の変数をもった対象のデータフレームを指定し、by引数で変数名を記述する、という形式になる。\n*_join関数群の例を見ていくために用意した2つのデータを用意しよう。df1は変数:x”と”y”をもつ。またdf2は変数”x”、“z”をもっている。どちらも3行2列のデータフレームであり、共通する変数は”x”である。\n\n# 模擬のデータフレームを生成する\n(df1 <- tibble(x = c(\"A\", \"B\", \"C\"),\n                   y = 1:3))\n\n# A tibble: 3 × 2\n  x         y\n  <chr> <int>\n1 A         1\n2 B         2\n3 C         3\n\n(df2 <- tibble(x = c(\"A\", \"B\", \"D\"),\n                   z = c(TRUE, FALSE, TRUE)))\n\n# A tibble: 3 × 2\n  x     z    \n  <chr> <lgl>\n1 A     TRUE \n2 B     FALSE\n3 D     TRUE \n\n\n\n*_join関数群には2種類の結合形式があるが、一つの結合形式は、left_join()、inner_join()など、別テーブルと共通する変数を照合し、一致する行に該当する別テーブルの変数を追加する結合の方法である。\n\nleft_join(x = df1, y = df2, by = \"x\")\n\n# A tibble: 3 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 B         2 FALSE\n3 C         3 NA   \n\n\n\n\n\n\n\n\n左外部結合 left_join\n\n\nこの例ではleft_join()で変数xをbyに指定している。2つのデータのx変数で共通な値は”A”と”B”である。この共通の値が含まれるdf2の変数をdf1に結合するのがleft_join()の働きとなる。\n結合を行う際、2つのデータフレームを紐付ける一つ以上の変数が必要であることが*_join()の働きを理解するのに重要である。この役割は引数byにより指定する。またby引数に指定する変数のデータ型は2つのデータで共通でなければならないという制約がある(変数名が同じであっても文字列型と数値型の変数では失敗する)。\ndplyrではデータフレームに共通する変数名を自動的に検出し、それを結合に利用する。そのためby引数の指定は必須ではない。しかし後でコードを見返したときのため、どの変数をキーとしたかをコードとして記述しておくことが好ましい。\n\n# byの指定をしなくても\n# 自動的に結合が行われる\ndf1 %>% left_join(df2)\n\nJoining, by = \"x\"\n\n\n# A tibble: 3 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 B         2 FALSE\n3 C         3 NA   \n\n\n結合する変数名がデータフレーム間で異なる場合には、引数byの値をc(col_a = COL_A)のように互いの変数名を紐付ける指定をする。先ほどleft_join()を用いた結合結果を示したdf1の変数xをrename()を使って変更し、その処理を見てみよう。\n\n# 共通する変数名が存在しないためエラーになる\ndf1 %>%\n  rename(X = x) %>%\n  left_join(df_y)\n# Error: `by` required, because the data sources have no common variables\n\n\ndf1 %>% rename(X = x) %>%\n  left_join(df2, by = c(\"X\" = \"x\"))\n\n# A tibble: 3 × 3\n  X         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 B         2 FALSE\n3 C         3 NA   \n\n\n*_join関数群で扱う対象データのうち、引数xに与えられるデータを特にマスタと呼ぶ。left_join()はマスタに指定したデータに変更を加えるが、マスタに含まれる情報が失われることはない。次に、*_joinの複数の種類を扱うが、*_join関数群の中で利用頻度の高い関数となるだろう。\nleft_join()と同様の働きをもつright_join()があるが、これは引数yのデータをマスタとして扱う関数である。そのため次の結果はdf1 %>% left_join(df2)と同じデータを返すが、注意すべきなのは変数の並びがdf2に含まれる変数が先に与えられている点である。\n\ndf2 %>% right_join(df1, by = \"x\")\n\n# A tibble: 3 × 3\n  x     z         y\n  <chr> <lgl> <int>\n1 A     TRUE      1\n2 B     FALSE     2\n3 C     NA        3\n\n\n\n\n\n\n\n\n右外部結合 right_join\n\n\n\ndf1 %>% inner_join(df2, by = \"x\")\n\n# A tibble: 2 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 B         2 FALSE\n\ndf1 %>% full_join(df2, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 B         2 FALSE\n3 C         3 NA   \n4 D        NA TRUE \n\n\ninner_join()は引数byで指定された変数の値をもつ行について結合を実施する。full_join()は2つのデータに含まれる全ての変数を結合する。\n\n\n\n\n\n\n内部結合 inner_join\n\n\n\n\n\n\n\n\n完全外部結合 full_join\n\n\nleft_join()、right_join()、full_join()のような結合形式は外部結合として知られる。外部結合で一致しない行に追加される変数の値には欠損値が与えられる。一方、inner_join()は内部結合と呼ばれる。\nleft_join()等の結合を利用する際、値の重複がある場合には注意が必要である。次の例は変数xを共通してもつ2つのデータの結合であるが、df2の変数xには”A”の値が複数出現しており、それぞれが異なるzの値をもっている。この状態だとx = \"A\", z = TRUE、x = \"A\", z = FALSEの2パターンである。このように結合する値がユニークに扱えない時、結合は一致する観測値のすべての組み合わせを返す。この機能は複数変数の組み合わせからなるデータを生成するのに役立つだろう。\n\ndf1 <- tibble(x = c(\"A\", \"B\", \"C\"),\n                  y = 1:3)\ndf2 <- tibble(x = c(\"A\", \"B\", \"D\", \"A\"),\n                  z = c(TRUE, FALSE, TRUE, FALSE))\n\ndf1 %>% left_join(df2, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 A         1 FALSE\n3 B         2 FALSE\n4 C         3 NA   \n\ndf1 %>% inner_join(df2, by = \"x\")\n\n# A tibble: 3 × 3\n  x         y z    \n  <chr> <int> <lgl>\n1 A         1 TRUE \n2 A         1 FALSE\n3 B         2 FALSE\n\n\n\n*_join()群のもう一つの結合形式は、これまで見てきたleft_join()などの関数が変数を追加する結合であったのに対して変数には影響することはない。マスタとするデータの変数は保持されるが、行について影響する結合となる。このような結合は、結合結果の不一致を確認するのに役立つだろう。\nsemi_join()は2つのデータに含まれる、引数byで指定した変数で共通する行を抽出する。anti_join()はマスタにのみ含まれる行を抽出する。\n\ndf1 %>% semi_join(df2, by = \"x\")\n\n# A tibble: 2 × 2\n  x         y\n  <chr> <int>\n1 A         1\n2 B         2\n\ndf1 %>% anti_join(df2, by = \"x\")\n\n# A tibble: 1 × 2\n  x         y\n  <chr> <int>\n1 C         3\n\n\n\n\n\n\n\n\n準結合 semi_join\n\n\n\n\n\n\n\n\n逆結合 anti_join"
  },
  {
    "objectID": "ch2.html#まとめ",
    "href": "ch2.html#まとめ",
    "title": "2  データ整形および操作: tidyr, dplyrパッケージの基礎",
    "section": "2.3 まとめ",
    "text": "2.3 まとめ\n\n多様なデータの表現形式と、コンピュータ処理に適したtidyデータへの相互変換の方法を学んだ。tidyrパッケージを使うことで、データを柔軟に変形することが可能になる。\ndplyrはデータ操作を「文法」として表現するパッケージで、基本的なデータ処理である、選択、抽出、並び替え、集計、加工の5工程を実行する関数をそれぞれ提供する。\ndplyrはtidyデータの利用を前提としており、関数内部ではデータフレームの変数名を指定した処理が中心となる。\ndplyrが提供する結合関数は、変数の対応関係を考慮したリレーショナルな処理も行える。"
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  表形式のデータソースからのデータ取得",
    "section": "",
    "text": "Rにはデータソースとして一般的な.csvや.txtなどのテキストファイルの入出力を行う関数が標準的に備わっている。また、パッケージを利用することでより多様なデータソース、例えばデータベース、地理空間データや画像、音声ファイルに対応することができる。標準の読み込み関数の機能改善を施したパッケージもある。ここではそうしたデータ読み込みパッケージの中から、データ分析の現場で汎用的なテキストファイルおよび表計算ソフトのエクセルファイルをR上で扱うものについて解説する。\nこれらのパッケージはいずれもtidyverseに含まれ、読み込みの挙動を制御するオプションが豊富に備わっている。また読み込まれたオブジェクトはデータフレームであるが、追加のクラスとしてtbl\\_dfが与えられる。"
  },
  {
    "objectID": "ch3.html#readr",
    "href": "ch3.html#readr",
    "title": "3  表形式のデータソースからのデータ取得",
    "section": "3.1 テキストファイル",
    "text": "3.1 テキストファイル\nテキストファイルは、特別なソフトを使わずに幅広い環境で利用可能な形式のファイルである。テキストファイルの一種であるカンマ(,)区切りのテキストファイル(.csv)は次のようにデータを格納する。この形式で記録されるデータの特徴は、行ごとにレコードを保持し、各項目をカンマによって区切っていくことだ。\ndate, value, type\n\"2017-04-29\", 14, b\n\"2017-04-29\", 8, a\n\"2017-04-30\", 21, b\n\"2017-04-30\", 4, c\n\"2017-04-30\", 9, c\n...\nテキストファイルに記録されたデータをRに読み込むためにreadrパッケージを用いよう。readrパッケージはその名から想像される通り、R上でさまざまな種類のデータを読み込むためのパッケージである。多様な形式のファイルに対応できるよう、豊富な関数が用意されているが、読み込み関数はread_*という接頭辞をもつ。*で示した部分には対象のデータ形式が入る。つまりcsvファイルに対してはread_csv()を使うことになる。関数に指定するのは、接尾辞となっている形式のファイルまたは、ファイルの形式に従った文字列である。\n\nlibrary(magrittr)\nlibrary(readr)\n\n\n# 本書のサンプルデータcsvファイルを読み込む\ndf_sns <- read_csv(file = \"data/sns.csv\")\n\nRows: 1000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): post_id, address, place_name, user_id, nationality\ndttm (1): post_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nこのパッケージのファイル読み込み・書き込みの関数は、Rの標準的な関数よりも読み込みが高速である。また、読み込まれたデータはtbl\\_dfクラスが与えられ、出力時に各変数のデータ型の表示が行われる。加えて、ファイルに含まれる各変数のデータ型の指定がより的確、柔軟に行える点が特徴である。この特徴について詳しく見ていこう。\n\n3.1.1 readrパッケージで扱えるデータ形式\nreadrのファイル読み込みでは、コンピュータに保存されているファイルだけでなく、インターネット上のファイルや圧縮ファイルも扱える。この場合、他のファイルの読み込み方法と変わらず、第一引数に対象のファイルがあるパスを指定する。readrで扱うファイルの種類と対応する読み込み関数について表XXに整理した。インターネット上にあるファイルの場合にはコンピュータ内のパスではなくURLを指定することとなる。\nファイルではなく、文字列を直接データとして扱うこともできる。この場合、利用する関数と区切り文字の関係に注意する必要がある。\n\n# カンマ区切りのデータを読み込む場合\n# read_csv()を利用する\nread_csv('date, value, type\\n\n\"2017-04-29\", 14, b\\n\n\"2017-04-29\", 8, a\\n\n\"2017-04-30\", 21, b\\n\n\"2017-04-30\", 4, c\\n\n\"2017-04-30\", 9, c')\n\nRows: 5 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): type\ndbl  (1): value\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# タブ区切りは\\tで表現される。\nread_tsv(\"x\\ty\\n1\\t2\\n3\\t4\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# 任意の区切り文字を使用している場合には、read_delim()のdelim引数で区切り文字を指定する。\nread_delim(\"x y\\n1 2\\n3 4\", delim = \" \")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nread_delim(\"x.y\\n1.2\\n3.4\", delim = \".\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \".\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\n読み込み関数\nファイル形式\n\n\n\n\nread_lines(), read_lines_chunked()\ntxt, csvなど\n\n\nread_tsv()\ntxt\n\n\nread_delim(), read_delim_chunked()\ntxt\n\n\nread_table(), read_table2()\ntxt\n\n\nread_csv(), read_csv2(), read_csv_chunked()\ncsv\n\n\nread_fwf()\ntxt (固定長)\n\n\nread_log()\nlog\n\n\nread_rds()\nrds\n\n\n\n\n\n3.1.2 データ型の判定と列指定読み込み\nreadrの関数を使ったデータ読み込みが実行されると、コンソール上で対象のデータがもつ変数と、各変数をどのように処理して読み込んだかが出力される。これはreadrがデータに含まれる各列の先頭100行分を読み、推測されたデータ型を自動的に判別し、適したデータ型への変換を行っている結果を示している。\n先の例の出力結果を改めて確認しよう。\nParsed with column specification:\ncols(\n  post_id = col_character(),\n  post_time = col_datetime(format = \"\"),\n  address = col_character(),\n  place_name = col_character(),\n  user_id = col_character(),\n  nationality = col_character()\n)\n\ncols(...)の中身がデータに対する処理の内容を記述しており、各行が対象データの列に対応する。cols(...)の中で6行分の出力がなされていることは対象データが6列からなることを示している。各行の内容は、変数名 = col_*()という形式になっており、左辺が変数名、右辺が読み込み時に適用されたデータ型の名を冠した関数である。この場合、文字列 (col_character())および日付型 (col_datetime())の2種類のデータ型が適用されている。\nRの標準関数を利用した場合、読み込み後の変数がどのデータ型として扱われているかは利用者が出力を行いながら確認する必要があるが、readrでは読み込み時にデータ型の出力を行う。そのため、例えば日付を示す変数が文字列型として処理されている場合などの非意図的なデータ型の変換トラブルを未然に防ぐことができるだろう。また文字列のデータを因子型として扱わない点も大きな違いとなる。これは標準的に備わっている関数read.csvの実行結果と比較するとよくわかる。\n\n# readrで読み込んだデータは、時間データに対して自動的な変換が行われている\ndf_sns$post_time[1]\n\n[1] \"2016-04-01 09:37:43 UTC\"\n\nclass(df_sns$post_time[1])\n\n[1] \"POSIXct\" \"POSIXt\" \n\n# 標準の関数では同じpost_id列の値が因子型として扱われてしまっている\ndf_sns2 <- read.csv(\"data/sns.csv\")\ndf_sns2$post_time[1]\n\n[1] \"2016-04-01T09:37:43Z\"\n\nclass(df_sns2$post_time[1])\n\n[1] \"character\"\n\n\nパッケージreadrを使ったデータ読み込みではデータ型が自動的に適切に処理される例を示したが、col_types引数に渡す値を変更することで変数のデータ型をユーザが指定した形式で処理させることができる。\ncol_types引数にはcols()を使って列の処理方法を指定することになる。これは先に見たreadrの読み込み時にコンソールへ出力されるメッセージと同様のものである。すなわち、自動的に読み込みを行う場合と次のデータ型を指定した読み込みでは同じデータの読み込み結果となる。\n\nread_csv(\"data/sns.csv\")\nread_csv(\"data/sns.csv\",\n         # col_types引数にデータ型を指定して読み込む変数名と\n         # その処理に用いる関数を指定する\n         col_types = cols(\n           post_id     = col_character(),\n           post_time   = col_datetime(format = \"\"),\n           address     = col_character(),\n           place_name  = col_character(),\n           user_id     = col_character(),\n           nationality = col_character()))\n\ncols()内でどのようなデータ型で処理するかを定義しなかった列は、readrによって自動的にデータ型が判定される。そのため利用時には特定の列だけをcols()で定義しておくと良いだろう。対して、特定の列を読み込みの対象外にすることもできる。col_skip()はそのための関数である。\ncol_types引数で指定するcols()内部で定義可能なデータ型として、論理値、数値（整数、実数）、文字列、日時および時間型などがあり、これらは表XXに示すような対応関係にある。また、各データ型の省略表記も用意されており、これを関数の代わりに文字列として指定することもできる。\n\n\n\n\nデータ型\n関数\n省略時の表記\n\n\n\n\n論理値型\ncol_logical()\nl\n\n\n数値型(数字と.以外は無視する)\ncol_number()\nn\n\n\n整数型\ncol_integer()\ni\n\n\n実数型\ncol_double()\nd\n\n\n文字列型\ncol_character()\nc\n\n\n日付型 (Y-m-d表記)\ncol_date(format)\nD\n\n\n日時型 (ISO8601形式)\ncol_datetime(format, tz)\nT\n\n\n時間型\ncol_time(format)\nなし\n\n\n因子型\ncol_factor(levels, ordered)\nなし\n\n\n読み込み対象としない\ncol_skip()\n_, -\n\n\n\ncols()は関数であるので、読み込みを実行するread_*()とは独立してオブジェクト化しておくと、同様の構造を持ったデータに対して使いまわせる。次の例は、あらかじめ各列をどのようなデータ型として扱うかを定義しておくもので、一部の変数については_によって読み込みから除外している。\n\ndata_vars <-\n  cols(post_id     = \"_\",\n       post_time   = \"c\",\n       user_id     = \"c\",\n       address     = \"_\",\n       place_name  = \"_\")\n\nread_csv(\"data/sns.csv\",\n         # 指定を省略した変数は自動的に判定されたデータ型が適用される\n         col_types = data_vars)\n\n# A tibble: 1,000 × 3\n   post_time            user_id    nationality\n   <chr>                <chr>      <chr>      \n 1 2016-04-01T09:37:43Z A-8vImElYu US         \n 2 2016-04-01T20:54:12Z A-zpNoCiF2 TH         \n 3 2016-04-01T22:01:57Z C-hbtHidIz HK         \n 4 2016-04-01T22:13:21Z B-QUU841zl ID         \n 5 2016-04-02T08:18:09Z B-JzaPVHpY US         \n 6 2016-04-02T11:16:27Z A-G6sGnoFV AU         \n 7 2016-04-02T20:06:11Z C-NxooWvcI SG         \n 8 2016-04-02T22:47:24Z C-xeQ41RtH CN         \n 9 2016-04-03T00:26:41Z B-FkHjECs0 HK         \n10 2016-04-03T01:36:18Z C-Bd0DCcdp TH         \n# … with 990 more rows\n\n\nまた、特定の列だけを読み込むcols_only()も提供されている。扱う列が多いデータの場合にはcols()ではなくこちらの関数を利用すると良い。\n\n\n\n\n\n\n3.1.3 ロケール\nロケールとは多種多様な言語や単位、時間といった表記などの総称であり、国や地域、利用環境によって異なるロケールが設けられている。例えば、日付の要素である曜日や月は国家によって表現が異なることが多い。また日本では時間を表現するために「12時30分」のような表記が行われるが、国際的に一般的なのは「12:30」といった形式だろう。しかし「12時30分」も「12:30」も同じものを表現している。\n日本語文字列を保存したファイルで生じる文字化けもロケールに関する問題である。これは実行環境のオペレーションシステム (OS)によってデフォルトの文字エンコードが異なっているために生じる。Windows環境では、多くの場合、Microsoft社によって定義された”CP932”と呼ばれる日本語用のエンコードが与えられる。その他、日本語を取り扱うためにR内で利用可能で整備されているエンコード形式として、“EUC-JP”、“SJIS”、“SHIFT_JIS”、“UTF8”などがある。\n次の例は、Windows環境で作成されたcsvファイルをMacやUbuntuといったUNIX環境で開いた場合に表示されるエラーメッセージである。\n\nread_csv(\"data/housekeeping_cp932.csv\")\n# Error in make.names(x) : invalid multibyte string at '<93><fa><95>t'\n\nRでは、ファイル読み込み関数の実行時にファイルに対するエンコードを指定して文字化けを回避できる。readrにおいては locale引数でlocale()という関数を実行する。locale()は、ロケールに関する設定を定義する関数であり、その中にエンコードを指定するencoding引数が用意されている。すなわち、先のファイルを適切に読み込むには次のコードを実行する必要がある。\n\n# readrパッケージではlocale引数でエンコードを指定したlocale()実行する\nread_csv(\"data/housekeeping_cp932.csv\", \n         locale = locale(encoding = \"cp932\"))\n\nロケールの指定は文字列のエンコードだけでなく、日付・時間に関しても指定が可能である。次の例は、同一のファイルに対して時間のロケールを変更した処理である。\n\nread_csv(\"data/sns.csv\", \n              locale = locale(tz = \"UTC\"),\n              col_types = cols_only(post_time = \"T\"))\n\n# A tibble: 1,000 × 1\n   post_time          \n   <dttm>             \n 1 2016-04-01 09:37:43\n 2 2016-04-01 20:54:12\n 3 2016-04-01 22:01:57\n 4 2016-04-01 22:13:21\n 5 2016-04-02 08:18:09\n 6 2016-04-02 11:16:27\n 7 2016-04-02 20:06:11\n 8 2016-04-02 22:47:24\n 9 2016-04-03 00:26:41\n10 2016-04-03 01:36:18\n# … with 990 more rows\n\nread_csv(\"data/sns.csv\", \n         locale = locale(tz = \"Asia/Tokyo\"),\n         col_types = cols_only(post_time = \"T\"))\n\n# A tibble: 1,000 × 1\n   post_time          \n   <dttm>             \n 1 2016-04-01 18:37:43\n 2 2016-04-02 05:54:12\n 3 2016-04-02 07:01:57\n 4 2016-04-02 07:13:21\n 5 2016-04-02 17:18:09\n 6 2016-04-02 20:16:27\n 7 2016-04-03 05:06:11\n 8 2016-04-03 07:47:24\n 9 2016-04-03 09:26:41\n10 2016-04-03 10:36:18\n# … with 990 more rows\n\nread_csv(\"data/sns.csv\", \n         locale = locale(tz = \"US/Arizona\"),\n         col_types = cols_only(post_time = \"T\"))\n\n# A tibble: 1,000 × 1\n   post_time          \n   <dttm>             \n 1 2016-04-01 02:37:43\n 2 2016-04-01 13:54:12\n 3 2016-04-01 15:01:57\n 4 2016-04-01 15:13:21\n 5 2016-04-02 01:18:09\n 6 2016-04-02 04:16:27\n 7 2016-04-02 13:06:11\n 8 2016-04-02 15:47:24\n 9 2016-04-02 17:26:41\n10 2016-04-02 18:36:18\n# … with 990 more rows\n\n\nlocale()で設定可能なロケールについて、表XXにまとめた。またlocale()を引数なしで実行すると現在の実行環境でのロケール設定が出力される。これらのロケール設定は、ファイル読み込み時のエンコード以外に、日付・時間データや文字列データを扱う場合に有効となる。これらの話題については本書の該当する章で議論する。\n\ndate_names\n\n日付に関するロケールを言語レベルで指定\n\ndate_format\n\n日付に関するロケールのフォーマット\n\ndecimal_mark\n\n小数点を区切るための記号を指定\n\ntz\n\nタイムゾーンの指定。Rで利用可能な時間帯の名称はOlsonNames()で確認できる\n\nencoding\n\n文字列のエンコード形式\n\n\n\n# 現在のロケール設定を出力\nlocale()\n\n<locale>\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n<date_names>\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM\n\n# 日本で利用される曜日、月、時間に関する設定を表示\nlocale(date_names = \"ja\")\n\n<locale>\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n<date_names>\nDays:   日曜日 (日), 月曜日 (月), 火曜日 (火), 水曜日 (水), 木曜日 (木), 金曜日\n        (金), 土曜日 (土)\nMonths: 1月, 2月, 3月, 4月, 5月, 6月, 7月, 8月, 9月, 10月, 11月, 12月\nAM/PM:  午前/午後\n\n# 日本で一般的な「年月日」の形式を日付として扱う\nparse_guess(\"2017年6月23日\", \n            locale = locale(date_format = \"%Y年%m月%d日\")) %>% \n  class()\n\n[1] \"Date\""
  },
  {
    "objectID": "ch3.html#readxl",
    "href": "ch3.html#readxl",
    "title": "3  表形式のデータソースからのデータ取得",
    "section": "3.2 エクセルファイル",
    "text": "3.2 エクセルファイル\n表計算アプリケーションで用いられるバイナリファイルもまた、頻繁に利用されるデータ形式である。Rでもファイル形式が.xlsおよび.xlsxからなるエクセルファイルを読み書きするパッケージが多く開発されている。これらのパッケージは表XXに示すように特徴が異なる。\n\n\n\n\n\n\n\n\n\n\n\nパッケージ\n読み込み\n書き込み\nシート名の取得\n特徴\n\n\n\n\nreadx\nreax_excel()\n-\nexcel_sheets()\n読み込みのみ。列のデータ型を細かく指定できる\n\n\nXLConnect\nloadWorkbook(), readWorksheetFromFile()\nwriteWorksheet()\ngetTables()\nファイルをデータベースのように読み書きすることができ、さらにはセルのスタイルなどの調整も可能。ただし、別途Javaがインストールされている必要がある\n\n\ngdata\nread.xls()\n(write.fwf())\nsheetNames()\n読み込みだけが可能。Rでのデータ操作に関わる豊富な機能を備えた関数がある\n\n\nopenxlsx\nread.xlsx()\nwrite.xlsx()\nsheets()\nJavaがインストールされていない環境でも利用可能\n\n\nxlsx\nread.xlsx(), read.xlsx2()\nwrite.xlsx()\n-\nセルの編集や加工ができる\n\n\nrepmis\nsource_XlsxData()\n-\n-\nURLを指定してファイルを読み込みことが可能\n\n\nrio\nreadxl::read_excel(), openxlsx::read.xlsx()\nopenxlsx::write.xlsx()\n-\n上記パッケージに実装された各関数へのラッパーとなっている\n\n\n\n本書では、このうちreadxlパッケージを紹介する。このパッケージは C++で実装されたエクセルファイルの読み込み ライブラリを導入しており、Javaがインストールされていない環境でも動作する。ただしエクセル形式での保存には対応していないため、エクセルファイルとして書き込みを実行するには、別途writexlパッケージを利用することになる。\n\nlibrary(readxl)\n\nreadxlでメインとなる関数は2つある。.xls、.xlsxファイルをデータフレームとして読み込むread_excel()と、シート名を取得するexcel_sheets()である。read_excel()はファイル形式を自動的に判別した読み込みを行う。一方でエクセルファイルの個々の形式に対応した関数、read_xls()(こちらはxls)、read_xlsx()(xlsx形式のファイルに対応)も用意されている。また、後述のようにread_excel()では読み込み対象のエクセルファイル内のシートを指定する必要があるため、適宜excel_sheets()を実行してファイル中に含まれるシートの一覧を確認することで、ファイルを開いて確認するという手間を省くことができる。read_excel()には読み込み時のオプションが豊富に用意されている。これらはエクセルファイルで特徴的なシートやセルに適用できるものが多い。以下にread_excel()の主要な引数をまとめるが、col_types引数はreadr::read_*のものとは挙動が異なるので注意が必要である。\n\npath\n\nxls、xlsx形式ファイル(およびそのパス)を与える。\n\nsheet\n\n読み込み対象のシート名または番号を指定する。初期値として最初のシートが読み込まれる。\n\nrange\n\n読み込み時の範囲指定を行うオプション。B3:D66のようにエクセルでのセルの選択形式が可能。\n\ncol_names\n\n初期値ではシートの第一行を変数名として扱うが、同じ長さのベクトルから変数名を定義することができる。\n\ncol_types\n\nR上での各変数のデータ型を指定する。初期値では自動的に推定されたデータ型が適用される (全ての変数に対してguessが適用される)。論理型 logical、数値 numeric、日付 date、文字列 textと、読み込みから除外する skip。一つの変数に複数のデータ型が含まれる場合、listとして読み込むのが適切である。\n\nna\n\n読み込み時に欠損値として処理する値。エクセルファイル上では欠損に空白を用いることが多いため、初期値で空白が指定されている。\n\nskip, n_max\n\n読み込みの行数を制御するオプション。skip引数で指定した行数が除外され、n_maxで読み込み対象にする行数を指定する。\n\n\n\n3.2.1 シート指定の読み込み\n\n# 対象のエクセルファイルのシート一覧を取得\nexcel_sheets(\"data/housekeeping.xlsx\")\n\n[1] \"Sheet1\" \"7月\"   \n\n\nエクセルファイルの読み込みを実行するread_excel()の基本的な使い方は、他のファイル読み込み関数と変わらず、読み込みの対象とするファイル名を第一引数に指定する。しかしエクセルファイルにはシートと呼ばれる機能があり、いわば一つのファイル中で複数のデータを格納することができるため、読み込み対象のシートについてもsheet引数で指定する必要がある。これを指定しない場合、read_excel()は一番目のシートを読み込む。\n\nread_excel(\"data/housekeeping.xlsx\",\n           sheet = 1)\n\n# A tibble: 35 × 4\n   日付                項目        金額 備考    \n   <dttm>              <chr>      <dbl> <chr>   \n 1 2009-05-10 00:00:00 帽子、洋服  9000 なし    \n 2 2009-05-18 00:00:00 靴、カバン 20000 なし    \n 3 2009-05-24 00:00:00 カメラ     40000 一眼レフ\n 4 2009-05-29 00:00:00 飲み代      4000 <NA>    \n 5 2009-06-05 00:00:00 散髪代      5000 なし    \n 6 2009-06-13 00:00:00 本          4000 なし    \n 7 2009-06-17 00:00:00 おもちゃ    5000 なし    \n 8 2009-06-24 00:00:00 洋服       10000 <NA>    \n 9 2009-06-25 00:00:00 靴          5000 <NA>    \n10 2009-06-28 00:00:00 電車代etc.  5000 <NA>    \n# … with 25 more rows\n\n\nsheet引数では、添字でシート番号を与えても良いし文字列でシート名を指定しても良い。この時、あらかじめexcel_sheets()でシート名をRオブジェクトとして取得し、それを利用することもできる。\n\nsheets <- excel_sheets(\"data/housekeeping.xlsx\")\nread_excel(\"data/housekeeping.xlsx\",\n           # 2番目のシートを読み込む\n           sheet = sheets[2])\n\n\n\n3.2.2 列指定の読み込み\nデータ型や列の除外により各列の読み込みの挙動を制御するcol_typesは、既定値ではNULLが与えられる。各列にはreadxlが自動的に判断したデータ型が与えられるが、ユーザが個別にデータ型を指定することもできる。col_types引数に指定可能な値は、論理型 logical、数値 numeric、日付 date、文字列 text、リスト listおよび、自動推定を実行する guessと読み込みから除外する skipがある。各列のデータ型を自動処理した時と個別に指定した時の挙動をそれぞれ見てみよう。\n\nread_excel(\"data/housekeeping.xlsx\") %>% str()\n\ntibble [35 × 4] (S3: tbl_df/tbl/data.frame)\n $ 日付: POSIXct[1:35], format: \"2009-05-10\" \"2009-05-18\" ...\n $ 項目: chr [1:35] \"帽子、洋服\" \"靴、カバン\" \"カメラ\" \"飲み代\" ...\n $ 金額: num [1:35] 9000 20000 40000 4000 5000 4000 5000 10000 5000 5000 ...\n $ 備考: chr [1:35] \"なし\" \"なし\" \"一眼レフ\" NA ...\n\nread_excel(\"data/housekeeping.xlsx\",\n           # 各列のデータ型はベクトルで定義する\n            # 無視する、文字列、文字列、文字列の順で列に含まれるデータ型を指定する\n            col_types = c(\"skip\", \"text\",\"text\", \"text\")) %>% str()\n\ntibble [35 × 3] (S3: tbl_df/tbl/data.frame)\n $ 項目: chr [1:35] \"帽子、洋服\" \"靴、カバン\" \"カメラ\" \"飲み代\" ...\n $ 金額: chr [1:35] \"9000\" \"20000\" \"40000\" \"4000\" ...\n $ 備考: chr [1:35] \"なし\" \"なし\" \"一眼レフ\" NA ...\n\n\n初めの例では”日付”列の値が日付を表すPOSIXctクラスとなっていることがわかる。他の列の値も R における適切なデータ型に変換されている。列のデータ型を指定した二番目の例では、col_types引数に与えた1番目の要素が skip となっており、読み込みから除外されている。また、他の列は全て text が与えられており、読み込み結果を見ると”日付”列は除外されており、他の列は文字列として処理されている。\n\n\n3.2.3 範囲を指定して読み込む\n次はデータの範囲指定を行った読み込みを行ってみよう。これにはrange引数を利用する。エクセルでのデータ入力の最小単位はセルであるが、セルには”A2”のような座標が与えられている。エクセルではA列からB列までの1から4行までのセルを読み込むには”A1:B4”とするが、rangeではエクセルでのセル指定と同じ方法が利用できる。\n\n\n\n表計算アプリケーションに保存されたデータ\n\n\n\nread_excel(\"data/housekeeping.xlsx\",\n           range = \"A1:B4\")\n\n# A tibble: 3 × 2\n  日付                項目      \n  <dttm>              <chr>     \n1 2009-05-10 00:00:00 帽子、洋服\n2 2009-05-18 00:00:00 靴、カバン\n3 2009-05-24 00:00:00 カメラ    \n\n\nrange引数の中では、セルの指定を補助するcell_cols()、cell_rows()といった関数が利用できる。また、より複雑な指定をするにはアンカー機能を実装したanchored()やcell_limits()を利用すると良い。こうした補助関数はcellrangerパッケージにより実装されている。\n例えば特定の列を選択するにはcell_cols()を利用して次のようにする。\n\n# 2列目 (セルB) を読み込む\n# 2つの例は同じ結果となる\nread_excel(\"data/housekeeping.xlsx\",\n           range = cell_cols(2))\nread_excel(\"data/housekeeping.xlsx\",\n           range = cell_cols(\"B\"))\n\nアンカー機能を利用し、B4セルを起点として2行3列分のデータを読み込む処理では、dim引数により範囲となる行と列の数を調整する。\n\nread_excel(\"data/housekeeping.xlsx\", \n           range = anchored(\"B4\", dim = c(2, 3)), \n           col_names = FALSE)\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n• `` -> `...3`\n\n\n# A tibble: 2 × 3\n  ...1    ...2 ...3    \n  <chr>  <dbl> <chr>   \n1 カメラ 40000 一眼レフ\n2 飲み代  4000 <NA>    \n\n\n読み込みの範囲を制御するオプションとして他に、skip、n_max引数が用意されている。これらはそれぞれ読み飛ばす先頭行の行数と読み込む行数を調整するものである。skipを用いる場合は次に示すように、読み飛ばした次の行が列名として与えられることになるので、col_namesでFALSEを指定するかcol_namesで列名を与えると良い。\n\n# 先頭に列名を指定した行がある場合、skipで読み飛ばされてしまう\nread_excel(\"data/housekeeping.xlsx\",\n           skip = 2) %>% names()\n\n[1] \"39951\"      \"靴、カバン\" \"20000\"      \"なし\"      \n\n# col_names = FALSEとした時には列名はreadxlにより自動的に与えられる\nread_excel(\"data/housekeeping.xlsx\",\n           skip = 2,\n           col_names = FALSE) %>% names()\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n\n\n[1] \"...1\" \"...2\" \"...3\" \"...4\"\n\n# col_namesで列名を与える\nread_excel(\"data/housekeeping.xlsx\",\n           col_names = c(\"日付\", \"項目\", \"金額\", \"備考\"),\n           skip = 2) %>% names()\n\n[1] \"日付\" \"項目\" \"金額\" \"備考\"\n\n\n\n\n3.2.4 特定の値を欠損とする\nエクセル上で欠損は、空白セルによって表現されることが多いため、readxlでは空白セルを欠損値として処理する。一方で、セルに対して明示的に欠損を示す文字列や値が与えられていることもあるだろう。例えば、これまで例として用いている「家計簿」データでは備考列で空白と「なし」と記述されたセルが混在している。読み込み後に「なし」を欠損として処理することも可能であるが、ここではna引数を用いて「なし」を欠損値として処理してみよう。naは任意の文字列を欠損値扱いにするためのオプションである。\n\n# 備考列の1, 2行目が「なし」となっている\nread_excel(\"data/housekeeping.xlsx\") %>% head(3)\n\n# A tibble: 3 × 4\n  日付                項目        金額 備考    \n  <dttm>              <chr>      <dbl> <chr>   \n1 2009-05-10 00:00:00 帽子、洋服  9000 なし    \n2 2009-05-18 00:00:00 靴、カバン 20000 なし    \n3 2009-05-24 00:00:00 カメラ     40000 一眼レフ\n\n# na引数で欠損値として処理する文字列を指定\nread_excel(\"data/housekeeping.xlsx\",\n                        na = \"なし\") %>% head(3)\n\n# A tibble: 3 × 4\n  日付                項目        金額 備考    \n  <dttm>              <chr>      <dbl> <chr>   \n1 2009-05-10 00:00:00 帽子、洋服  9000 <NA>    \n2 2009-05-18 00:00:00 靴、カバン 20000 <NA>    \n3 2009-05-24 00:00:00 カメラ     40000 一眼レフ"
  },
  {
    "objectID": "ch3.html#まとめ",
    "href": "ch3.html#まとめ",
    "title": "3  表形式のデータソースからのデータ取得",
    "section": "3.3 まとめ",
    "text": "3.3 まとめ\n\nRで取り扱うことができるファイルの形式は多様であるが、本章では中でも利用頻度の高いテキストファイル、エクセルファイルを取り扱う2つのパッケージ、readr、readxlを紹介した。\nreadrはR標準のテキストファイル入出力を実行する関数よりも高速である。またreadrを使ったデータ読み込みは、各変数のデータ型が自動的に判定され、文字列の因子型への自動変換を行わないなどの特徴を備えている。また読み込まれたデータには、データフレームクラスの拡張であるtbl\\_dfクラスが与えられる。\nエクセルファイルの読み込みとしてreadxlを導入することで、エクセルのシートや柔軟なセル範囲の指定を行った読み込みが可能となる。readxlには保存用の関数が用意されていないため、エクセルファイルへの出力はwritexlなどのパッケージを使うことになる。"
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  データ型に応じた処理",
    "section": "",
    "text": "これまではtidyverseで利用可能な、データ操作に特化したパッケージの働きについて見てきたが、この章では前半に文字列、因子データの処理としてstringrパッケージおよびforcatsを導入する。また後半では日付・時間のデータを扱うためのlubridateパッケージを紹介する。これらのパッケージはすべてtidyverseに含まれており、library(tidyverse)で利用可能になる。パッケージの関数は単一で利用しても便利であるが、パイプ演算子やdplyr、purrrといったパッケージと組み合わせることで、より利用範囲を広げられるだろう。"
  },
  {
    "objectID": "ch4.html#string",
    "href": "ch4.html#string",
    "title": "4  データ型に応じた処理",
    "section": "4.1 文字列処理",
    "text": "4.1 文字列処理\nデータ分析において文字列は、性別や曜日などの決まった項目が用いられるものからユーザ名のように利用可能な文字や長さに制限があるもの、アンケートの回答結果など様々だ。文字列の操作には結合や分割、検索、置換などがある。まずはRにおける文字列データの基本を覚えよう。\n\n4.1.1 Rにおける文字列の扱い\nRでは文字列を場合、引用符により文字列を囲む必要がある。引用符は一重引用符 (シングルクオート ')でも二重引用符 (ダブルクオート \")でも構わないが、文字列を囲む際はその種類を揃えておかなければならない。\n\n# 引用符によって囲んだ文字が文字データとして扱われる\n\"こんにちは\"\n\n[1] \"こんにちは\"\n\nclass('引用符で囲む')\n\n[1] \"character\"\n\n\n引用符を揃えない限り、Rコンソールは文字列の入力を受け付ける状態となる。始まりと終わりと引用符の種類が揃っていれば良いと先に述べたが、極力ドキュメントやプロジェクト中で引用符の種類は変更しないことが望ましい。\n\"引用符の種類は統一しておく'\n# +\nでは文字列の中で引用符を利用するにはどうするか。これは文字に含ませる引用符と異なる引用符を囲み文字に採用するか、バックスラッシュ記号 (“\\”) で引用符をエスケープすることで可能となる。\n\n# cat()は与えられた文字列をコンソールに出力する標準出力のための関数\ncat(\"引用符 ' を使う\")\n\n引用符 ' を使う\n\ncat('引用符 \" を使う')\n\n引用符 \" を使う\n\n# バックスラッシュによるエスケープも可能\ncat('引用符 \\' を使う')\n\n引用符 ' を使う\n\n\n文章を区切るための改行は、Rでは\\nで表現する。\\nで一行分の改行となる。\n\ncat(\"こんにちは。\\n今日は天気が良いですね。\")\n\nこんにちは。\n今日は天気が良いですね。\n\n\n\n\n4.1.2 stringrパッケージを使った文字列操作\ntidyverseにはstringrパッケージが文字列の処理に特化したパッケージとして用意される。本書でもこのパッケージを利用した例を紹介するが、stringrの関数が提供する正規表現の機能は、名称の類似したstringiが元となっている。興味のある読者はstringiの関数やドキュメントを参照すると良い。\nstringrパッケージの関数による文字列操作はRで標準利用可能な関数としても提供されているものが多いが、標準で利用できる関数はパイプ処理と組み合わせて利用することを想定していない。またstringrではstringiパッケージがサポートするIUC (International Components for Unicode)ライブラリの機能を利用できる。ICUライブラリはユニコードに関するさまざまな処理や操作に優れている。また、いくつかのRの標準関数が因子型のデータを入力値として利用できないのに対して、stringrは文字列型と同様に処理するという違いもある。\n\nここからはstringrパッケージを使った文字列の操作方法について紹介していこう。また、一部でstringiパッケージの関数を参照することもあるが、stringrパッケージのインストールが済んでいれば追加でインストールする必要はない。stringrパッケージのほとんどの関数の多くはstr_*()で実行される。str_の接頭辞のあとは関数が行う処理の内容を示している。\n\nlibrary(stringr)\n\n\n4.1.2.1 結合\nstr_c()は、引数に与えた複数の文字列を一つの文字列に結合する関数である。結合の際に文字列の間に任意の文字を指定可能であり、これはsep引数で指定する。既定値は”“(空白文字列)となっている。引数内で複数の要素をもつベクトルが与えられた時、結合はベクトルの各要素に対して行われる。\n\n# 引数に与えられた文字を結合する\nstr_c(\"こんにちは\", \"今日の天気は\", \"晴れです\")\n\n[1] \"こんにちは今日の天気は晴れです\"\n\n# sep引数で文字の区切りとなる位置に指定した文字を挿入する\nstr_c(\"こんにちは\", \"今日の天気は\", \"晴れです\", sep = \"_\")\n\n[1] \"こんにちは_今日の天気は_晴れです\"\n\n# 長さが異なるベクトルが与えられた時、要素は再利用される\n# 返り値は要素の数が最も多いベクトルに合わせて出力される\nstr_c(\"こんにちは今日の天気は\", \n      c(\"晴れ\", \"曇り\", \"雨\"), \n      \"です\")\n\n[1] \"こんにちは今日の天気は晴れです\" \"こんにちは今日の天気は曇りです\"\n[3] \"こんにちは今日の天気は雨です\"  \n\n\ncollapse引数を利用することで、返り値が複数の要素になる出力においても、単一のベクトルとして結合が行われる。collapseに指定した文字は、入力に与えられた要素と要素をつなぐ文字列として利用される。\n\n# 3つの長さのベクトルをcollapse引数で与えた文字列によって単一のベクトルに結合する\nstr_c(\"こんにちは今日の天気は\", \n      c(\"晴れ\", \"曇り\", \"雨\"), \n      \"です\", \n      collapse = \"。\")\n\n[1] \"こんにちは今日の天気は晴れです。こんにちは今日の天気は曇りです。こんにちは今日の天気は雨です\"\n\n\n同じく、str_flatten()も与えられた文字列の要素を結合して一つの文字列として返却する。\n\nstr_flatten(c(\"あ\", \"い\", \"う\", \"え\", \"お\"))\n\n[1] \"あいうえお\"\n\nstr_flatten(c(\"あ\", \"い\", \"う\", \"え\", \"お\"), collapse = \"_\")\n\n[1] \"あ_い_う_え_お\"\n\n\n\n\n4.1.2.2 抽出\n文字列から一部の文字を取り出すにはstr_sub()およびstr_subset()を使う。str_sub()は引数startとendそれぞれで抽出したい文字の位置を指定する。この際、負値を与えることも可能であり、例えばstartに-2を与えると、文字列の末尾から2文字分遡った位置が起点となる。\n\nstr_sub(string = \"こんにちは\", start = 3, end = 5)\n\n[1] \"にちは\"\n\n# 負値により、対象が文字の末尾になる\nstr_sub(string = \"こんにちは\", start = -2)\n\n[1] \"ちは\"\n\n\n文字の位置ではなく、対象の文字列、パターンが含まれるものを取り出すにはstr_subset()を使う。また、値そのものではなく、該当箇所の位置に興味がある場合はstr_which()を用いると良い。\n\n# 「ん」を含んだ文字列の要素を返却する\nstr_subset(c(\"おはよう\", \"こんにちは\", \"こんばんは\"), \n           pattern = \"ん\")\n\n[1] \"こんにちは\" \"こんばんは\"\n\n# 2, 3番目の要素に「ん」が含まれる\nstr_which(c(\"おはよう\", \"こんにちは\", \"こんばんは\"), \n           pattern = \"ん\")\n\n[1] 2 3\n\n\n\n\n4.1.2.3 長さを調べる、回数を数える\n「こんにちは」は長さが5の文字列であるがstringrでは文字の長さを数えるのにstr_length()を利用する。\n\nstr_length(string = \"こんにちは\")\n\n[1] 5\n\nstr_length(c(\"つくば\", \"Tsukuba\", \"筑波\"))\n\n[1] 3 7 2\n\n\n入力に欠損が与えられた場合にはNAが返される。これはRに標準で利用可能な関数として用意されるnchar()と同じ挙動であるが、str_length()は因子型のデータを与えた場合にも文字列のカウントを実行する(nchar()は因子型のデータの入力を受け付けない)。\n\nstr_length(NA)\n\n[1] NA\n\nanimals <- \n  c(\"cat\", \"dog\", \"mouse\", \"boar\") %>% \n  factor()\n\nstr_length(animals)\n\n[1] 3 3 5 4\n\n\n文字列中に含まれる単語や一文の数を数える処理はstr_count()でも実行できる。この関数は引数patternで指定した文字列が含まれる回数を数えるもので、次の処理ではベクトルの各要素に含まれる「ん」の文字数を数えている。\n\nstr_count(c(\"こんにちは\", \"こんばんは\", \"こん\"), \n          pattern = \"ん\")\n\n[1] 1 2 1\n\n\n\n\n4.1.2.4 パターンマッチ\n対象の文字列の中の一部、あるいは特定の要素に対してのみ処理を実行する、という時にはパターンマッチを利用した処理が有効である。例えばパターンマッチの方法には、先にstr_count()の例で見たように特定の文字列を与えることもできるが、stringrではより柔軟に文字列のマッチングを行える仕組みが備わっている。\nstringrパッケージで利用可能な関数の引数patternは、次の方法によるパターンマッチが可能である(既定ではregex()を用いて指定する正規表現が適用される)。これらの方法と関連する話題について詳細を見ていくために、まずは4つの文字を含んだ文字列ベクトルを用意しよう。続いてstr_detect()を用いながら、引数patternに与える処理を変えながら、それぞれの挙動を確認する。\nfixed(): 与えられた文字を直接評価するパターンマッチ\nregex(): 正規表現によるパターンマッチ。ICUの正規表現規則に従う\ncoll(): ロケールを考慮したパターンマッチ\nboundary(): 文章の境界、すなわち文字 character、単語 word、改行 line_breake、段落 sentenceで指定するパターンマッチ\nstr_detect()は指定したパターンが対象の文字列に含まれるかを判定する関数だ。返り値は論理値である。パターンと一致すればTRUEとなる。最初の例では、“^a”をpatternに指定した。2番目はfixed()の内部で同じ文字列を指定したが、結果は異なっている。fixed()は、patternに与えられた文字列を直接パターンマッチに利用するためである。“^a”で使っている”^“は、後続のパターンが先頭文字を示す記号として評価されるもので、対象となるstringsの中にはその文字は含まれない。\npatternに指定可能な関数には、boundary()を除いてignore_case引数が用意されている。これはアルファベットの大文字と小文字を区別しないためのオプションで、通常はFALSEが与えられる(大文字と小文字を厳密に区別する)。3番目の例ではregex()を使っているいるが、igreno_caseをTRUEとし、先頭が”a”となる大文字・小文字がある場合にTRUEを返すようになっている。\n\n# \\u0061 はaのユニコードエスケープ\nstrings <- \n  c(\"a\", \"\\u0061\", \"A\", \"あ\")\n\n# 既定値はregex()の指定と等しい\nstr_detect(string = strings, \n           pattern = \"a\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n# fixed()を使用\nstr_detect(string = strings, \n           pattern = fixed(pattern = \"^a\"))\n\n[1] FALSE FALSE FALSE FALSE\n\n# regex()を明示して使用。先頭がaまたはAで始まる文字にマッチ\nstr_detect(string = strings, \n           pattern = regex(pattern = \"^a\", \n                           ignore_case = TRUE))\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nまたregex()は他にmultiline、comments、dotall引数を備えている。これらはいずれも論理値を指定する。既定ではFALSEが与えられている。multilneが与えられた要素に改行(“\\n”)があった時に、改行ごとにパターンマッチを評価するかを、commentsはTRUEの時に、要素に含まれる、先頭文字の”#“記号および空白文字列を無視する、dotallは”.”を文字として評価しないことをそれぞれ意味している。これらのオプションはregex()は、“^”や”$“、”.”といった記号が次に述べるように、正規表現の記号と区別するために機能する。\n\nstr_detect(\"今日の天気は晴れです。\\n気温は26度を超えます。\",\n           regex(\"です。$\", multiline = TRUE))\n\n[1] TRUE\n\nstr_detect(\"# 今日の天気は晴れです。\",\n           regex(\"です。$\", comments = TRUE))\n\n[1] TRUE\n\nstr_extract(\"# 今日の天気は晴れです\\n\",\n           regex(\"晴れです.\", dotall = FALSE))\n\n[1] NA\n\nstr_extract(\"# 今日の天気は晴れです\\n\",\n           regex(\"晴れです.\", dotall = TRUE))\n\n[1] \"晴れです\\n\"\n\n\n続いて、coll()をpatternに指定した場合は次のような結果を返す。coll()は、ロケール(3章参照)を考慮したパターンマッチを適用する関数で、例えば、大文字のIはユニコード表記では”\\u49”となるがトルコ語などはIと区別してİを利用する。これはユニコード表記では”\\u0130”である。どちらも文字としてはIを示すため、localeにトルコの言語ロケールである”tr”を指定するとマッチングする。また、ここで指定可能なロケール一覧はstringi::stri_locale_list()によって確認できる。\n\ni <- \n  c(\"I\", \"\\u0130\")\n\nstr_extract(string = i, \n           pattern = coll(pattern = \"I\"))\n\n[1] \"I\" NA \n\nstr_extract(string = i, \n           pattern = coll(pattern = \"i\", ignore_case = TRUE, locale = \"tr\"))\n\n[1] NA  \"İ\"\n\n\n最後にboundary()の例であるが、これは文字の分割を行うstr_split()の節で紹介する。\n\n4.1.2.4.1 正規表現\n正規表現は文字列のパターンを表現する方法の一つである。文字列の中から特定の文字を検出したり、検出した文字を置換する、といった処理が正規表現を通して実行できる。正規表現は、Rの標準機能にも備わっているが、stringrパッケージが実装するICUのライブラリはより自由度の高い文字列パターンを扱うことができる。実際にこの章で紹介してきた関数で引数patternがあるものでは、同様の指定と処理が可能である。正規表現の詳細については奥が深く、すべてを理解することは困難であるが、ここで述べるような基本的な利用方法や、ある程度の内容について理解しておくだけで充分な価値がある。\n3つの要素を文字ベクトルから引数patternに指定した文字「ほくそ」が含まれているかを判定する処理は以下のようになる。\n\n\nhoxom <- \n  c(\"ほくそえむ\", \"ほくそうむ\", \"ほくそうり\")\n\nstr_detect(hoxom, \"ほくそ\")\n\n[1] TRUE TRUE TRUE\n\n\nベクトルhoxomに格納したすべての文字列は「ほくそ」を含んでいるため、この結果はすべて真となる。では次の例はどうだろうか。\n\nstr_detect(string = hoxom, pattern = \"む$\")\n\n[1]  TRUE  TRUE FALSE\n\n\n今度は3つ目の「ほくそうり」でFALSEを返すようになった。上記のコードで与えたパターンは「む」で終わる、である。記号$は文字列の終わりを示している。この他にも正規表現で利用可能な記号はいくつかの種類があり、メタ文字と呼ばれる正規表現の中で利用できる修飾子である。メタ文字を利用したパターンマッチの処理の例をいくつか見てみよう。\n\n# xの要素はすべて「ほくそ」で始まるため、任意の一文字を示す . を使うとマッチしなくなる\nstr_detect(string = hoxom, pattern = \".ほくそ\")\n\n[1] FALSE FALSE FALSE\n\n# 先頭および末尾の文字列を、^と$で指定したパターン\n# 「ほくそ」と「む」の中には別の文字列が含まれていても良いとする\nstr_detect(string = hoxom, pattern = \"^ほくそ.+む$\")\n\n[1]  TRUE  TRUE FALSE\n\n\n\nfruits <- \n  c(\"バナナ\", \"リンゴ\", \"パイナップル\")\n\n# 「ナ」が含まれる文字列にTRUEを返す\nstr_detect(fruits, \"ナ\")\n\n[1]  TRUE FALSE  TRUE\n\n# {2}は「ナ」の2回繰り返しを示す\n# バナナはナが連続して出現するが、パイナップルはナが1回しか出現しない\nstr_detect(fruits, \"ナ{2}\")\n\n[1]  TRUE FALSE FALSE\n\n# 3回以上4回未満の「ナ」の出現はいずれの要素にも含まれない\nstr_detect(fruits, \"ナ{3,4}\")\n\n[1] FALSE FALSE FALSE\n\n\n\nprefs <- \n  c(\"神奈川県\", \"東京都\", \"沖縄県\", \"岡山県\", \"富山県\")\n\n# 「県」で終わり、山または川を含んだ文字列にマッチ\nstr_detect(prefs, \"(山|川)県$\")\n\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\n\n\n\n\n修飾子\n意味\n指定の種類\n\n\n\n\n^\n以後の文字が先頭にある\n位置\n\n\n$\n以前の文字が後尾にある\n位置\n\n\n.\n任意の一文字\n対象\n\n\n|\n複数のパターンに区切る\n対象\n\n\n(), [], {}\nパターンをグループ化する\n対象\n\n\n*\n直前パターンをn回繰り返し(n >= 0)\n繰り返し\n\n\n?\n直前パターンをn回繰り返し(n = 0, 1)\n繰り返し\n\n\n+\n直前パターンをn回以上の繰り返し\n繰り返し\n\n\n{n}\n直前パターンをn回繰り返し\n繰り返し\n\n\n{n, }\n直前パターンをn回以上の繰り返し\n繰り返し\n\n\n{n, m}\n直前のパターンをn回以上m回以下繰り返し\n繰り返し\n\n\n\n指定したパターンが文字列中のどの位置でマッチしているかを確認する方法として、stringrではstr_view()あるいはstr_view_all()が用意されている。この関数はstr_detect()などの関数同様、対象の文字列とパターンマッチに用いる文字列を指定するが、実行するとウェブブラウザまたはRStudioを利用している場合はViewerパネル中に、マッチする箇所を表示する画面が出現する。グレーでハイライトされている箇所が引数patternで指定した文字列とマッチする箇所を示している。引数matchはstringに与えられた文字列中にマッチする要素だけ、あるいはマッチしない要素だけを表示するかを選択するオプションでり、論理値で指定する。既定ではNAが与えられ、いずれの要素も表示する。\n\nstr_view(string = hoxom, pattern = \"ほくそ\")\n\nstr_view(string = c(\"ほくそ\", \"もなぎ\"), \n         pattern = \"ほくそ\", match = FALSE)\n\n\n\n\nstr_view()によるマッチ箇所の確認\n\n\n\n\n4.1.2.4.2 文字クラス・POSIX文字クラス\n対象の文字が特定の文字列の組み合わせで生成されている場合には、文字クラスあるいはPOSIX文字クラスを利用したパターンマッチが可能になる。文字クラスでは、ブラケット ([])で挟んだ範囲の文字をパターンとみなし、更に連続する文字列の場合、例えばAからZまでのアルファベッドを指定する際にABC..Zとするのではなく、A-Zと対象の文字列の範囲を-で文字をつなぐことで複数の文字をパターンに含めることができる。\n\n\n\n\n文字クラス\n対象\n\n\n\n\n[0-9]\nアラビア数字\n\n\n[a-z]\n小文字アルファベット\n\n\n[A-Z]\n大文字アルファベット\n\n\n[ぁ-ん]\nひらがな\n\n\n[ァ-ヶ]\nカタカナ\n\n\n[一-龠]\n漢字 (すべての漢字に対応できるわけではない)\n\n\n[\\x01-\\x7E]\n1バイト文字\n\n\n\n\nstrings <- \n  c(\"ひらがな\", \"カタカナ\", \"漢字\", \"ABC\", \"abc\", \"123\", \"a2c4e6\")\n\n# 大文字のA, B, Cのいずれかを含んだパターン\nstr_detect(strings, \"[A-C]\")\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n\n# ひらがなにマッチするパターン\nstr_detect(strings, \"[ぁ-ん]\")\n\n[1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# 複数の文字クラスやメタ文字を扱うこともできる\n# アルファベットのAからZまで(大文字、小文字)を含んだパターンの検出\n# 「ABC」, 「abc」, 「a2c4e6」にマッチする\nstr_detect(strings, \"[A-Za-z]\")\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n\nstr_detect(strings, regex(\"[A-Z]\", ignore_case = TRUE))\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n\n# 先頭および末尾が数値で、長さが3の文字とマッチするパターン\n# 「123」に該当\nstr_detect(strings, \"^[0-9]{3}$\")\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n# ひらがな、カタカナ、漢字の一部を含んだパターンを検出\n# 「ひらがな」, 「カタカナ」, 「漢字」にマッチ\nstr_detect(strings, \"[ぁ-んァ-ヶ一-龠]\")\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nPOSIX文字クラスは一致するパターンを特定の種類にまとめ、特殊な表記を行うことで複数の文字を一度に対象パターンとして取り扱う。POSIX文字クラスは文字クラス同様、ブラケットを利用した記述を利用するが、加えてコロン (:)によってPOSIX文字クラス名を囲むという特徴がある。Rで利用可能なPOSIX文字クラスについて表XXに示した。\n\n\n\n\nPOSIX文字クラス\n対象\n\n\n\n\n[:alnum:]\nアルファベットと数値([:alpha:] + [:digit:])\n\n\n[:alpha:]\n大小文字アルファベット([:upper:] + [:lower:])\n\n\n[:upper:]\n大文字アルファベット\n\n\n[:lower:]\n小文字アルファベット\n\n\n[:digit:]\n数値\n\n\n[:blank:]\n空白文字、スペースとタブ\n\n\n[:cntrl:]\n制御文字\n\n\n[:graph:]\n空白以外の文字 ([:alnum:] + [:punct:])\n\n\n[:print:]\n印字可能な文字([:graph:] + スペース)\n\n\n[:punct:]\n補助符号を含めた句読点(! ” # $ % & ’ ( ) * + , - . /)\n\n\n[:space:]\nすべての空白文字\n\n\n[:xdigit:]\n16進数で認められている文字(0-9a-fA-F)\n\n\n\n\nstrings <- \n  c(\"alphabet\", \"123456\", \"alnum789\", \"123 456\")\n\nstr_extract(string = strings, pattern = \"[[:alpha:]]\")\n\n[1] \"a\" NA  \"a\" NA \n\nstr_extract(strings, \"[[:digit:]]\")\n\n[1] NA  \"1\" \"7\" \"1\"\n\nstr_extract(strings, \"[[:space:]]\")\n\n[1] NA  NA  NA  \" \"\n\n\nここで用いたstr_extract()は、パターンにマッチした箇所を文字列で返却する。もし対象の文字列に含まれるパターンが指定されない場合、欠損が与えられる。str_extract()ではメタ文字による繰り返しを指定しない場合は最初に該当した箇所の文字を返すが、これに対して、対象の文字列中ですべての該当箇所を返すようにするにはstr_extract_all()を利用すると良い。この関数の返り値はリストであるが、引数simplifyをTRUEとすることで各要素でマッチした箇所を含んだ行列になる。\n\n# 対象文字列でマッチするすべての文字列をリストで返す\nstr_extract_all(string = strings, pattern = \"[[:alpha:]]\")\n\n[[1]]\n[1] \"a\" \"l\" \"p\" \"h\" \"a\" \"b\" \"e\" \"t\"\n\n[[2]]\ncharacter(0)\n\n[[3]]\n[1] \"a\" \"l\" \"n\" \"u\" \"m\"\n\n[[4]]\ncharacter(0)\n\n# 各要素を行とし、マッチした箇所を含んだ行列として返却\nstr_extract_all(strings, \"[[:alpha:]]\", simplify = TRUE)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] \"a\"  \"l\"  \"p\"  \"h\"  \"a\"  \"b\"  \"e\"  \"t\" \n[2,] \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"  \n[3,] \"a\"  \"l\"  \"n\"  \"u\"  \"m\"  \"\"   \"\"   \"\"  \n[4,] \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"  \n\n\n複数の要素を与えた際、どの要素に対してマッチが行われているかを判断するために、str_which()を使う方法もある。\n\n\n4.1.2.4.3 文字ロケールの利用\n[WIP]\n\n\n\n\n4.1.2.5 文字列の削除と置換\n文字列の中には、思わぬ空白文字列や改行が含まれ、処理の邪魔となることがある。str_trim()やstr_squish()はこれらの余分な空白文字列や余分な改行を取り除く処理を実行する。次の文字列から、余分な改行や空白を取り除いてみよう。この文字列には、前後および途中で改行が与えられており、「でも」の後に空白文字列が与えられている。\n\n# 前後および文章の中に空白を含んだ文字列ベクトルを用意する\n(x <- c(\"\\n こんにちは。今日は天気が良いですね。\\nでも    明日は雨が降るらしいです \\n\\n\\n\"))\n\n[1] \"\\n こんにちは。今日は天気が良いですね。\\nでも    明日は雨が降るらしいです \\n\\n\\n\"\n\n\nstr_trim()は引数sideの指定により、除去を行う方向を指定する。既定値では文字列の両側を対象にする(“both”)が、他に左右の指定(“left”および”right”)が可能である。\n\n# 文字列に含まれる空白文字の除去。既定値で両側の空白が対象になる\nstr_trim(x)\n\n[1] \"こんにちは。今日は天気が良いですね。\\nでも    明日は雨が降るらしいです\"\n\nstr_trim(x, side = \"left\")\n\n[1] \"こんにちは。今日は天気が良いですね。\\nでも    明日は雨が降るらしいです \\n\\n\\n\"\n\n\nさて、str_trim()によりいくつかの文字が取り除かれたが、依然として文章中に必要以上の空白が残っている。これはstr_squish()で、単一の空白文字列へと置換可能できる。\n\nstr_squish(x)\n\n[1] \"こんにちは。今日は天気が良いですね。 でも 明日は雨が降るらしいです\"\n\n\n\n特定の文字列や文字列の一部を任意の値に変更する処理を実行するためにstr_replace()が利用できる。この関数には引数patternとreplaceがあり、対象のパターン、置換後の文字を指定する。\n\nstr_replace(\"こんばんは\",\n            pattern     = \"ばんは\", \n            replacement = \"にちは\")\n\n[1] \"こんにちは\"\n\n\nstr_replace()では、パターンに一致した箇所の置換を、最初に一致した箇所のみ対象にするが、すべての一致する箇所で置換を適用するにはstr_replace_all()を使う。\n\n# 「こんばんは」中の「ん」を空白文字列に置換\n# str_replace()では、最初の「ん」しか変更されない\nstr_replace(\"こんばんは\", \"ん\", \" \")\n\n[1] \"こ ばんは\"\n\n# str_replace_all()を用いることですべての箇所が置換の対象になる\nstr_replace_all(\"こんばんは\", \"ん\", \" \")\n\n[1] \"こ ば は\"\n\n\nまたstr_replace_all()では、複数の置換パターンを与えることも可能だ。これは引数patternにc(対象文字列 = 置換後の文字列)の形で指定する。\n\n# 複数のパターンの置換を同時に実行する\nx <- \n  c(\"こんばんは\", \"こんにちは 今日は寒いですね\")\n\nstr_replace_all(x, \n                c(\"こんばんは\" = \"今晩は\", \"寒い\" = \"暑い\"))\n\n[1] \"今晩は\"                      \"こんにちは 今日は暑いですね\"\n\n\n次の例は、pattern引数に指定した住所文字列にマッチするパターンとして「東京都」「道」「府」「県」の4種を指定している。複数のパターンに対してマッチングを行うにはこのようにして|記号を用いて文字列を区切る。また京都府の場合には、2文字目の「京”都”府」にマッチしてしまうので、「都」を含んだ文字列でマッチングさせるのは「東京都」となるようにした。\n\naddress <- \n  c(\"東京都渋谷区桜ヶ丘\", \n    \"岡山県岡山市北区清心町\", \n    \"茨城県つくば市小野川\",\n    \"京都府舞鶴市字浜\")\n\nstr_replace(string = address, \n            pattern = \"(東京都|道|府|県).+\",\n            replacement = \"\\\\1\")\n\n[1] \"東京都\" \"岡山県\" \"茨城県\" \"京都府\"\n\n\n正規表現では、パターンに含める文字列を括弧で囲むことで、マッチさせた文字列を利用できるようになる。これは後方参照と呼ばれる。マッチした文字列は、置換後の値を指定する引数replacementで\\\\1として呼び出している(\\を繰り返すのはエスケープのため)。\nマッチした箇所の文字列だけを除外するには、str_remove()を使う方法がある。これはstr_replace(string, pattern, replacement = \"\")と同じ働きをもつ。この関数も対象文字列中でマッチする全箇所を対象にするstr_replace_all()がある。\n\nstr_remove(\"文字列の「この部分」を削除する\", \"この部分\")\n\n[1] \"文字列の「」を削除する\"\n\n# 最初にマッチした箇所を除外する\nstr_remove(\"Hello world\", \"o\")\n\n[1] \"Hell world\"\n\n# 全てのマッチ箇所を除外の対象にする\nstr_remove_all(\"Hello world\", \"o\")\n\n[1] \"Hell wrld\"\n\n\nこのほか、文字列置換の特殊な用途として、欠損 NAを文字列に変換するstr_replace_na()が用意されている。\n\nstr_replace_na(c(NA_character_, \"abc\", \"こんにちは\"))\n\n[1] \"NA\"         \"abc\"        \"こんにちは\"\n\n\n\n\n4.1.2.6 分割\nstr_split()およびstr_split_fixed()は、指定したパターンにマッチした箇所を起点に、対象の文字列を分割する関数である。ここで、分割に指定したパターンは、boundary()による文字の境界以外は返り値に含まれないという特徴がある。通常、対象の文字列でマッチする箇所があるたびに分割が行われるが、これは引数nに分割する数値を指定して制御できる(既定ではInfが指定される)。またこのオプションはstr_split()ではリストの要素数の指定となるが、str_split_fixed()では行列の列数となり、ユーザの指定が必要なものとなる。なおstr_split()の返り値はリストであるが、simplify引数でTRUEを指定するとstr_split_fixed()と同じく行列が返されるようになる。後続の処理の用途に応じて使い分けると良いだろう。\n\nsouseki_text <- \n  c(\"吾輩は猫である。名前はまだない。\\nどこで生れたかとんと見当けんとうがつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\")\n\n\n# 「である。」および「つかぬ。」が出現する位置で要素を分割する\nstr_split(string = souseki_text, pattern = \"(である|つかぬ)。\")\n\n[[1]]\n[1] \"吾輩は猫\"                                                                \n[2] \"名前はまだない。\\nどこで生れたかとんと見当けんとうが\"                    \n[3] \"何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\"\n\n# 引数nで返り値の要素数を制御\n# n以上の分割は行われない\nstr_split(souseki_text, \n          \"(である|つかぬ)。\", \n          n = 2)\n\n[[1]]\n[1] \"吾輩は猫\"                                                                                                                            \n[2] \"名前はまだない。\\nどこで生れたかとんと見当けんとうがつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\"\n\n# str_split_fixed()の返り値は行列で、引数nにより列数を指定する\nstr_split_fixed(string = souseki_text, \n                pattern = \"(である|つかぬ)。\", \n                n = 2)\n\n     [,1]      \n[1,] \"吾輩は猫\"\n     [,2]                                                                                                                                  \n[1,] \"名前はまだない。\\nどこで生れたかとんと見当けんとうがつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\"\n\n\npatternの指定にboundary()を利用する場合、入力に用いた文字は残り、分割だけが行われる。\n\nstr_split(souseki_text, boundary(\"character\"))\n\n[[1]]\n [1] \"吾\" \"輩\" \"は\" \"猫\" \"で\" \"あ\" \"る\" \"。\" \"名\" \"前\" \"は\" \"ま\" \"だ\" \"な\" \"い\"\n[16] \"。\" \"\\n\" \"ど\" \"こ\" \"で\" \"生\" \"れ\" \"た\" \"か\" \"と\" \"ん\" \"と\" \"見\" \"当\" \"け\"\n[31] \"ん\" \"と\" \"う\" \"が\" \"つ\" \"か\" \"ぬ\" \"。\" \"何\" \"で\" \"も\" \"薄\" \"暗\" \"い\" \"じ\"\n[46] \"め\" \"じ\" \"め\" \"し\" \"た\" \"所\" \"で\" \"ニ\" \"ャ\" \"ー\" \"ニ\" \"ャ\" \"ー\" \"泣\" \"い\"\n[61] \"て\" \"い\" \"た\" \"事\" \"だ\" \"け\" \"は\" \"記\" \"憶\" \"し\" \"て\" \"い\" \"る\" \"。\"\n\n# マルチバイト文字を含んだ改行はうまくいかないことがある\nstr_split(souseki_text, boundary(\"line_break\"))\n\n[[1]]\n [1] \"吾\"     \"輩\"     \"は\"     \"猫\"     \"で\"     \"あ\"     \"る。\"   \"名\"    \n [9] \"前\"     \"は\"     \"ま\"     \"だ\"     \"な\"     \"い。\\n\" \"ど\"     \"こ\"    \n[17] \"で\"     \"生\"     \"れ\"     \"た\"     \"か\"     \"と\"     \"ん\"     \"と\"    \n[25] \"見\"     \"当\"     \"け\"     \"ん\"     \"と\"     \"う\"     \"が\"     \"つ\"    \n[33] \"か\"     \"ぬ。\"   \"何\"     \"で\"     \"も\"     \"薄\"     \"暗\"     \"い\"    \n[41] \"じ\"     \"め\"     \"じ\"     \"め\"     \"し\"     \"た\"     \"所\"     \"で\"    \n[49] \"ニャー\" \"ニャー\" \"泣\"     \"い\"     \"て\"     \"い\"     \"た\"     \"事\"    \n[57] \"だ\"     \"け\"     \"は\"     \"記\"     \"憶\"     \"し\"     \"て\"     \"い\"    \n[65] \"る。\"  \n\nstr_split(\"blah\\nblah\\nblah\", boundary(\"line_break\"))\n\n[[1]]\n[1] \"blah\\n\" \"blah\\n\" \"blah\"  \n\nstr_split(souseki_text, boundary(\"word\"))\n\n[[1]]\n [1] \"吾輩\"         \"は\"           \"猫\"           \"で\"           \"ある\"        \n [6] \"名前\"         \"は\"           \"まだ\"         \"ない\"         \"どこ\"        \n[11] \"で\"           \"生れ\"         \"たか\"         \"とんと\"       \"見当\"        \n[16] \"け\"           \"ん\"           \"と\"           \"うがつ\"       \"か\"          \n[21] \"ぬ\"           \"何でも\"       \"薄暗い\"       \"じめじめ\"     \"した\"        \n[26] \"所\"           \"で\"           \"ニャーニャー\" \"泣\"           \"い\"          \n[31] \"て\"           \"いた事\"       \"だけ\"         \"は\"           \"記憶\"        \n[36] \"し\"           \"て\"           \"いる\"        \n\nstr_split(souseki_text, boundary(\"sentence\"))\n\n[[1]]\n[1] \"吾輩は猫である。\"                                                        \n[2] \"名前はまだない。\\n\"                                                      \n[3] \"どこで生れたかとんと見当けんとうがつかぬ。\"                              \n[4] \"何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。\"\n\n\n\n\n4.1.2.7 要素の並び替え\n文字列要素の順番や並び替えに関する関数は2つ存在する。str_order()\n\nstr_order(c(\"あ\", \"え\", \"い\", \"う\", \"え\", \"お\", \"あ\", \"お\"),\n          locale = \"ja_JP\")\n\n[1] 1 7 3 4 2 5 6 8\n\nstr_sort(c(\"あ\", \"え\", \"い\", \"う\", \"え\", \"お\", \"あ\", \"お\"))\n\n[1] \"あ\" \"あ\" \"い\" \"う\" \"え\" \"え\" \"お\" \"お\"\n\nstringi::stri_order(c(\"い\", \"あ\", \"う\", \"あ\"))\n\n[1] 2 4 1 3\n\nstr_order(c(\"a\", \"b\", \"a\"))\n\n[1] 1 3 2\n\nstr_sort(letters)\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n\n4.1.2.8 エンコード変換\n日本語などを含んだ文字列が正常に表示できていない状態、文字化けを起こしている場合、str_conv()を使い、エンコードし直すことで問題に対応することができる。新たに指定するエンコードは引数encodingで、エンコード名を文字列で与えるが、その一覧はstringi::stri_enc_list()で確認できる。この中で日本語に関するエンコードとしては”EUC-JP”、“ISO-2022-JP”、“cp932”、“UTF-8”などがある。\n\nx <- \"\\x82\\xa0\\x82\\xa2\\x82\\xa4\\x82\\xa6\\x82\\xa8\"\n# str_conv()では元のエンコードを指定するだけで自動的に現在のエンコード形式に変更する\nstr_conv(x, encoding = \"cp932\")\n\n[1] \"あいうえお\"\n\n\n\n\n4.1.2.9 文字列フォーマット\n指定した書式（フォーマット）に基づき、与えられた値を文字列に変換する文字列フォーマットは、str_interp()ならびにstr_glue()により提供される。これらの関数は、Rオブジェクトの値を利用することも可能である。やや指定方法に癖があるが、慣れれば、柔軟に文字列の生成が行えるので大変便利な関数である。\nstr_interp()、str_glue()にはいくつかの利用方法がある。まず、Rオブジェクトの値を文字列中に利用するには、次のようにドル記号(“$”)に次いでブラケット(“{}”)で囲んだ中にRオブジェクトの変数名を与える。フォーマットを指定する場合、ドル記号の後、“{”ブラケットの前に”[“ブラケットの囲み文字でフォーマット指定を行う。指定可能なフォーマットはRで標準利用できるsprintf()と同じく、代表的なものを表XXに整理している。\n\nmy_name <- \"真也\"\nage_year <- 1989\nage_jp_year <- 1\n# 3つのオブジェクトを文字列の中で利用する。\n# age_jp_year, age_yearの2箇所でフォーマットを指定している(ともに整数)\nstr_interp(\"私の名前は${my_name}です。\n平成$[02d]{age_jp_year}($[4d]{age_year})年の生まれです。\")\n\n[1] \"私の名前は真也です。\\n平成01(1989)年の生まれです。\"\n\n\n関数内で参照するRオブジェクトの評価は、文字列の出力時に行われるため、次のように修正を加えることもできる。\n\nstr_interp(\"$[4d]{age_year - 4}年の生まれです。\")\n\n[1] \"1985年の生まれです。\"\n\n\n関数内部でオブジェクトを生成し、それを利用することも可能である。これにはlist()を使い、変数名と値の組み合わせを指定する。\n\nstr_interp(\"$[4d]{age_year}年の生まれです。\", \n           env = list(age_year = 2000))\n\n[1] \"2000年の生まれです。\"\n\n\nstr_glue()はglueパッケージが提供する機能を実装したもので、glue::glue()、glue::glue_data()と同じ引数オプションが利用できる。\n\nstr_glue(\n  \"{when}の時刻は {datetime}\\n\",\n  \"時刻を書式化すると{format(datetime, '%Y年%m月%d日 %X')}\\n\",\n  \"日付は {format(datetime, '%Y年%m月%d日')}　です\\n\",\n  \"時刻は {format(datetime, '%X')} です\",\n  when = \"ただいま\",\n  datetime = Sys.time()\n)\n\nただいまの時刻は 2022-05-02 16:57:10\n時刻を書式化すると2022年05月02日 16:57:10\n日付は 2022年05月02日　です\n時刻は 16:57:10 です\n\ndf_jyunishi <- \n  data.frame(\n    eto = c(\"子\", \"丑\", \"寅\", \"卯\", \"辰\", \"巳\", \"午\",\n            \"未\", \"申\", \"酉\", \"戌\", \"亥\"), \n    stringsAsFactors = FALSE)\n\n# データフレームの変数を参照する\n# 複数の要素が与えられた時は、与えられた数の出力を行う\ndf_jyunishi %>% \n  str_glue_data(\"{2008 + 0:11}年の干支は{eto}です\")\n\n2008年の干支は子です\n2009年の干支は丑です\n2010年の干支は寅です\n2011年の干支は卯です\n2012年の干支は辰です\n2013年の干支は巳です\n2014年の干支は午です\n2015年の干支は未です\n2016年の干支は申です\n2017年の干支は酉です\n2018年の干支は戌です\n2019年の干支は亥です\n\n\n\n\n\n\n\n\n\n\n\n表記\n表現\n表記の例\n\n\n\n\n%a\n曜日名の略称\nFri\n\n\n%A\n完全な曜日名\nFriday\n\n\n%b\n月名の略称\nDec\n\n\n%B\n完全な月名\nDecember\n\n\n%c\n日付と時間 (“%a %b %e %H:%M:%S %Y”)\nFri Dec 25 14:00:00 2015\n\n\n%y\n西暦の下２桁\n15\n\n\n%Y\n西暦\n2015\n\n\n%m\n月\n12\n\n\n%d\n日(１月の範囲)\n25\n\n\n%j\n日(１年の範囲)\n359\n\n\n%H\n時間(24時間)\n14\n\n\n%I\n時間(12時間)\n02\n\n\n%M\n分\n01\n\n\n%p\n午前と午後の区分\nPM\n\n\n%S\n秒\n30\n\n\n%w\n曜日\n5\n\n\n%x\n年月日 (“%m/%d/%y”)\n12/25/2015\n\n\n%X\n時刻 (“%H:%M:%S”)\n14:01:30\n\n\n\n\n\n4.1.2.10 桁揃えと丸め込み\n文字数を揃える際には、str_pad()、str_trunc()の2つの関数が役に立つ。これらはそれぞれ、桁揃え、丸め込みを行う関数で、処理を適用する方向を前後、両側あるいは中央のいずれかで指定できる。\n例えば、1から20までの数値ですべての値を2桁に揃えるという時には、次のようにwidthで桁数、padに補間する任意の文字列、“0”を与えたstr_pad()を実行する。\n\nstr_pad(1:20, width = 2, pad = \"0\")\n\n [1] \"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\"\n\n\nstr_pad()では、補間する文字列を挿入する位置を指定するside引数を除いた引数の値はベクトル化される。すなわち次のように与えられた値と処理の組み合わせを返却する。\n\nstr_pad(\"a\", width = c(5, 3, 4), pad = \"_\", side = \"right\")\n\n[1] \"a____\" \"a__\"   \"a___\" \n\nstr_pad(\"a\", width = 2, pad = c(\"_\", \"-\"), side = \"right\")\n\n[1] \"a_\" \"a-\"\n\n\nstr_trunc()はstringの文字列の長さをwidthの数値になるよう調節する機能をもつ。既定では”…“が省略時の文字列として使われるが、これはellipsisによりユーザが変更可能である。\n\nstr_trunc(\"aaaaaa\", 5)\n\n[1] \"aa...\"\n\n# side引数で省略位置の指定を行う\nstr_trunc(\"aaaaaa\", 5, side = \"center\")\n\n[1] \"a...a\"\n\n# ellipsis引数で省略記号を変更する\nstr_trunc(\"aaaaaa\", 5, ellipsis = \"(略)\")\n\n[1] \"aa(略)\"\n\n\n\n\n\n4.1.3 forcatsパッケージ\n順序や項目に意味のある因子型データ (factor, カテゴリデータ)を扱うのに便利なパッケージとしてforcatsを取り上げる。因子型のデータは、3章においてdata.frame()で文字列を変数に与えた場合に変換される見たように、Rの多くの標準関数、統計処理(glm()などの関数で指定するformulaに文字列型データを与えた場合、因子型へ自動変換が行われる)で利用されている。\n一方で、どのように因子型として扱われているのか、水準の並びがどうなっているのかを把握せずに処理を進めていくと予想外の結果を招く危険もある。そのためtidyverseのパッケージおよびtibble形式のデータフレームは文字列の因子型への変換を自動的には実行しない仕組みになっている。因子型への変換は、必要に応じて実行すれば良い。\nforcatsには、ユーザの使い勝手を向上させた、因子型の操作関数が豊富に含まれている。その使い方を見ていくことにしよう。forcatsの関数の多くは第一引数に対象とする文字列または因子型のベクトルを指定する。また関数名にはfct_という接頭辞が与えられている。\n\nlibrary(forcats)\n\nまずforcatsの因子型変換の基本であるが、Rで標準利用可能な因子型への変換を行う関数as.factor()と異なる挙動として、要素が出現した順に水準を与える。これはロケールの設定により結果が異なる可能性のある挙動を制御するのに有効な仕様である。\n\n# 標準関数ではロケールの設定を反映した並び替えが行われる\nas.factor(c(\"A\", \"C\", \"B\"))\n\n[1] A C B\nLevels: A B C\n\nas.factor(c(\"あ\", \"う\", \"い\", \"お\"))\n\n[1] あ う い お\nLevels: あ い う お\n\n# forcats::as_factor() では要素の出現した順番に水準が与えられる\nas_factor(c(\"A\", \"C\", \"B\"))\n\n[1] A C B\nLevels: A C B\n\n# sort()を適用して水準を定義する処理\nc(\"あ\", \"う\", \"い\", \"お\") %>% \n  sort() %>% \n  as_factor()\n\n[1] あ い う お\nLevels: あ い う お\n\n\nここからは架空SNSデータを用いてforcatsパッケージの因子操作関数の解説を行う。データの読み込みは3章で説明したreadrパッケージを使った方法と同じである。df_snsにはnationalityという列があり、この列はSNSへの投稿を行ったユーザの国籍をアルファベット2文字で示した国コードである。\n\ndf_sns <- \n  readr::read_csv(\"data/sns.csv\")\n\nRows: 1000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): post_id, address, place_name, user_id, nationality\ndttm (1): post_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf_sns$nationality[1:5]\n\n\n4.1.3.1 水準のカウント\nfct_count()は指定したベクトルの頻度をカウントするdplyr::count()関数と類似の機能をもつ関数である。count()との違いは、ベクトルで頻度を数える値を指定すること、集計後の水準が因子型になること、出力結果はデータフレームであるが、変数名が対象のベクトルを指すfと頻度のnからなることである。しかし水準の並びは規定では、頻度ではなく文字の並びとなるので注意が必要である。これは引数sortでTRUEを指定することでデータフレームの並びが頻度の降順になる。\n\ndf_count <- \n  fct_count(df_sns$nationality)\n\n# ベクトルで与えた変数は因子として扱われる\ndf_count$f %>% levels()\n\n [1] \"AU\" \"CN\" \"ES\" \"FR\" \"GB\" \"HK\" \"ID\" \"IN\" \"KR\" \"MY\" \"PH\" \"SG\" \"TH\" \"TW\" \"US\"\n\n# 出力されるデータフレームの並びを頻度の降順にする\nfct_count(df_sns$nationality, sort = TRUE) %>% \n  head()\n\n# A tibble: 6 × 2\n  f         n\n  <fct> <int>\n1 TH      166\n2 US      152\n3 KR      138\n4 ID      119\n5 CN       84\n6 PH       77\n\n\n\n\n4.1.3.2 水準の並びかえ\nRの因子型データへの変換は、ユーザが順位づけを指定しない場合、文字列の並びを優先して並びをつける。これに対して水準の並びを入れ替える関数fct_inorder()とfct_infreq()はそれぞれ、値の出現した順、頻度の多い順に水準の順位づけを行う。\n\n# アルファベットの並びで順序が与えられる\ndf_sns$nationality %>% \n  factor() %>% \n  levels()\n\n [1] \"AU\" \"CN\" \"ES\" \"FR\" \"GB\" \"HK\" \"ID\" \"IN\" \"KR\" \"MY\" \"PH\" \"SG\" \"TH\" \"TW\" \"US\"\n\n# fct_inorder()... それぞれの水準の並びの違いに注意\ndf_sns$nationality %>% \n  sort(decreasing = TRUE) %>% # アルファベットの降順ソートを行う\n  fct_inorder() %>% \n  levels()\n\n [1] \"US\" \"TW\" \"TH\" \"SG\" \"PH\" \"MY\" \"KR\" \"IN\" \"ID\" \"HK\" \"GB\" \"FR\" \"ES\" \"CN\" \"AU\"\n\n# fct_infreq()... 頻度による順序づけ\ndf_sns$nationality %>% \n  fct_infreq() %>% \n  levels()\n\n [1] \"TH\" \"US\" \"KR\" \"ID\" \"CN\" \"PH\" \"HK\" \"GB\" \"MY\" \"SG\" \"TW\" \"AU\" \"ES\" \"FR\" \"IN\"\n\n\n既存の水準の並びを変更する関数として、このほか、fct_reorder()やfct_rev()、fct_shift()などがある(表XX)。これらは下記のような作図コード中での利用が効果的になるだろう。\n\n\n\n\n関数\n並び替えの方法\n\n\n\n\nfct_infreq()\n頻度で順位をつける\n\n\nfct_inorder()\n出現した順番にする\n\n\nfct_relevel()\n文字列による順番の指定\n\n\nfct_reorder()\nカウントや最大値など異なる水準値を指定\n\n\nfct_rev()\n順番の反転\n\n\nfct_shift()\n指定した数だけずらす\n\n\nfct_shuffle()\nランダム\n\n\n\n\nlibrary(ggplot2)\n\np1 <- df_count %>% \n  ggplot(aes(f, n)) + \n  geom_bar(stat = \"identity\") +\n  ggtitle(\"default\")\n# 別水準(ここでは頻度)による並び替え\np2 <- df_count %>% ggplot(aes(fct_reorder(f, n), n)) + \n  geom_bar(stat = \"identity\") +\n  ggtitle(\"fct_reorder\")\n# 順位を逆転させる\np3 <- df_count %>% \n  ggplot(aes(fct_rev(fct_reorder(f, n)), n)) + \n  geom_bar(stat = \"identity\") +\n  ggtitle(\"fct_rev\")\n# fct_shiftでは引数nに指定した値分、順位をずらす\np4 <- df_count %>% \n  ggplot(aes(fct_shift(fct_reorder(f, n), n = 3), n)) + \n  geom_bar(stat = \"identity\") +\n  ggtitle(\"fct_reorder\")\n\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\nFigure 4.1: forcatsパッケージの水準並び替え\n\n\n\n\n\n\n4.1.3.3 水準の変更、追加・削除\n因子型のベクトルの水準の値を手動で変更する関数としてfct_recode()を紹介する。この関数は、新たに定義したい水準とそれに含まれる要素を文字列で指定する。また、水準から除外する要素がある場合はNULLを指定する。\n\n(x <- as_factor(df_sns$nationality[1:5]))\n\n[1] US TH HK ID US\nLevels: US TH HK ID\n\n# US, THの値を「アメリカ」、「タイ」に変換\n# IDをNAとして処理し、水準から除外する\nx %>% \n  fct_recode(\n    `アメリカ` = \"US\",\n    `タイ`     = \"TH\",\n    NULL       = \"ID\"\n  )\n\n[1] アメリカ タイ     HK       <NA>     アメリカ\nLevels: アメリカ タイ HK\n\n\n因子型のベクトルは、個々の要素に与えられた水準が与えられており、次のように一部を参照した場合でも元の水準を含んでいる。\n\n# 国籍のベクトルを因子型にしたデータの一部を参照\n# 水準を引き継ぐ\n(x[1:2])\n\n[1] US TH\nLevels: US TH HK ID\n\n\n使われていない水準を削除するには、標準関数のdroplevels()が有効であるが、fct_drop()を使うことで個別の水準を削除することが可能となる。この関数はdroplevels()同様、既定では参照されていない水準をすべて除外するが、only引数に対象の水準名を指定することで特定の水準のみを削除することができる。なお、水準が参照されている状態では削除は行われない。\n\ndroplevels(x[1:2])\n\n[1] US TH\nLevels: US TH\n\nfct_drop(x[1:2])\n\n[1] US TH\nLevels: US TH\n\n# IDのみを削除 (HKを残す)\nfct_drop(x[1:2], only = c(\"ID\"))\n\n[1] US TH\nLevels: US TH HK\n\n\nデータに大量の水準がある時、あるいは水準をより大きな水準として処理したい時、fct_collapse()による変換が役立つ。次の処理は、df_snsのnationality (国籍)列を大陸ごとにまとめて新たに水準を与える例である。\n\nfct_count(df_sns$nationality, sort = TRUE) %>% \n  head()\n\n# A tibble: 6 × 2\n  f         n\n  <fct> <int>\n1 TH      166\n2 US      152\n3 KR      138\n4 ID      119\n5 CN       84\n6 PH       77\n\nfct_collapse(df_sns$nationality,\n             Asia    = c(\"CN\", \"HK\", \"ID\", \"IN\", \"KR\", \n                         \"MY\", \"PH\", \"SG\", \"TH\", \"TW\"),\n             America = c(\"US\"),\n             Europa  = c(\"ES\", \"FR\", \"GB\"),\n             Oceania = c(\"AU\")) %>% \n  fct_count(sort = TRUE)\n\n# A tibble: 5 × 2\n  f           n\n  <fct>   <int>\n1 Asia      731\n2 America   152\n3 Europa     73\n4 Oceania    23\n5 <NA>       21\n\n\nfct_lump()は要素の数が多いカテゴリデータに対し、頻度の少ない要素を一つのグループにまとめる処理を実行する。引数nで実行後の要素数を定めるか、propでは全体の頻度から各要素の頻度を割った値を閾値とし、閾値がpropの値未満である時にother_levelの値が適用される。\n\n# nationalityは15要素からなり、合計で500になる\ntable(df_sns$nationality)\n\n\n AU  CN  ES  FR  GB  HK  ID  IN  KR  MY  PH  SG  TH  TW  US \n 23  84  20  19  34  58 119   1 138  30  77  29 166  29 152 \n\n# nで再グループ化後の要素数を定義する\ndf_sns$nationality %>% \n  fct_lump(n = 6, other_level = \"その他\") %>% \n  table()\n\n.\n    CN     ID     KR     PH     TH     US その他 \n    84    119    138     77    166    152    243 \n\n# 全体に対するCNの割合は0.128、GBは0.042\n# propで閾値を設定する... GBはその他のグループに含まれる\ndf_sns$nationality %>% \n  fct_lump(prop = 0.05, other_level = \"その他\") %>% \n  table()\n\n.\n    CN     HK     ID     KR     PH     TH     US その他 \n    84     58    119    138     77    166    152    185 \n\n\n因子型のベクトルを扱うとき、欠損値は除外して処理されることがある。一方、fct_explicit_na()を用いると欠損値を一時的に文字列へと変換が行われる。欠損値の水準は、水準の最後の値として扱われる。これは例えばtable()を使った次の処理で因子型のベクトルの頻度を数えることが可能となる。\n\nnationality <- \n  df_sns$nationality %>% \n  as_factor()\n\n# 欠損値は除外されている\ntable(nationality)\n\nnationality\n US  TH  HK  ID  AU  SG  CN  MY  PH  KR  GB  ES  FR  TW  IN \n152 166  58 119  23  29  84  30  77 138  34  20  19  29   1 \n\n# 欠損値 NAは (Missing) という文字列に置換され、結果に含まれる\nnationality %>% \n  fct_explicit_na() %>% \n  table()\n\n.\n       US        TH        HK        ID        AU        SG        CN        MY \n      152       166        58       119        23        29        84        30 \n       PH        KR        GB        ES        FR        TW        IN (Missing) \n       77       138        34        20        19        29         1        21 \n\n# 欠損値は水準の最後の値に採用される\nc(\"(aa)\", \"(zz)\", NA) %>% \n  fct_explicit_na()\n\n[1] (aa)      (zz)      (Missing)\nLevels: (aa) (zz) (Missing)"
  },
  {
    "objectID": "ch4.html#date-and-time",
    "href": "ch4.html#date-and-time",
    "title": "4  データ型に応じた処理",
    "section": "4.2 日付・時間データの扱い",
    "text": "4.2 日付・時間データの扱い\n日付や時間はデータが記録された時点の値が記述されることが多く、データを比較する際の一つの基準として用いられる。例えば定点観測によって得られるログデータでは、時間の推移状態から異常を検知したり、不安定な変動の発生を記録する。日付・時間データは、今日の1日前が昨日、2日後は明後日というように、数値的な算術演算が可能である。一方で日付・時間には表記方法やタイムゾーンの違いを考慮する必要があり、また丸め込みといった処理によってデータが異なるものに変化することがあるため、その扱いには気をつけなければいけない。ここではRにおける日付・時間データの基礎的な操作方法とともにパッケージを使った日付・時間データの処理と応用的な処理の例を見ていくことにする。\n\n4.2.1 Rにおける日付・時間データ\n日付・時間を表す形式(2018年1月1日を示す2018-01-01など)にはさまざまなものがあるが、Rでは、こうした日付を示す文字列を定義しても文字型のデータとみなされる。これを日付として扱うには、as.*関数群の中にあるas.Date関数を利用する必要がある。\n\nx <- \"2018-02-05\"\nclass(x)\n\n[1] \"character\"\n\n\n\n# 日付を表す文字を変換する\nas.Date(x) %>% class()\n\n[1] \"Date\"\n\n\nRでは日付や時間に関するデータの取り扱いのために専用のクラスを用意しており、日付を扱うオブジェクトとしてDate、日付・時間のオブジェクトを表すPOSIXlt並びにPOSIXctと呼ばれるクラスが該当する。これらのクラスは、Dateが日付だけを扱うのに対してPOSIXltおよびPOSIXctは日付と時間(秒単位まで)を扱うという点で異なっている。日付や時間を示すこれらのクラスは、日数の集計や現在から30分後の時間といった演算処理を可能にするために協定世界時 UTC (Universal Time Coordinated)と呼ばれる基準点を利用している。UTCでは1970年1月1日の0時0分を基準とし、年月日や時間に関する計算はこれを0とみなして実行される。次に示す簡単な例でこの概念を確認してみよう。\n\n# 協定世界時での1970年1月1日が数値的に0として扱われることを確認\nas.numeric(as.Date(\"1970-01-01\"))\n\n[1] 0\n\n# tz引数に協定世界時 UTCを与えた際の1970年1月1日0時0分を数値化\nas.numeric(as.POSIXlt(\"1970-01-01 00:00:00\", tz = \"UTC\"))\n\n[1] 0\n\n# 日付オブジェクトを生成するas.Date()関数では、1970年1月1日から一日進むごとに数値が1加えられる\nas.Date(\"1970-01-02\") %>% as.numeric()\n\n[1] 1\n\n# 時間オブジェクトを生成するas.POSIXlt()関数では1秒ごとに1追加される\nas.numeric(as.POSIXlt(\"1970-01-01 00:00:01\", tz = \"UTC\"))\n\n[1] 1\n\n# 本書執筆時の日付・時間とその数値変換した値の出力とその数値的表現\nSys.Date()\n\n[1] \"2022-05-02\"\n\nas.numeric(Sys.Date())\n\n[1] 19114\n\nSys.time()\n\n[1] \"2022-05-02 16:57:13 JST\"\n\nas.numeric(Sys.time())\n\n[1] 1651478234\n\n\n日付・時間を扱うオブジェクトでは、算術演算子を用いた演算処理が可能である。また同一クラスのオブジェクトであれば、オブジェクト間で算術演算が利用できる。例えば、Sys.Date()により求めた「今日」の日付から1を引くと「昨日」の日付が出力される。\n\n# Dateオブジェクトでは1日を単位1として演算処理を行う\nx <- Sys.Date()\n# 昨日の日付を求める\nx - 1\n\n[1] \"2022-05-01\"\n\n# 現在の日付から10日後の日付を返す\nx + 10\n\n[1] \"2022-05-12\"\n\n# POSIXctオブジェクトでは1秒を1単位として扱う\nx <- Sys.time()\nx - 3 * 60 * 60\n\n[1] \"2022-05-02 13:57:13 JST\"\n\n# クラスが異なるために警告が表示される\nSys.time() - Sys.Date()\n\nWarning: Incompatible methods (\"-.POSIXt\", \"-.Date\") for \"-\"\n\n\n[1] \"2022-05-02 11:38:39 JST\"\n\n# 指定されていない日付・時間は丸め込み処理が行われる\nas.POSIXct(\"1970-01-01\") - as.POSIXct(\"2015-12-25 21:00:00\")\n\nTime difference of -16794.88 days\n\n\n\n\n4.2.2 lubridateパッケージ\n日付・時間データの処理に特化したパッケージはいくつかあるが、本書ではlubridateを取り上げる。このパッケージはGarrett GrolemundとHadley Wickhamによって開発されたもので、開発思想として利用者が日付・時間のデータや時間帯の処理を直感的に操作できるようになることを目的としている。またtidyverse群に含まれるパッケージであり、データ操作のdplyrやパイプ処理と相性が良い。またlubridateの関数が返す値の多くはPOSIXctクラスをもっているため、従来の日付・時間クラスとの演算が可能となっている。\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n# 直感的に理解しやすい関数を豊富に備えている\ntoday()\n\n[1] \"2022-05-02\"\n\nnow()\n\n[1] \"2022-05-02 16:57:13 JST\"\n\n\n\n4.2.2.1 日付・時間の生成\nymd()やymd_hms()といった関数は利用者が入力した日付・時間を示す文字列を整形して出力する。これらの関数は日付と時間を構成する文字列、年であればyear、月はmonth、日はdayのように、対象の文字列が示す日付・時間の表記形式に対応して読み込みを実行する。dmy()のように構成要素の順番を入れ替えることであらゆる日付・時間の表記に対応することができる。対象がベクトルで複数の要素を含んでいる場合でも、日付・時間の構成要素が共通していれば出力が行われる。また日付・時間以外の文字列が含まれている際には日付・時間を示す文字のみ抽出し、暦に用いられる文字列の場合には適当な置換を行う仕様となっている。\n\n# 年月日形式の文字列を日付として扱う\nymd(\"20151225\", tz = \"Asia/Tokyo\")\n\n[1] \"2015-12-25 JST\"\n\n# 年月日という表記形式に対応する\nymd(c(\"20151225\", \"2017-02-24\", \"2001年8月31日\"))\n\n[1] \"2015-12-25\" \"2017-02-24\" \"2001-08-31\"\n\n# 表記形式の異なる要素では読み込みに失敗する\nymd_hms(c(\"20151225 12:30:40\", \"2017-02-24 12:30:40\", \"August 31, 2001\"))\n\nWarning: 1 failed to parse.\n\n\n[1] \"2015-12-25 12:30:40 UTC\" \"2017-02-24 12:30:40 UTC\"\n[3] NA                       \n\n# 日付・時間の要素のみを対象にする\nymd(c(\"今日は2017年2月25日です。\",\n               \"明日は2017年2月26日です\",\n               \"20170227\"))\n\n[1] \"2017-02-25\" \"2017-02-26\" \"2017-02-27\"\n\n\n他に、日付・時間の個々の構成要素を引数で指定して実行するmake_date()、make_datetime()を用いて日付・時間オブジェクトを生成することができる。\n\nmake_date(year = 1970, month = 1:3, day = 5:3)\n\n[1] \"1970-01-05\" \"1970-02-04\" \"1970-03-03\"\n\n# 指定を省略した要素には0あるいは1が与えられる\nmake_datetime(1970, 1, 1, hour = 3, sec = 30)\n\n[1] \"1970-01-01 03:00:30 UTC\"\n\n\n\n\n4.2.2.2 日付・時間の演算と更新\n日付・時間の算術演算は+や-演算子を利用して行うことが可能である。数値を与えた場合、算術演算は日付・時間オブジェクトの最小単位に対して実行される。そこで、演算の対象とする単位をlubridateのquick_periods()を用いて実行するのが効率的である。例えば2月26日から同日の一ヶ月後となる3月26日に変更したいとき、月の日数を考慮して処理しなければいけなかったものが、months()を使って以下のように処理できる。quick_periods()の関数群の名前は日付・時間の単位を複数形で表現したものとなる。\n\n# 算術演算子を使った処理\nymd(\"20160226\") - 7\n\n[1] \"2016-02-19\"\n\nymd_hms(\"20160226 04:55:21\") + 20\n\n[1] \"2016-02-26 04:55:41 UTC\"\n\n# 3月26日を求める\nymd(\"20160226\") + 29\n\n[1] \"2016-03-26\"\n\nymd(\"20160226\") + months(1)\n\n[1] \"2016-03-26\"\n\nymd(\"19891127\") + years(28) + months(3) - days(30)\n\n[1] \"2018-01-28\"\n\n\nただし+や-演算子を使った処理では、末日が現実に存在しない値を得る場合に処理が失敗する。具体的には11月の末日30日であるが2月は28日で終わる(閏年の場合は29日まである)。そのため、次のように11月30日から1ヶ月ごとの末日を得ようとすると2月の値は演算に失敗してしまう。lubridateが提供する%m+%演算子は、この問題を考慮し、演算を実行する。\n\n# 1989年11月から1ヶ月刻みで増やしていく\nymd(\"19891130\") + months(1:12)\n\n [1] \"1989-12-30\" \"1990-01-30\" NA           \"1990-03-30\" \"1990-04-30\"\n [6] \"1990-05-30\" \"1990-06-30\" \"1990-07-30\" \"1990-08-30\" \"1990-09-30\"\n[11] \"1990-10-30\" \"1990-11-30\"\n\nymd(\"19891130\") %m+% months(1:12)\n\n [1] \"1989-12-30\" \"1990-01-30\" \"1990-02-28\" \"1990-03-30\" \"1990-04-30\"\n [6] \"1990-05-30\" \"1990-06-30\" \"1990-07-30\" \"1990-08-30\" \"1990-09-30\"\n[11] \"1990-10-30\" \"1990-11-30\"\n\nymd(\"19891130\") %m+% years(1:12)\n\n [1] \"1990-11-30\" \"1991-11-30\" \"1992-11-30\" \"1993-11-30\" \"1994-11-30\"\n [6] \"1995-11-30\" \"1996-11-30\" \"1997-11-30\" \"1998-11-30\" \"1999-11-30\"\n[11] \"2000-11-30\" \"2001-11-30\"\n\n\n関数update()を使い、日付・時間への修正を加えることもできる。この関数は引数に日付・時間の構成要素名を与えて実行する。\n\n(x <- now())\n\n[1] \"2022-05-02 16:57:13 JST\"\n\nupdate(x, year = 1989)\n\n[1] \"1989-05-02 16:57:13 JST\"\n\nupdate(x, hour = 11, minute = 30, second = 00)\n\n[1] \"2022-05-02 11:30:00 JST\"\n\n\n\n\n4.2.2.3 期間\n日付・時間において2つの時刻を扱う場合には期間という概念が発生する。そのためlubridateパッケージでは、日付・時間に関する特殊なオブジェクトとして期間に関する3つのクラスを提供する。これらはinterval()、duration()、period()という関数名が示す通り、それぞれ、時刻Aから時刻Bまでの間を示す期間、長さとして与えられる時間の間隔、時間の単位で構成される長さを示すものとなっている。この中で特にduration()は、継続中の時間を示す期間の概念である。\n2つの任意の時刻における間の時間を求める際にはinterval()が関数が役に立つ。また関数の代わりに演算子%--%を利用しても良い。\n\n# 1970年1月1にから今日までのIntervalクラスオブジェクトを作る\ntime_int <- \n  interval(ISOdate(\"1970\", \"01\", \"01\"), today(), tzone = \"Asia/Tokyo\")\ntime_int\n\n[1] 1970-01-01 21:00:00 JST--2022-05-02 09:00:00 JST\n\ninterval(as.Date(\"2016-01-01\"), today())\n\n[1] 2016-01-01 UTC--2022-05-02 UTC\n\n# %--%演算子によるIntervalクラスオブジェクトの生成\nISOdate(\"2016\", \"01\", \"01\") %--% today()\n\n[1] 2016-01-01 12:00:00 GMT--2022-05-02 GMT\n\n\nIntervalに関しては、Intervalオブジェクトであるかを検証するis.interval()をはじめとし、期間の始点と終点を返すint_start()およびint_end()、2つの期間の重なりを検知するint_overlaps()などが適用できる。またint_shift()やint_start()、int_end()を使って既存の期間の値を変更することが可能である。\n\n# Intervalオブジェクトに対する検証\nis.interval(time_int)\n\n[1] TRUE\n\n# 期間の始点と終点を返す\nint_start(time_int)\n\n[1] \"1970-01-01 21:00:00 JST\"\n\nint_end(time_int)\n\n[1] \"2022-05-02 09:00:00 JST\"\n\n# 期間を反転させる\nint_flip(time_int)\n\n[1] 2022-05-02 09:00:00 JST--1970-01-01 21:00:00 JST\n\n# 期間の長さを秒単位で表示する(1日の長さを示す)\nint_length(interval(today() - 1, today(), tzone = \"Asia/Tokyo\"))\n\n[1] 86400\n\n# 期間中の時間を標準化させる\nint_standardize(time_int)\n\n[1] 1970-01-01 21:00:00 JST--2022-05-02 09:00:00 JST\n\n# 期間をずらす\nint_shift(time_int, duration(days = 30))\n\n[1] 1970-01-31 21:00:00 JST--2022-06-01 09:00:00 JST\n\nint_shift(time_int, duration(days = -5))\n\n[1] 1969-12-27 21:00:00 JST--2022-04-27 09:00:00 JST\n\n\nint_aligns()およびint_overlaps()は2つのオブジェクトを比較し、期間の重なりを検知して論理値を返す。int_aligns()は期間のうち、始点や終点のいずれかが共通である時にTRUEを返す。またint_overlaps()は2つのオブジェクトの期間が重複している場合にTRUEとなる。\n\nint_aligns(interval(ISOdate(\"1970\", \"01\", \"01\"), ymd(\"2012-04-01\"), tzone = \"Asia/Tokyo\"), \n           interval(ISOdate(\"1989\", \"11\", \"17\"), today(), tzone = \"Asia/Tokyo\"))\n\n[1] FALSE\n\n# 2つの期間は終点が同じであるため、返り値がTRUEとなる\nint_aligns(interval(ISOdate(\"1970\", \"01\", \"01\"), today(), tzone = \"Asia/Tokyo\"), \n           interval(ISOdate(\"1989\", \"11\", \"17\"), today(), tzone = \"Asia/Tokyo\"))\n\n[1] TRUE\n\nint_overlaps(interval(ISOdate(\"1970\", \"01\", \"01\"), ymd(\"2012-04-01\"), tzone = \"Asia/Tokyo\"), \n           interval(ISOdate(\"1989\", \"11\", \"17\"), today(), tzone = \"Asia/Tokyo\"))\n\n[1] TRUE\n\ninterval(as.Date(\"2016-01-01\"), today()) / ddays(2)\n\n[1] 1156.5\n\n\nある日付・時間のオブジェクトが、2期間の間に含まれるかを判定する演算子が%within%として提供されている。この演算子は、左辺に対象の日付・時間オブジェクト、右辺にIntervalオブジェクトを指定して実行する。返り値は論理値である。\n\n# 5月1日から8日までの日付オブジェクトを生成\n(x <- \n  make_date(2018, 5, 1:8))\n\n[1] \"2018-05-01\" \"2018-05-02\" \"2018-05-03\" \"2018-05-04\" \"2018-05-05\"\n[6] \"2018-05-06\" \"2018-05-07\" \"2018-05-08\"\n\nrenkyu <- \n  interval(ymd(\"2018-05-03\"), ymd(\"2018-05-06\"))\n\nx %within% renkyu\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\n\ninterval()が生成するIntervalに対し、duration()やdseconds()などの関数で実行される返り値はDurationオブジェクトとなる。\n\nx.duration <- duration(day = -1, units = \"day\")\nclass(x.duration)\n\n[1] \"Duration\"\nattr(,\"package\")\n[1] \"lubridate\"\n\nis.duration(x.duration)\n\n[1] TRUE\n\nduration(second = 90)\n\n[1] \"90s (~1.5 minutes)\"\n\n# 文字列からも作成できる\nduration(\"31 days\")\n\n[1] \"2678400s (~4.43 weeks)\"\n\n# 1年の秒表記\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\n# 1週間と7日間は同じ期間である\ndweeks(1) - ddays(7)\n\n[1] \"0s\"\n\n# 1~3時間の期間を秒数で返す\nc(1:3) * dhours(1)\n\n[1] \"3600s (~1 hours)\"  \"7200s (~2 hours)\"  \"10800s (~3 hours)\"\n\n# 1日の秒数を返す\nint_length(interval(today() - 1, today(), tzone = \"Asia/Tokyo\")) %>% dseconds()\n\n[1] \"86400s (~1 days)\"\n\n\n最後の時間の期間を示すオブジェクトはperiod()によって作成されるPeriodクラスである。period()では引数にunitsがあり、時間の単位を指定する。\n\nperiod(1, \"days\")\n\n[1] \"1d 0H 0M 0S\"\n\ntoday()\n\n[1] \"2022-05-02\"\n\ntoday() - period(day = 10)\n\n[1] \"2022-04-22\"\n\n\nなおこれまでに見てきた3つの期間に関係するクラスは、as.*()による関数で相互にクラスの変更が可能である。\n\n\n4.2.2.4 時間帯の指定\nlubridateでは既存の関数による時間帯やロケールの指定に対応した関数が備わっている。時間帯が指定可能な関数では引数tzあるいはtzoneにより指定を行う。引数tzあるいはtzoneで指定することが多い。例えば同一時刻について異なる時間帯での時刻を確認するにはwith_tz()に対象のオブジェクトと引数tzoneに時間帯を指定して実行する。利用可能な時間帯の名称についてはOlsonNames()により確認できる。\n\n\n# 現在の時刻を出力する。時間帯にはJST 日本標準時が使われている\n(x <- now())\n\n[1] \"2022-05-02 16:57:14 JST\"\n\n# with_tz()では、引数tzone に与えた時間帯での時間を返す\nwith_tz(x, tzone = \"UTC\")\n\n[1] \"2022-05-02 07:57:14 UTC\"\n\nwith_tz(x, tzone = \"America/Puerto_Rico\")\n\n[1] \"2022-05-02 03:57:14 AST\"\n\n\n\n# 引数tzで時間帯を指定する\nymd(\"2015-October-17\", tz = \"Asia/Tokyo\")\n\n[1] \"2015-10-17 JST\"\n\nymd(\"2015年10月10日\", tz = \"Asia/Tokyo\")\n\n[1] \"2015-10-10 JST\"\n\nc(25122015, \"27-11-2015\", \"2日1月2000年\") %>% dmy(tz = \"Asia/Tokyo\")\n\n[1] \"2015-12-25 JST\" \"2015-11-27 JST\" \"2000-01-02 JST\""
  },
  {
    "objectID": "ch4.html#まとめ",
    "href": "ch4.html#まとめ",
    "title": "4  データ型に応じた処理",
    "section": "4.3 まとめ",
    "text": "4.3 まとめ\n\n因子型を含めた文字、日付・時間のデータは、数値とは扱いが異なり、tidyverseでは専用の処理パッケージ、stringr、forcats、lubridateが用意されている。\nstringrはICUの正規表現ライブラリを実装しており、柔軟なパターンマッチを可能にする\nforcatsは因子型データの処理に特化し、Rの標準の因子データの扱いの使い勝手を向上させる。\nlubridateには日付・時間のデータ処理を容易にする、日付・時間の生成関数make_date()やymd()、期間を扱うinterval()などの関数、日付・時間の演算のための%m+%、%interval%演算子を提供する。"
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  効率的なデータ操作",
    "section": "",
    "text": "この章では、効率的にデータを処理・操作する方法を学ぶ。ここでの効率的、とは、コード内での繰り返しの記述を減らしたり、見通しの良い記述での実行を意味している。また対象として単なる繰り返しだけでなくグループごとに同じ処理を適用することも含んでいる。これらの技法を学ぶことは、データの前処理だけでなく統計解析、モデリングの関数や可視化を行うために役立ち、十分に投資する価値のあるものである。"
  },
  {
    "objectID": "ch5.html#繰り返しの処理はプログラムの得意分野",
    "href": "ch5.html#繰り返しの処理はプログラムの得意分野",
    "title": "5  効率的なデータ操作",
    "section": "5.1 繰り返しの処理はプログラムの得意分野",
    "text": "5.1 繰り返しの処理はプログラムの得意分野\n一つの状況を考えてみよう。データフレームの変数が、数値を格納するか、文字列であるのかは、Rの処理を行う上で意識しておきたい事項である。今、手持ちのデータフレームについて、各変数のデータ型が不明であるとき、どのように確認すれば良いだろう。ここではオブジェクトのクラスを確認するため、names()でデータフレームに含まれる変数の名前を確認した後、各変数への参照をclass()で表示する方法を示す。\n\nnames(trees) # データフレームの変数名 (列名)を取得する\n\n[1] \"Girth\"  \"Height\" \"Volume\"\n\nclass(trees$Girth) # 変数Girthの値をベクトル形式で参照する\n\n[1] \"numeric\"\n\nclass(trees$Height) \n\n[1] \"numeric\"\n\nclass(trees$Volume)\n\n[1] \"numeric\"\n\n\nもう一つ、データフレームの列数を確認した後に[で値を参照する方法は以下のようになる。\n\nclass(trees[, 1])\n\n[1] \"numeric\"\n\nclass(trees[, 2])\n\n[1] \"numeric\"\n\nclass(trees[, 3])\n\n[1] \"numeric\"\n\n\n目的は達成できたが、一方でデータフレームの中身が同一であっても変数名が異なっていたり、同じ変数が大量にあるデータフレームで同じことを行う場合はどうだろうか。これらの方法は確実ではあるが、変化やコードへ記述する内容が増えてしまうという欠点がある。変化がある度に内容を書き換えたり、同じ内容の記述を行うと、どこかで入力間違いが発生するリスクも高まる。これから示すpurrrによる処理は、これらの欠点を解消するのに有効な手法となる。\n\n\n5.1.1 引数に関数をもつ関数\nRの標準関数にはapply()やsapply()など、関数を引数にもつ高階関数 (汎関数)がある。これらのapply族と呼ばれる関数は、繰り返し行われる処理を効率的に行うために利用される。例えばこれをデータフレームの各変数に実行する場合、次のコードを実行すれば良い。\n\n# apply族関数を用いてtreesの変数のデータ型を確認\napply(X = trees, MARGIN = 2, FUN = class)\n\n    Girth    Height    Volume \n\"numeric\" \"numeric\" \"numeric\" \n\nlapply(X = trees, FUN = class)\n\n$Girth\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Volume\n[1] \"numeric\"\n\n\nいずれの関数も対象のデータをXで指定し、データに適用する関数をFUNで与えている。またapply()の中で利用している**MARGINは関数の適用する方向を示しており、2は列の指定を示している。FUN*には関数の名前が使われるが、文字列と区別するために引用符は用いられない。一方で+や%*%などの演算子を与える場合には演算子の名前を引用符あるいはバッククオート(“`”)で囲む。これは$や[演算子を使った参照をデータフレームの各変数に適用するよりも効率的である。特に変数の多いデータではapply族関数が効果的である。\ntidyverseに含まれるpurrrrパッケージは、こうしたapply族関数と同機能をもつ関数map()をはじめとして、効率的にRオブジェクトを扱うための関数を提供する。またpurrrrで扱うmap()をはじめとした関数は、apply族関数よりも実行の仕方や返り値のデータ型についてわかりやすくなっている。加えてパイプ処理やdplyr、ggplot2といった他のtidyverseで利用されるパッケージとも相性が良い。purrrの関数mapで先の処理を実行すると次のようになる。\n\nlibrary(purrr) # purrrはtidyverseに含まれる\n\nmap(.x = trees, .f = class)\n\n$Girth\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Volume\n[1] \"numeric\"\n\n# purrrはtidyverseに含まれており、\n# 第一引数に対象のオブジェクトを受け取るため\n# パイプ処理が適用しやすい\ntrees %>% \n  map(class)\n\n$Girth\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Volume\n[1] \"numeric\"\n\n\nこれはlapply(trees, class)の結果と同じで、返り値は3つの要素からなるリストとなっている。map()はpurrrの基礎となる関数である。map()は引数.xに与えられた各要素に対して、.fで指定された関数を適用し、入力と同じ長さのベクトルを返却する関数である。\nmap()の第二引数の.fには、次のように、無名関数も指定可能であるが、~を用いることで関数定義を短縮して記述することも可能である。すなわち以下のコードは全て同様の結果を導く。\n\n1:3 %>% \n  map(rnorm, n = 3)\n# 無名関数を定義して与えても良い\n1:3 %>% \n  map(function(x) rnorm(3, x))\n# ~ は function(x)を省略した記述となる\n1:3 %>% \n  map(~ rnorm(3, .))\n\n無名関数を定義する場合、その第一引数にデータを指定する。~はfunction(x)の省略型であり、ラムダ式として無名関数を表現できる。その際はデータを”.”または”.x”、“..1”で示す。数字を使う場合、ドットが2回続くのに注意である。“.x”や”..1”といった表現は、与えられる引数の位置を把握するのに優れ、次に述べる、複数の入力を元に処理を適用する場合にさらに便利である。\nまたmap()では、添字を使った位置や名前による要素の指定も行える。ここでは引数に対象の要素の位置を指定するか、名前を与えて実行すれば良い。これは特にリストを扱う上で効果的である。\n\n(x <- list(list(id = 1,\n          name = \"hoxom\"),\n     list(id = 2,\n          name = \"uribo\",\n          age = 28)))\n\n[[1]]\n[[1]]$id\n[1] 1\n\n[[1]]$name\n[1] \"hoxom\"\n\n\n[[2]]\n[[2]]$id\n[1] 2\n\n[[2]]$name\n[1] \"uribo\"\n\n[[2]]$age\n[1] 28\n\n\nここでxはid、name、ageの3つの名前をもつリストである。リストにはさらに2つのリストが含まれている。ここからxの各要素を参照するための処理は次のようになる。\n\n# 各リスト中の第一の要素を参照する\nx %>% \n  map(1)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n# リスト中のnameの値を参照する\nx %>% \n  map(\"name\")\n\n[[1]]\n[1] \"hoxom\"\n\n[[2]]\n[1] \"uribo\"\n\n# 返却する要素が無い場合は NULLが与えられる\n# NULLは引数.defaultの指定により変更可能\nx %>% \n  map(\"age\")\n\n[[1]]\nNULL\n\n[[2]]\n[1] 28\n\nx %>% \n  map(\"age\", .default = NA)\n\n[[1]]\n[1] NA\n\n[[2]]\n[1] 28\n\n\nmap()の返り値がリストになることは上記のコードで示した通りだが、map_*()を使ってベクトルとして得ることもできる(表XX)。map_*()には、返り値のベクトルのデータ型を定義するもので、それは、map_*()の接尾辞によって決まる。例えば文字列として出力する場合にはmap_chr()、整数値であるならばmap_int()という具合である。もちろん、ここでもベクトル内部でのデータ型の混在は許されず、不可逆的なデータ型への変換は行えない。\nmap_*()の例として、長さ(文字数)の異なる3つの文字列を用意し、その長さを数えてみよう。ここでは文字列の長さを数える関数としてnchar()を適用する。nchar()の返り値は整数であり、ベクトルとして出力するためにmap_int()が利用できる。また\n\nx <- c(\"こんにちは\", \"Hello\", \"您好\")\n\nx %>% \n  map(nchar)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 2\n\nx %>% \n  map_int(nchar)\n\n[1] 5 5 2\n\n\n次に関数greplを用いて、Hで始まる要素の有無を確認してみる。map()は適用する関数の引数を関数内部で指定可能な高階関数であるので、次のようにgrepl()の引数patternをmap()の引数として指定する。これはgrepl(x[1], pattern = \"^H\"\")、grepl(x[2], pattern = \"^H\"\")、そしてgrepl(x[3], pattern = \"^H\"\")の処理を実行して得る結果に等しい。\n\nx %>% \n  map_lgl(grepl, pattern = \"^H\")\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n関数\n返り値\n\n\n\n\nmap_lgl()\n論理値\n\n\nmap_chr()\n文字列\n\n\nmap_int()\n整数\n\n\nmap_dbl()\n実数\n\n\n\nベクトルの他、データフレームを返却するmap_dfc()、map_dfr()も利用できる。これらは処理結果をデータフレームの列および行方向に結合した結果を返す。\n以下の例はmtcarsのclyごとにデータを分割し、その後、分割された各データにdispの平均値を算出し、平均値をmap_dfc()によりデータフレームの横方向に結合する、というものになる。\n\nmtcars %>% \n  # splitはRで標準的に利用可能な関数\n  # 与えられた要素ごとにデータを分割する\n  split(.$cyl) %>% \n  map_dfc(~ mean(.$disp))\n\n# A tibble: 1 × 3\n    `4`   `6`   `8`\n  <dbl> <dbl> <dbl>\n1  105.  183.  353.\n\n\n\n\n\n5.1.2 mapの派生\nmap()にはmap_int()をはじめとして多数の派生した関数がある。これらは処理を適用する要素を制限したり、複数の入力を関数に与えることを可能にする。また、出力を伴わない処理を実行する関数も備わっている。\n\n5.1.2.1 位置や条件で適用する: map_at / map_if\nmap()では与えられた全ての要素に関数を適用をするが、map_if()やmap_at()を使うことで条件や位置を元に適用箇所を制限することができる。2つの関数はそれぞれ.p、.atをもち、そこに適用する要素の条件およびその位置を指定する。なお、これらの関数の返り値は、共通のデータ型である場合でも常にリストとなる。\n3つの要素からなる文字列ベクトルにnchar()を実行し、各文字数を数えるが、ここで条件を定義して適用する要素を制限するというコードは次のように記述する。\n\nx\n\n[1] \"こんにちは\" \"Hello\"      \"您好\"      \n\n# xの各要素でアルファベットを含むものを対象に\n# nchar()を適用する\nx %>% \n  map_if(.p = grepl(\"[a-z]\", .), nchar)\n\n[[1]]\n[1] \"こんにちは\"\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] \"您好\"\n\n# パイプ処理を用いない場合、 データの指定を次のようにする必要がある\n# map_if(x, .p = ~ grepl(\"[a-z]\", .x), nchar)\n\nここでは引数.pが条件として与えられる処理になる。条件には、関数greplにより、対象がアルファベッドに含まれる文字列であるものを指定した。grepl()の中で”.”で示されるのはパイプ処理によって代名詞として利用可能な”x”を示している。条件に当てはまらない要素では、関数は適用されず、元の値が返却される。\n条件の代わりに要素の位置を基準として処理を施すには、関数map_atの引数.atでその要素の位置を指定する。下記の例では2、3番目の要素にnchar()が実行された結果が出力される。\n\n# xの要素から、2、3番目のものに対して\n# nchar()を実行\nx %>% \n  map_at(.at = c(2, 3), nchar)\n\n[[1]]\n[1] \"こんにちは\"\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 2\n\n\n\n\n5.1.2.2 複数の入力値を利用する: map2 / pmap\nmap()では、指定した関数に与えることができるデータは一つであるが、複数のデータや異なる引数の値を関数に引き渡したい時にはmap()の派生であるmap2関数群を利用する。例えばmap2()は、map()の.x、.f引数に加えて.yをもち、ここでデータを指定できる。長さが等しい2つの文字列を入力に用いてpaste()を実行する例を示す。長さの異なるデータを与えた場合にはエラーとなる。\n\nmap2(\n  .x = c(\"Uryu\", \"Ishida\"),\n  .y = c(\"Shinya\", \"Motohiro\"),\n  # paste()は引数に与えられた値を文字列として結合する\n  .f = paste\n)\n\n[[1]]\n[1] \"Uryu Shinya\"\n\n[[2]]\n[1] \"Ishida Motohiro\"\n\n\n\n# .xには3つの要素が与えれるが.yは要素が2つしかない。\n# 要素の数が.xと.yで一致しない場合、処理は停止される\nmap2(\n  .x = c(\"Uryu\", \"Ishida\", \"Makiyama\"),\n  .y = c(\"Shinya\", \"Motohiro\"),\n  .f = paste\n)\n# Error: `.x` (3) and `.y` (2) are different lengths\n\nmap2()で指定した引数は既定では、.fの関数の第一引数(.x)と第二引数(.y)に与えられる。ラムダ式を用いる場合、.yのデータは”.y”あるいは”..2”を関数内で明示的に示す必要がある。ここで”.y”が使われているのは、第一引数の”.x”、“..1”と区分するためである。\n\n# 以下の結果は等しい\nmap2(\n  .x = c(\"Uryu\", \"Ishida\"),\n  .y = c(\"Shinya\", \"Motohiro\"),\n  # 無名関数の引数には.x, .yで与えられたデータが引き渡される\n  .f = function(x, y) paste(x, y)\n)\nmap2(\n  .x = c(\"Uryu\", \"Ishida\"),\n  .y = c(\"Shinya\", \"Motohiro\"),\n  .f = ~ paste(.x, .y)\n)\nmap2(\n  .x = c(\"Uryu\", \"Ishida\"),\n  .y = c(\"Shinya\", \"Motohiro\"),\n  .f = ~ paste(..1, ..2)\n)\n\nmap2()の利用方法として、次の例のようにデータ以外に引数の値を変更するのに使うこともできる。この例では、round()の引数digitsに異なる値を与えている。\n\nmap2(c(1, 2.1, 0.332),\n     c(1, 0, 2),\n     ~ round(.x, digits = .y))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 0.33\n\n\n2つ以上のデータを入力に用いる際、map3()、map4()といった具合に関数名の数値の部分を増やせば良い、という話ではない。2つ以上のデータを指定するにはpmap()を利用することになる。ラムダ式では”..3”、“..4”と添字を増やしていくことで多数の引数の値を受け取ることができる。\n\nx <- list(a = list(1:3),\n          b = list(4:6),\n          c = list(7:9))\n\n# .lで指定した要素は、順次.fの引数に渡される\npmap(.l = x,\n     .f = sum)\n\n[[1]]\n[1] 45\n\nx %>% pmap(\n  ~ ..1 + ..2 - ..3\n)\n\n[[1]]\n[1] -2 -1  0\n\n\npmap()に与えるデータの数よりも関数の引数が少ない場合はエラーになるが、引数…を含んでいる場合や…を引数にもった無名関数として定義することで処理を実行することができる。\n\nsum_xy <- function(a, b) {\n  a + b\n}\n\n\nx %>% pmap(sum_xy)\n# Error in .f(.l[[c(1L, 1L)]], .l[[c(2L, 1L)]], .l[[c(3L, 1L)]], ...) : \n#   unused argument (.l[[c(3, 1)]])\n\n\n# 引数に ... を含んだ関数では、\n# .lで指定される余分な要素に対しても処理は実行される\nsum_xy2 <- function(a, b, ...) {\n  a + b\n}\n\n# x[[1]]とx[[2]] の値が使われ、x[[3]]については利用されない\nx %>% \n  pmap(sum_xy2)\n\n[[1]]\n[1] 5 7 9\n\n\n\nx %>% \n  pmap(function(...) sum_xy(..1, ..2))\n\n[[1]]\n[1] 5 7 9\n\nx %>% \n  pmap(function(a, b, ...) a + b)\n\n[[1]]\n[1] 5 7 9\n\n\nRの関数は...引数をもつものが多いが、...引数を定義しない関数を実行する場合には、関数をラップし、...引数をもった自作の関数を定義すると良いだろう。\nmap2()、pmap()はいずれも返り値をベクトル化するmap2_int()のような関数が用意されている。ここで指定可能なデータ型と関数名は、先に説明したmap_*()と対応する。"
  },
  {
    "objectID": "ch5.html#まとめ",
    "href": "ch5.html#まとめ",
    "title": "5  効率的なデータ操作",
    "section": "5.2 まとめ",
    "text": "5.2 まとめ\n\nR言語徹底解説 関数型プログラミング云々の話はR言語徹底解説を参照するのが良い"
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6  tidyverseの組み合わせ",
    "section": "",
    "text": "前章までに扱ってきたtidyverseに含まれる個々のパッケージは、組み合わせて使うことでより多様な場面で活用することが期待される。例えばデータの読み込みをreadrで行い、パイプ処理でdplyrによる処理を適用できる。また、データフレームに含まれる文字列変数の操作にstringrを導入することも可能である。また、tidyrverseに含まれないパッケージであってもdplyrのデータ操作関数に適用しても良い。この章では、そのようなtidyverseのパッケージの組み合わせた活用方法を見ていくことにする。新たに用いる関数については都度解説を加えていくが、個別の関数の説明は該当するパッケージを扱っている章を参照してほしい。"
  },
  {
    "objectID": "ch6.html#snsデータの処理",
    "href": "ch6.html#snsデータの処理",
    "title": "6  tidyverseの組み合わせ",
    "section": "6.1 SNSデータの処理",
    "text": "6.1 SNSデータの処理\nそれではtidyverseパッケージを読み込もう。library(tidyverse)を実行すると1章で述べたように、tidyverseに含まれる複数のパッケージが一度に利用可能な状態となる。\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n✔ tibble  3.1.6     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nまた、追加で次のパッケージを読み込んでおく。これらは4章で扱った、文字列および日付・時間の処理を効率的に行うための関数を提供する。\n\n# tidyverseパッケージに含まれないパッケージを追加で利用可能にしておく\nlibrary(stringi)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n対象データとして、架空SNSのデータを利用する。こちらもreadrの関数を用いて作業スペースに保存しておこう。readrパッケージはtidyverseに含まれるパッケージであるため、すでにデータ読み込み関数read_csv()を呼び出せるようになっている。\n\n# library(readr) を実行する必要はない\ndf_sns <- \n  read_csv(\"data/sns.csv\")\n\nRows: 1000 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): post_id, address, place_name, user_id, nationality\ndttm (1): post_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# データの先頭行および各変数のデータ型を確認\ndf_sns\n\n# A tibble: 1,000 × 6\n   post_id      post_time           address       place_name user_id nationality\n   <chr>        <dttm>              <chr>         <chr>      <chr>   <chr>      \n 1 EM7O3RINyaLS 2016-04-01 09:37:43 石川県鳳珠郡… <NA>       A-8vIm… US         \n 2 vrpVbF83Kqhi 2016-04-01 20:54:12 大阪府大阪市… Gudetama … A-zpNo… TH         \n 3 vwPvcEXCOa67 2016-04-01 22:01:57 大阪府泉南郡… 南海電鐵 … C-hbtH… HK         \n 4 KPkonKKzmTep 2016-04-01 22:13:21 福島県福島市… <NA>       B-QUU8… ID         \n 5 rwaUYGGTYoqz 2016-04-02 08:18:09 千葉県成田市… Tokyo,…    B-JzaP… US         \n 6 ymuZbb5Mds54 2016-04-02 11:16:27 東京都江東区… Totem-Cir… A-G6sG… AU         \n 7 yeQWTgSsvPYJ 2016-04-02 20:06:11 長崎県佐世保… Huis Ten … C-Nxoo… SG         \n 8 beG3YjOzN4ac 2016-04-02 22:47:24 熊本県上益城… <NA>       C-xeQ4… CN         \n 9 ZZzCIGx6H5j7 2016-04-03 00:26:41 福岡県福岡市… <NA>       B-FkHj… HK         \n10 GNam2fNqbeiE 2016-04-03 01:36:18 東京都江戸川… <NA>       C-Bd0D… TH         \n# … with 990 more rows\n\n\n\n6.1.1 期間を指定したデータの抽出\ndf_snsの変数post_timeは日付・時間を示すPOSIXctクラスである。例えばこのような変数をもつデータに対して、特定の期間だけを抽出するというような処理を実行してみよう。これを行うには2つの方法が考えられ、一つはlubridateパッケージの演算子%within%とinterval()を用いる方法、もう一つはdplyr::between()の上限と下限を設定する方法である。\n\n# POSIXctの変数から2つの時期を指定して抽出する\n# %within% 演算子は lubridateパッケージに含まれる\ndf_filter_date1 <-\n  df_sns %>% \n  filter(\n    post_time %within% interval(ymd_hms(\"2016-06-01 00:00:00\"), ymd_hms(\"2016-06-05 23:59:59\"))\n  )\n\ndf_filter_date2 <- \n  df_sns %>% \n  filter(between(\n    post_time,\n    ymd_hms(\"2016-06-01 00:00:00\"),\n    ymd_hms(\"2016-06-05 23:59:59\")\n))\n\n# 2つの処理の結果は等しい\nall.equal(df_filter_date1, df_filter_date2)\n\nなおこの例では対象の期間を指定するためにlubridate::ymd_hms()を用いたが、Rの標準関数であるSys.time()やas.POSIXct()を使って作成したPOSIXctクラスのオブジェクトであっても問題はない。\n\n# ymd_hms()の代わりにas.POSIXct()を用いる方法\ndf_sns %>% \n  filter(between(\n    post_time,\n    as.POSIXct(1464739200, origin = \"1970-01-01\"),\n    as.POSIXct(1465171199, origin = \"1970-01-01\")\n))\n\n\n\n6.1.2 日付による集計\nSNSデータには、投稿時間とともにユーザを識別するuser_id列がある。これを利用することで、興味のある月ごとにどれだけの投稿があったのか、どれだけのユーザが活動していたのかということや、ユーザの利用状況について集計することができる。こうした処理について、dplyrによる実行例を見てみよう。\n\n6.1.2.1 利用期間の算出\n最初の例は、データ中でのユーザの利用期間について、ユーザ毎の最初と最後の投稿時間を得て、そこから算出するものである。この処理はいくつかの段階に分かれて行われる。順を追って説明しておこう。\nまずlubridate::as_date()で日付・時間の変数を日付へと変換する。次にユーザごとの処理を適用するためにgroup_by()を実行し、ユーザごとのデータの集計値としてsummarise()で投稿の回数をn()で、最初と最後の投稿時間をfirst()およびlast()を使って求めている。\n\ndf_sns_user_session <- \n  df_sns %>% \n  # 日付・時間の変数を日付データに変換した列を追加\n  mutate(date = as_date(post_time)) %>% \n  group_by(user_id) %>% \n  # 投稿の回数, 投稿日時から最初と最後のデータを取得する\n  summarise(n          = n(),\n            first_post = first(date),\n            last_post  = last(date))\n\ndf_sns_user_session\n\n# A tibble: 775 × 4\n   user_id        n first_post last_post \n   <chr>      <int> <date>     <date>    \n 1 A-0baeebfD     2 2016-05-12 2016-07-13\n 2 A-0BKhYkbN     1 2016-06-18 2016-06-18\n 3 A-0c8xSlst     2 2016-05-07 2016-05-14\n 4 A-0fASEKnR     1 2016-05-15 2016-05-15\n 5 A-0G36mkoc     1 2016-06-06 2016-06-06\n 6 A-0YECSFRb     1 2016-04-08 2016-04-08\n 7 A-1dtiP8Yq     2 2016-06-03 2016-06-18\n 8 A-1kIsN3FU     1 2016-05-27 2016-05-27\n 9 A-1MbeVyol     1 2016-06-15 2016-06-15\n10 A-1Po7EMHT     1 2016-06-25 2016-06-25\n# … with 765 more rows\n\n\nこの出力は、ユーザID user_id、投稿回数 n、投稿を行った最初と最後の日付 first_postおよびlast_postである。続いて、各ユーザの最初の投稿日と最後の投稿日からlubridate::interval()を使ってデータ中の利用期間を求める。ここで投稿が一度しかないユーザは、利用期間が0となる。また、interval()によって算出されるIntervalオブジェクトの結果を理解しやすくするため、期間を日数に変換した列を作成する。\n\ndf_sns_user_session_period <- \n  df_sns_user_session %>% \n  transmute(user_id,\n            n,\n            usage_period      = interval(first_post, last_post),\n            usage_period_days = as.numeric(usage_period, \"days\"))\n\ndf_sns_user_session_period\n\n# A tibble: 775 × 4\n   user_id        n usage_period                   usage_period_days\n   <chr>      <int> <Interval>                                 <dbl>\n 1 A-0baeebfD     2 2016-05-12 UTC--2016-07-13 UTC                62\n 2 A-0BKhYkbN     1 2016-06-18 UTC--2016-06-18 UTC                 0\n 3 A-0c8xSlst     2 2016-05-07 UTC--2016-05-14 UTC                 7\n 4 A-0fASEKnR     1 2016-05-15 UTC--2016-05-15 UTC                 0\n 5 A-0G36mkoc     1 2016-06-06 UTC--2016-06-06 UTC                 0\n 6 A-0YECSFRb     1 2016-04-08 UTC--2016-04-08 UTC                 0\n 7 A-1dtiP8Yq     2 2016-06-03 UTC--2016-06-18 UTC                15\n 8 A-1kIsN3FU     1 2016-05-27 UTC--2016-05-27 UTC                 0\n 9 A-1MbeVyol     1 2016-06-15 UTC--2016-06-15 UTC                 0\n10 A-1Po7EMHT     1 2016-06-25 UTC--2016-06-25 UTC                 0\n# … with 765 more rows\n\n\n\n\n6.1.2.2 期間中のカウント\n次は、任意の期間を設定し、期間中の投稿件数と活動のあったユーザの数を調べてみよう。ここでは月単位でのデータを扱うことにする。df_snsには月を示す変数がないため、まずは既存の変数を元に、投稿の年月を示すpost_ym列を用意する。\n\ndf_sns_yr <- \n  df_sns %>%\n  transmute(\n    user_id,\n    # 日付・時間の変数 post_timeの値から年、月を抽出\n    post_ym = str_c(year(post_time), \n                    # 月の桁数を2桁に揃えるためにstringr::str_pad()を利用\n                    str_pad(month(post_time), width = 2, pad = \"0\")))\n\nここでは、ユーザを識別するための user_id列以外の列は不要となるため、transmutate()を利用した。年および月の値の抽出にはlubridateの関数を使っている。また月の値が1,12と桁数が異なるため、stringr::str_pad()による桁揃えを実行した。これにより作成されたdf_sns_yrは次のようになっている。\n\ndf_sns_yr\n\n# A tibble: 1,000 × 2\n   user_id    post_ym\n   <chr>      <chr>  \n 1 A-8vImElYu 201604 \n 2 A-zpNoCiF2 201604 \n 3 C-hbtHidIz 201604 \n 4 B-QUU841zl 201604 \n 5 B-JzaPVHpY 201604 \n 6 A-G6sGnoFV 201604 \n 7 C-NxooWvcI 201604 \n 8 C-xeQ41RtH 201604 \n 9 B-FkHjECs0 201604 \n10 C-Bd0DCcdp 201604 \n# … with 990 more rows\n\n\nでは月ごとに全ユーザでどれだけの投稿があったかを求めよう。対象の行数を指定してカウントを行うcount()を実行するだけで計算が行われる。\n\n# 月ごとの投稿件数 n を求める\ndf_sns_users <- \n  df_sns_yr %>% \n  count(post_ym)\n\ndf_sns_users\n\n# A tibble: 4 × 2\n  post_ym     n\n  <chr>   <int>\n1 201604    146\n2 201605    379\n3 201606    325\n4 201607    150\n\n\n今度は、月ごとに投稿のあったユーザの数、いわゆるユニークユーザ数を調べてみよう。ユニークユーザを求めるには、1月の中で複数のデータがあるユーザに対して1つのデータとして処理する必要がある。これは次のようにdistinct()を使うことで対応できる。後の処理は上記の月ごとの投稿件数と同じであるが、最後にrename()による列名の変換を行う。\n\ndf_sns_unique_users <- \n  df_sns_yr %>% \n  # ユニークなデータとする組み合わせを指定\n  distinct(user_id, post_ym) %>% \n  count(post_ym) %>% \n  rename(unique_user = n)\n\nでは、月ごとのユニークユーザ数および投稿件数をまとめて見てみよう。共通のキーとなる変数をもった2つのデータを結合するleft_join()を利用する。引数byで指定するキー変数はここでは投稿月 post_ymである。先ほどユニークユーザ数の算出でカウントした変数名の変更を行ったのは、キー変数に追加しない含めないためである。\n\n# 2つのデータフレームを結合\ndf_sns_user_count <- \n  left_join(\n  df_sns_unique_users,\n  df_sns_users,\n  by = \"post_ym\"\n)\n\ndf_sns_user_count\n\n# A tibble: 4 × 3\n  post_ym unique_user     n\n  <chr>         <int> <int>\n1 201604          130   146\n2 201605          304   379\n3 201606          267   325\n4 201607          130   150\n\n\n最後に、この結果を可視化するためのコードを以下に示す。ユニークユーザ数も投稿件数も、同一の月ごとに計算されたカウントの値であるため、tidyr::gather()を使い、一つの列に記録することとした。また可視化の際のラベルに日本語を用いるためにrecode()による値の置換を追加している。\n\ndf_sns_user_count_plot <- \n  df_sns_user_count %>% \n  gather(\"type\", \"count\", -post_ym) %>% \n  mutate(type = recode(type,\n                       n = \"投稿件数\", \n                       unique_user = \"ユニークユーザ数\"))\n\ndf_sns_user_count_plot\n\n# A tibble: 8 × 3\n  post_ym type             count\n  <chr>   <chr>            <int>\n1 201604  ユニークユーザ数   130\n2 201605  ユニークユーザ数   304\n3 201606  ユニークユーザ数   267\n4 201607  ユニークユーザ数   130\n5 201604  投稿件数           146\n6 201605  投稿件数           379\n7 201606  投稿件数           325\n8 201607  投稿件数           150\n\n\n\nggplot(df_sns_user_count_plot,\n       aes(post_ym, count, group = type)) +\n  geom_point() + \n  geom_line(aes(lty = type))\n\n\n\n\n続いて、国籍・曜日別の投稿件数を調べるという処理を示す。曜日の情報は次のようにlubridate::wday()を使うことで、日付の要素を含んだデータから簡単に求めることができる。\n\ndf_sns_wod <- \n  df_sns %>% \n  # 国籍の情報がない行を除外\n  filter(!is.na(nationality)) %>% \n  transmute(nationality,\n            # 投稿時間から曜日を取得。日本語での表記を採用するため\n            # locale引数を調整する\n            wod = wday(post_time, label = TRUE, abbr = TRUE, locale = \"ja_JP.UTF-8\"))\n\ndf_sns_wod\n\n# A tibble: 979 × 2\n   nationality wod  \n   <chr>       <ord>\n 1 US          金   \n 2 TH          金   \n 3 HK          金   \n 4 ID          金   \n 5 US          土   \n 6 AU          土   \n 7 SG          土   \n 8 CN          土   \n 9 HK          日   \n10 TH          日   \n# … with 969 more rows\n\n\n次にwday(locale = \"ja_JP.UTF-8\")の指定により曜日の表記が日本語になったことに対応させて、国籍名も日本語に修正し、国籍・曜日別のデータの可視化を行おう。\n\ndf_sns_wod <- \n  df_sns_wod %>% \n  mutate(nationality = recode(nationality,\n                              AU = \"オーストラリア\",\n                              CN = \"中国\",\n                              ES = \"スペイン\",\n                              FR = \"フランス\",\n                              GB = \"イギリス\",\n                              HK = \"香港\",\n                              ID = \"インドネシア\",\n                              IN = \"インド\",\n                              KR = \"韓国\",\n                              MY = \"マレーシア\",\n                              PH = \"フィリピン\",\n                              SG = \"シンガポール\",\n                              TH = \"タイ\",\n                              TW = \"台湾\",\n                              US = \"アメリカ\"))\n\n\ndf_sns_wod %>% \n  count(wod, nationality) %>% \n  ggplot(aes(wod, n)) + \n  geom_bar(stat = \"identity\") +\n  xlab(NULL) +\n  ylab(\"投稿件数\") +\n  facet_wrap(~ nationality)\n\n\n\n\n同様に、国籍を大陸ごとにまとめてプロットを行う例を以下に示す。\n\ndf_sns_wod %>% \n  mutate(continent = fct_collapse(nationality,\n             アジア    = c(\"中国\", \"香港\", \"インドネシア\", \"インド\", \"韓国\", \n                         \"マレーシア\", \"フィリピン\", \"シンガポール\", \"タイ\", \"台湾\"),\n             アメリカ = c(\"アメリカ\"),\n             ヨーロッパ  = c(\"スペイン\", \"フランス\", \"イギリス\"),\n             オセアニア = c(\"オーストラリア\"))) %>% \n  count(wod, continent) %>% \n  ggplot(aes(wod, n)) + \n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ continent) +\n  theme_bw(base_size = 14) \n\n\n\n\n\n\n\n6.1.3 地名の分割\ndf_snsに記録されているaddress列は、都道府県名、市区町村名、町字の3要素に分解可能な住所文字列である。これをdplyrを使い、データフレームを対象とした操作で個々の要素に分割した列を作成する処理を考えてみよう。\n\n# addressには住所が記録されている\ndf_sns$address[1:5]\n\n[1] \"石川県鳳珠郡 能登町\"            \"大阪府大阪市 北区角田町5\"      \n[3] \"大阪府泉南郡 田尻町泉州空港中1\" \"福島県福島市入江町1\"           \n[5] \"千葉県成田市木の根3\"           \n\n\nここではstringrパッケージの関数を用いる例を説明する。都道府県と市区町村の抽出にはそれぞれ、文字列置換の関数str_replace()と文字列分割の関数str_split()を使う。データフレームに対して処理を適用する前にまずはベクトルを操作対象として結果を確認しておこう。\n都道府県名を取得するために、「都」、「道」、「府」、「県」を境界とし、前半を都道府県名にするパターンが考えられる。一方でこのパターンでは「京都府」の分割に失敗してしまう。そこで「都」の代わりに「東京都」を指定した次のパターンを実行することにする。\n\n# 抽出に失敗するパターン\nstr_replace(\n  c(\"東京都渋谷区桜ヶ丘\", \n    \"岡山県岡山市北区清心町\", \n    \"茨城県つくば市小野川\",\n    \"京都府舞鶴市字浜\"), \n  pattern     = \"(都|道|府|県).+\", \n  replacement = \"\\\\1\")\n\n[1] \"東京都\" \"岡山県\" \"茨城県\" \"京都\"  \n\n# 都道府県名の抽出\n# 一致したパターンを再利用するために後方参照を行う\nstr_replace(df_sns$address[5:10], \n            pattern     = \"(東京都|道|府|県).+\", \n            replacement = \"\\\\1\")\n\n[1] \"千葉県\" \"東京都\" \"長崎県\" \"熊本県\" \"福岡県\" \"東京都\"\n\n\n次に市区町村名であるが、これは先ほどの都道府県以降の文字列を得るために分割を行い、その後改めて後方参照による文字列置換を行うという方法をとる。\n\n# 市区町村名の抽出\nstr_split(df_sns$address[15:20], \n          pattern = \"(東京都|道|府|県)\", \n          n       = 2) %>% \n  purrr::map_chr(2) %>% \n  str_replace(string      = ., \n              pattern     = \"(区|市|.+市 .+区|町|村).+\", \n              replacement =  \"\\\\1\")\n\n[1] \"成田市\"        \"泉南郡 田尻町\" \"渋谷区\"        \"新宿区\"       \n[5] \"御殿場市\"      \"松戸市\"       \n\n\nベクトルでの結果を確認したら、次はその処理をdplyrを使いデータフレームへ適用しよう。変数の参照にデータフレームのオブジェクト名とドル記号を省略し、変数名を直接指定したものを、transmute()あるいはmutate()へとコピーペーストするだけで良い。\n\n# transmute内の変数への処理は上記のコードをコピーペーストしたもの\ndf_sns %>% \n  transmute(address,\n            geo_prefecture = str_replace(address, \"(東京都|道|府|県).+\", \"\\\\1\"),\n            geo_city       = str_split(address, \"(東京都|道|府|県)\", n = 2) %>% \n              map_chr(2) %>% \n              str_replace(string      = ., \n                          pattern     = \"(区|市|.+市 .+区|町|村).+\", \n                          replacement =  \"\\\\1\"))\n\n# A tibble: 1,000 × 3\n   address                         geo_prefecture geo_city       \n   <chr>                           <chr>          <chr>          \n 1 石川県鳳珠郡 能登町             石川県         鳳珠郡 能登町  \n 2 大阪府大阪市 北区角田町5        大阪府         大阪市 北区    \n 3 大阪府泉南郡 田尻町泉州空港中1  大阪府         泉南郡 田尻町  \n 4 福島県福島市入江町1             福島県         福島市         \n 5 千葉県成田市木の根3             千葉県         成田市         \n 6 東京都江東区青海一丁目1         東京都         江東区         \n 7 長崎県佐世保市ハウステンボス町8 長崎県         佐世保市       \n 8 熊本県上益城郡 嘉島町           熊本県         上益城郡 嘉島町\n 9 福岡県福岡市 中央区             福岡県         福岡市         \n10 東京都江戸川区東葛西五丁目19    東京都         江戸川区       \n# … with 990 more rows\n\n\n\n\n6.1.4 入れ子データへのグループ処理\nグループ化したデータに含まれる値を操作するには、データを入れ子にしておくと便利である。ここではtidyrとpurrrによる入れ子データへのグループ処理の例を見ていこう。\nまず処理を別々に適用したい入れ子データの作成であるが、これはtidyr::nest()を直接使う方法とdplyr::group_by()を間にはさむ実行方法がある。次の例はdf_snsデータの国籍 nationality列ごとにデータをグループ化し、入れ子データとして格納するものであるが、いずれも結果は等しい。\n\ndf_sns_nest_nations <- \n  df_sns %>% \n  group_by(nationality) %>% \n  nest()\n\n# 上記のコードと同じ処理を実行\ndf_sns_nest_nations <- \n  df_sns %>% \n  nest(-nationality)\n\nWarning: All elements of `...` must be named.\nDid you want `data = -nationality`?\n\n\nでは国籍ごとに投稿件数が多いユーザ上位3名のuser_idおよび投稿件数を抽出する処理を実行しよう。ここで入れ子データへの関数の適用はpurrrのmap*()を使う。\n\ndf_sns_nest_nations %>% \n  transmute(\n    nationality,\n    max_n = map(data, ~\n      .x %>% \n        count(user_id) %>% \n        top_n(3, wt = n))\n  ) %>% \n  unnest()\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(max_n)`\n\n\n# A tibble: 165 × 3\n   nationality user_id        n\n   <chr>       <chr>      <int>\n 1 US          A-Tz38wKFN     3\n 2 US          C-EWrFgfFO     3\n 3 US          C-vXf2Pfso     3\n 4 TH          A-hmIIeiyU     3\n 5 TH          B-3rIzdYs2    10\n 6 TH          C-v48Px9pW     4\n 7 TH          C-VaZADtzv     3\n 8 HK          C-3njvd4vb     9\n 9 HK          C-hbtHidIz     5\n10 HK          C-OvYgclto     5\n# … with 155 more rows\n\n\nもう一つ、今度は国籍別の投稿件数とユニークユーザ数を求める処理を示す。\n\ndf_sns_nest_nations %>% \n  transmute(\n    nationality,\n    n = map_int(data, ~ nrow(..1)),\n    uu = map_int(data, ~ ..1 %>% distinct(user_id) %>% \n                   nrow())\n  )\n\n# A tibble: 16 × 3\n   nationality     n    uu\n   <chr>       <int> <int>\n 1 US            152   134\n 2 TH            166   131\n 3 HK             58    35\n 4 ID            119    97\n 5 AU             23    23\n 6 SG             29    29\n 7 CN             84    62\n 8 MY             30    26\n 9 PH             77    67\n10 KR            138    67\n11 GB             34    32\n12 ES             20    16\n13 FR             19    14\n14 <NA>           21    21\n15 TW             29    24\n16 IN              1     1"
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7  オープンデータの整形",
    "section": "",
    "text": "オープンデータとは、許可される範囲内で誰もが自由に利用・再利用・再配布できるデータのことを指す。日本でも、政府や地方行政、研究機関が統計調査の結果を公開する例が増えているが、公開されたデータはRでの処理をすぐに実行できない状態であることがしばしばある。これらのファイルもtidyverseのパッケージを使うことで読み込みからデータ整形、可視化までの作業を一貫して行うことが可能である。本章では、実際に公開されているオープンデータを例にtidyverseによる作業の例を紹介しよう。"
  },
  {
    "objectID": "ch7.html#政府統計の総合窓口-e-stat",
    "href": "ch7.html#政府統計の総合窓口-e-stat",
    "title": "7  オープンデータの整形",
    "section": "7.1 政府統計の総合窓口 e-Stat",
    "text": "7.1 政府統計の総合窓口 e-Stat\n政府統計の総合窓口 (e-Stat) は日本国内の政府統計のポータルサイトである。各府省等が公表する各種の統計情報を閲覧、ダウンロード可能であり、Rなどのプログラムを介したデータ参照を行うためのAPIも整備されている。\nRからはe-StatのAPIを操作するestatapiパッケージを使うことで、提供されるデータの一覧や、データの読み込み、ファイルダウンロードが可能となる。APIの利用にはe-Statでのメールアドレス等の登録が必要となるが、ここではあらかじめ用意したアプリケーションIDを用いる例を紹介する。なお本書の付録として、e-Statからダウンロードしたデータを掲載しているので、アプリケーションIDの取得を行わないでも実行可能である。\n\nlibrary(estatapi)\n\nこのサービスは、政府統計総合窓口(e-Stat)のAPI機能を使用していますが、サービスの内容は国によって保証されたものではありません。\n\n\n\n7.1.1 学校保健統計調査\n「学校保健統計調査」は、学校における幼児、児童及び生徒の発育及び健康の状態を明らかにすることを目的に行われている。調査の対象は、文部科学大臣があらかじめ指定する学校に在籍する満5歳から17歳（4月1日現在）までの幼児、児童及び生徒となっている。調査は毎年実施され、学校保健安全法により義務づけられている健康診断の結果に基づき、発育及び健康状態に関する事項（身長、体重及び被患率等）に関する調査結果が記録されている。\nこのデータがe-StatのAPIで取得可能かどうか、まずはestatapiパッケージのestat_getStatsList()で調べてみよう。estat_getStatsList()には引数appIdがあり、ここに取得したアプリケーションIDを指定する。\n\ndf_list <- \n  estat_getStatsList(appId      = \"<APIで取得したアプリケーションID>\", \n                     searchWord = \"学校保健統計調査\")\n\ndf_listを見ると、学校保健統計調査に関する様々なデータがあることが確認できる。その中から、今回は「学校保健統計調査による身体発育値」の名目で記録されたデータを抽出してみよう。\n\ndf_list %>% \n  filter(str_detect(TITLE, regex(\"学校保健統計調査による身体発育値\")))\n\n# A tibble: 5 × 13\n  `@id`      STAT_NAME GOV_ORG STATISTICS_NAME TITLE CYCLE SURVEY_DATE OPEN_DATE\n  <chr>      <chr>     <chr>   <chr>           <chr> <chr> <chr>       <chr>    \n1 0003079318 学校保健… 文部科… 学校保健統計調… （参… 年度… 0           2014-03-…\n2 0003107473 学校保健… 文部科… 学校保健統計調… （参… 年度… 201404-201… 2015-03-…\n3 0003107409 学校保健… 文部科… 学校保健統計調… （参… 年度… 201404-201… 2015-03-…\n4 0003146920 学校保健… 文部科… 学校保健統計調… （参… 年度… 201504-201… 2016-03-…\n5 0003146941 学校保健… 文部科… 学校保健統計調… （参… 年度… 201504-201… 2016-03-…\n# … with 5 more variables: SMALL_AREA <chr>, MAIN_CATEGORY <chr>,\n#   SUB_CATEGORY <chr>, OVERALL_TOTAL_NUMBER <chr>, UPDATED_DATE <chr>\n\n\n「学校保健統計調査による身体発育値」の項目には5件のデータが該当している。STATISTICS_NAME列を見ると、データは平成22年度以降、平成26年度、平成27年度の3期間に渡り、平成26年と27年ではLMS法を採用した項目が別にあることが確認できる。\nestatapiパッケージを使ったe-Statのデータ取得には、対象データの第一列に記録された”@id”の値が必要となる。そのため、取得対象のデータを決めたらこの値を控えておくと良い。次の処理では、データフレーム中の”@id”列の値を参照するが、LMS法の項目を区別するために正規表現による抽出を試みている。\n\n# 「学校保健統計調査による身体発育値」のデータを取得するためのIDを控えておく\nids <- \n  df_list %>% \n  # regex()内で正規表現のパターンマッチを行い、「身体発育値」で終わる項目を抽出する\n  filter(str_detect(TITLE, regex(\"学校保健統計調査による身体発育値$\"))) %>% \n  .$`@id`\n\nids\n\n[1] \"0003079318\" \"0003107473\" \"0003146920\"\n\n\n抽出されたデータの件数は3件である。次はこの”@id”から、R上にデータを読み込んでみよう。データ取得はestat_getStatsData()で対象の統計データIDおよびアプリケーションIDを指定して行う。\n\nestat_getStatsData(\n      appId = config::get(\"estat_token\"),\n      statsDataId = ids[1])\n\n学校保健統計調査は年に一度であるが、今回取得可能なデータの件数は3期間分ある。これを一つのデータフレームとして処理するには、次のようにpurrr::map_dfr()を使い、estat_getStatsData()のstatsDataId引数にidsの要素を一つずつ渡して実行し、行方向への結合を行うと良い。\n\ndf_age_height <- \n  ids %>% \n  map_dfr(\n    ~ estat_getStatsData(\n      appId = \"<API token>\",\n      statsDataId = .x\n    )\n  )\n\nそれではtibble::glimpse()を使い取得したデータを確認しよう。\n\nglimpse(df_age_height)\n\nRows: 2,184\nColumns: 14\n$ tab_code                     <chr> \"0000300\", \"0000300\", \"0000300\", \"0000300…\n$ 表章項目                     <chr> \"身長のパーセンタイル値(cm)\", \"身長のパー…\n$ cat01_code                   <chr> \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\",…\n$ 性別                         <chr> \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\",…\n$ cat03_code                   <chr> \"00000100\", \"00000100\", \"00000100\", \"0000…\n$ パーセンタイル               <chr> \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\",…\n$ cat06_code                   <chr> \"10\", \"10\", \"20\", \"20\", \"30\", \"30\", \"40\",…\n$ `学校種別-年齢別（5～17歳）` <chr> \"幼稚園（5歳）\", \"幼稚園（5歳）\", \"小学校…\n$ area_code                    <chr> \"00000\", \"00000\", \"00000\", \"00000\", \"0000…\n$ 都道府県別                   <chr> \"全国\", \"全国\", \"全国\", \"全国\", \"全国\", \"…\n$ time_code                    <chr> \"2013000000\", \"2012000000\", \"2013000000\",…\n$ `時間軸(年次)`               <chr> \"2013年\", \"2012年\", \"2013年\", \"2012年\", \"…\n$ value                        <dbl> 101.7, 101.6, 107.5, 107.7, 113.0, 113.3,…\n$ `時間軸（年度次）`           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n次に、ここからdplyrのデータ操作関数を使いながらデータを加工していこう。まずは今後使う列だけを選択する。また、この際に日本語の列名から適当な列名に変更を行う。各列の値はそれぞれ以下の項目を示している。\ntype: 計測項目。身長、体重、座高の3種がある。\ngender: 性別。「男」と「女」の二値が記録されている。\npercent: データのパーセンタイル値。\nage: 対象者の学年および年齢。\nfiscal_year: 調査の実施年度。\nvalue: 計測項目 typeごとの計測値。\n\ndf_age_height <- \n  df_age_height %>% \n  select(type = `表章項目`,\n         gender = `性別`,\n         percent = `パーセンタイル`,\n         age = `学校種別-年齢別（5～17歳）`,\n         fiscal_year = `時間軸(年次)`,\n         value)\n\nglimpse(df_age_height)\n\nRows: 2,184\nColumns: 6\n$ type        <chr> \"身長のパーセンタイル値(cm)\", \"身長のパーセンタイル値(cm)\"…\n$ gender      <chr> \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\"…\n$ percent     <chr> \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\"…\n$ age         <chr> \"幼稚園（5歳）\", \"幼稚園（5歳）\", \"小学校（6歳）\", \"小学校…\n$ fiscal_year <chr> \"2013年\", \"2012年\", \"2013年\", \"2012年\", \"2013年\", \"2012年\"…\n$ value       <dbl> 101.7, 101.6, 107.5, 107.7, 113.0, 113.3, 118.2, 118.6, 12…\n\n\ntype、ageの列には、括弧の中に計測値の単位、対象者の年齢が記録されている。これをtidyrのextract()で新たな列として独立させよう。\n\ndf_age_height <- \n  df_age_height %>% \n  extract(type, into = c(\"type\", \"unit\"),\n           regex = \"(.+)(\\\\([[:alnum:]]{1,}+\\\\))\") %>% \n  extract(age, into = c(\"school\", \"age\"),\n          regex = \"(^.+)(（.+$)\")\n\nglimpse(df_age_height)\n\nRows: 2,184\nColumns: 8\n$ type        <chr> \"身長のパーセンタイル値\", \"身長のパーセンタイル値\", \"身長…\n$ unit        <chr> \"(cm)\", \"(cm)\", \"(cm)\", \"(cm)\", \"(cm)\", \"(cm)\", \"(cm)\", \"(…\n$ gender      <chr> \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\", \"男\"…\n$ percent     <chr> \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\", \"3%\"…\n$ school      <chr> \"幼稚園\", \"幼稚園\", \"小学校\", \"小学校\", \"小学校\", \"小学校\"…\n$ age         <chr> \"（5歳）\", \"（5歳）\", \"（6歳）\", \"（6歳）\", \"（7歳）\", \"（…\n$ fiscal_year <chr> \"2013年\", \"2012年\", \"2013年\", \"2012年\", \"2013年\", \"2012年\"…\n$ value       <dbl> 101.7, 101.6, 107.5, 107.7, 113.0, 113.3, 118.2, 118.6, 12…\n\n\n次に、単位のついたageとfiscal_yearの列を対象として、数値を抽出する処理を実行する。ここでは対象列の指定にmutate_at()、数値の抽出にparse_number()を適用する。また、mutate_at()の適用後も、変数は元の文字列のデータ型を保持しているため、これをtype_convert()を使い数値変数として扱えるようにしておこう。\n\ndf_age_height <- \n  df_age_height %>% \n  mutate_at(vars(c(\"age\", \"fiscal_year\")), parse_number) %>% \n  type_convert()\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  type = col_character(),\n  unit = col_character(),\n  gender = col_character(),\n  percent = col_character(),\n  school = col_character()\n)\n\n\n\ndf_age_height\n\n# A tibble: 2,184 × 8\n   type                   unit  gender percent school   age fiscal_year value\n   <chr>                  <chr> <chr>  <chr>   <chr>  <dbl>       <dbl> <dbl>\n 1 身長のパーセンタイル値 (cm)  男     3%      幼稚園     5        2013  102.\n 2 身長のパーセンタイル値 (cm)  男     3%      幼稚園     5        2012  102.\n 3 身長のパーセンタイル値 (cm)  男     3%      小学校     6        2013  108.\n 4 身長のパーセンタイル値 (cm)  男     3%      小学校     6        2012  108.\n 5 身長のパーセンタイル値 (cm)  男     3%      小学校     7        2013  113 \n 6 身長のパーセンタイル値 (cm)  男     3%      小学校     7        2012  113.\n 7 身長のパーセンタイル値 (cm)  男     3%      小学校     8        2013  118.\n 8 身長のパーセンタイル値 (cm)  男     3%      小学校     8        2012  119.\n 9 身長のパーセンタイル値 (cm)  男     3%      小学校     9        2013  123.\n10 身長のパーセンタイル値 (cm)  男     3%      小学校     9        2012  123.\n# … with 2,174 more rows\n\n\nこうして加工した学校保健統計調査のデータを、最後にggplot2を使って可視化してみよう。まずは3種の計測項目(“type”)のデータのばらつきをgeom_density()で表示させる。複数の項目からなるデータを、項目別に描画するには次のようにfacet_wrap()を使うと良い。\n\ndf_age_height %>% \n  ggplot(aes(value, color = gender)) + \n  geom_density() +\n  # typeごとに図を分ける。\n  facet_wrap(~ type, \n             # x軸はtypeごとに独立した軸をもつことを指定する\n             scales = \"free_x\",\n             # 分割して描画する図の列数\n             ncol = 3)\n\n\n\n\n次の図は、2013年に記録された男女の身長データについてプロットしたものである。まずfilter()を使い対象のデータを抽出する。x軸には年齢を指定するが、ここで元の”age”が数値であるためにforcats::as_factor()を使い因子へと変換させた。\n\ndf_age_height %>%\n  filter(str_detect(type, \"身長\"), \n         fiscal_year == 2013) %>% \n  ggplot(aes(forcats::as_factor(as.character(age)), value, \n             group = percent, color = percent)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"年齢\") +\n  ylab(\"身長 (cm)\")\n\n\n\n  facet_wrap(~ gender, ncol = 1)\n\n<ggproto object: Class FacetWrap, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetWrap, Facet, gg>"
  },
  {
    "objectID": "ch7.html#気象庁の過去データ",
    "href": "ch7.html#気象庁の過去データ",
    "title": "7  オープンデータの整形",
    "section": "7.2 気象庁の過去データ",
    "text": "7.2 気象庁の過去データ\n気象庁のウェブサイト ()では、過去の起床データをダウンロードできるサービスが提供されている。ここでは、観測地点「つくば（館野）」での2017年における気象データを用意した。このデータを使い、気象庁のデータ整形と簡単な可視化を行おう。このデータを使わなくても気象庁の「過去の気象データ・ダウンロード」ページでダウンロードしたファイルには同様の処理が有効だろう。\nまずダウンロードしたファイルの読み込みを実施する。元のデータはcsv形式で提供されるが、エンコーディングがWindows向けとなっているため、文字化けを防ぐためにreadr::locale()でエンコードの指定する。また先頭行はファイルをダウンロードした時刻と対象の観測所名が記録されているので、これらの読み込みを引数skipで除外しよう。\n\ndf_weather <- \n  read_csv(\"data-raw/jma_tsukubka_2017data.csv\", \n           locale = locale(encoding = \"cp932\"), \n           skip = 3)\n\nNew names:\nRows: 367 Columns: 24\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(17): 年月日, 平均気温(℃)...3, 平均気温(℃)...4, 最低気温(℃)...6,\n最低気温(℃)...7, 最高気温(℃)..... dbl (6): 平均気温(℃)...2, 最低気温(℃)...5,\n最高気温(℃)...8, 降水量の合計(mm)...11, 日照時間(時間)..... lgl (1):\n平均雲量(10分比)...22\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `平均気温(℃)` -> `平均気温(℃)...2`\n• `平均気温(℃)` -> `平均気温(℃)...3`\n• `平均気温(℃)` -> `平均気温(℃)...4`\n• `最低気温(℃)` -> `最低気温(℃)...5`\n• `最低気温(℃)` -> `最低気温(℃)...6`\n• `最低気温(℃)` -> `最低気温(℃)...7`\n• `最高気温(℃)` -> `最高気温(℃)...8`\n• `最高気温(℃)` -> `最高気温(℃)...9`\n• `最高気温(℃)` -> `最高気温(℃)...10`\n• `降水量の合計(mm)` -> `降水量の合計(mm)...11`\n• `降水量の合計(mm)` -> `降水量の合計(mm)...12`\n• `降水量の合計(mm)` -> `降水量の合計(mm)...13`\n• `降水量の合計(mm)` -> `降水量の合計(mm)...14`\n• `日照時間(時間)` -> `日照時間(時間)...15`\n• `日照時間(時間)` -> `日照時間(時間)...16`\n• `日照時間(時間)` -> `日照時間(時間)...17`\n• `日照時間(時間)` -> `日照時間(時間)...18`\n• `平均風速(m/s)` -> `平均風速(m/s)...19`\n• `平均風速(m/s)` -> `平均風速(m/s)...20`\n• `平均風速(m/s)` -> `平均風速(m/s)...21`\n• `平均雲量(10分比)` -> `平均雲量(10分比)...22`\n• `平均雲量(10分比)` -> `平均雲量(10分比)...23`\n• `平均雲量(10分比)` -> `平均雲量(10分比)...24`\n\n\nデータを確認すると、平均気温、最高気温などの各項目にその値と品質情報、均質番号が記録された列があることがわかる。欲しいのは項目の値であるので品質情報および均質番号の列はデータから削除しよう。これには、品質情報および均質番号の列が最初の行だけ値があり、あとは欠損値であることを利用して、対象の変数をベクトルとして得る次の処理を実行すると良いだろう。\n\nglimpse(df_weather)\n\nRows: 367\nColumns: 24\n$ 年月日                  <chr> NA, NA, \"2017/1/1\", \"2017/1/2\", \"2017/1/3\", \"2…\n$ `平均気温(℃)...2`       <dbl> NA, NA, 4.0, 3.9, 4.8, 5.7, 3.6, 0.7, 0.8, 3.0…\n$ `平均気温(℃)...3`       <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `平均気温(℃)...4`       <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `最低気温(℃)...5`       <dbl> NA, NA, -3.0, -1.4, -2.2, -2.4, -3.2, -5.6, -5…\n$ `最低気温(℃)...6`       <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `最低気温(℃)...7`       <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `最高気温(℃)...8`       <dbl> NA, NA, 13.3, 12.5, 13.8, 14.1, 9.7, 8.3, 8.8,…\n$ `最高気温(℃)...9`       <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `最高気温(℃)...10`      <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `降水量の合計(mm)...11` <dbl> NA, NA, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.5…\n$ `降水量の合計(mm)...12` <chr> NA, \"現象なし情報\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ `降水量の合計(mm)...13` <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `降水量の合計(mm)...14` <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `日照時間(時間)...15`   <dbl> NA, NA, 9.5, 4.9, 9.3, 9.2, 8.9, 8.1, 9.3, 0.3…\n$ `日照時間(時間)...16`   <chr> NA, \"現象なし情報\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ `日照時間(時間)...17`   <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `日照時間(時間)...18`   <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `平均風速(m/s)...19`    <dbl> NA, NA, 1.4, 1.3, 2.0, 2.3, 1.8, 1.3, 1.1, 2.2…\n$ `平均風速(m/s)...20`    <chr> NA, \"品質情報\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", …\n$ `平均風速(m/s)...21`    <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n$ `平均雲量(10分比)...22` <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `平均雲量(10分比)...23` <chr> NA, \"品質情報\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", …\n$ `平均雲量(10分比)...24` <chr> NA, \"均質番号\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n\nselect_vars <-\n  df_weather %>%\n  slice(2L) %>%\n  select_if(.predicate = is.na(.)) %>%\n  names()\n\ndf_weather <- \n  df_weather %>% \n  slice(-c(1:2L)) %>% \n  select(select_vars)\n\nNote: Using an external vector in selections is ambiguous.\nℹ Use `all_of(select_vars)` instead of `select_vars` to silence this message.\nℹ See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis message is displayed once per session.\n\n\n\ndf_weather <- \n  df_weather %>% \n  type_convert() %>% \n  rename(date = `年月日`) %>% \n  mutate(date = ymd(date))\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  年月日 = col_character()\n)\n\ndf_weather %>% \n  # extract(`平均気温(℃)`, into = \"temp_mean\", convert = TRUE) %>% \n  # extract(`最低気温(℃)`, into = \"temp_min\", convert = TRUE) %>% \n  # extract(`最高気温(℃)`, into = \"temp_max\", convert = TRUE) %>% \n  gather(type, value, -date, convert = TRUE) %>% \n  type_convert()\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  type = col_character()\n)\n\n\n# A tibble: 2,555 × 3\n   date       type            value\n   <date>     <chr>           <dbl>\n 1 2017-01-01 平均気温(℃)...2   4  \n 2 2017-01-02 平均気温(℃)...2   3.9\n 3 2017-01-03 平均気温(℃)...2   4.8\n 4 2017-01-04 平均気温(℃)...2   5.7\n 5 2017-01-05 平均気温(℃)...2   3.6\n 6 2017-01-06 平均気温(℃)...2   0.7\n 7 2017-01-07 平均気温(℃)...2   0.8\n 8 2017-01-08 平均気温(℃)...2   3  \n 9 2017-01-09 平均気温(℃)...2   5.6\n10 2017-01-10 平均気温(℃)...2   6  \n# … with 2,545 more rows\n\ndf_weather_temp <- \n  df_weather %>% \n  select(1, contains(\"気温\"))\n\n\ndf_weather_temp %>% \n  gather(class, value, -date) %>% \n  ggplot(aes(date, value, color = class)) +\n  geom_line()"
  },
  {
    "objectID": "ch7.html#青果物卸売市場調査平成29年年間計及び月別結果農林水産省",
    "href": "ch7.html#青果物卸売市場調査平成29年年間計及び月別結果農林水産省",
    "title": "7  オープンデータの整形",
    "section": "7.3 「青果物卸売市場調査（平成29年年間計及び月別結果）」（農林水産省）",
    "text": "7.3 「青果物卸売市場調査（平成29年年間計及び月別結果）」（農林水産省）\n\nlibrary(readxl)\n\nxx <- c(\"卸売数量\", \"卸売価額\", \"卸売価格\")\n\ndf_1 <- \n  read_excel(\"data-raw/seika_oroshi17.xls\", sheet = 7, skip = 5, \n             col_names = c(NA, \n                           \"国産_輸入\", \"品目\", \"種別\",\n                           NA,\n                           str_c(\"主要都市の市場計\", \"_\", xx),\n                           str_c(\"対　　前　　年　　比\", \"_\", xx),\n                           str_c(\"１月\", \"_\", xx),\n                           str_c(\"２月\", \"_\", xx),\n                           str_c(\"３月\", \"_\", xx),\n                           NA)) %>% \n  mutate_at(vars(contains(\"月\")), .funs = funs(recode(., `-` = NA_character_))) %>% \n  slice(-c(1:4)) %>% \n  select(-num_range(\"X__\", 1:3)) %>% \n  fill(`国産_輸入`, `品目`, .direction = \"down\") %>% \n  drop_na(`国産_輸入`)\n\ndf_2 <- \n  read_excel(\"data-raw/seika_oroshi17.xls\", sheet = 8, skip = 5,\n             col_names = c(NA, \n                           \"国産_輸入\", \"品目\", \"種別\",\n                           NA,\n                           str_c(\"４月\", \"_\", xx),\n                           str_c(\"５月\", \"_\", xx),\n                           str_c(\"６月\", \"_\", xx),\n                           str_c(\"７月\", \"_\", xx),\n                           str_c(\"８月\", \"_\", xx),\n                           str_c(\"９月\", \"_\", xx),\n                           NA)) %>% \n  mutate_at(vars(contains(\"月\")), .funs = funs(recode(., `-` = NA_character_))) %>% \n  slice(-c(1:4)) %>% \n  select(-num_range(\"X__\", 1:3)) %>% \n  fill(`国産_輸入`, `品目`, .direction = \"down\") %>% \n  drop_na(`国産_輸入`)\n\ndf_3 <- \n  read_excel(\"data-raw/seika_oroshi17.xls\", sheet = 9, skip = 5,\n           col_names = c(NA, \"国産_輸入\", \"品目\", \"種別\",\n                         NA,\n                         str_c(\"10月\", \"_\", xx),\n                         str_c(\"11月\", \"_\", xx),\n                         str_c(\"12月\", \"_\", xx))) %>% \n  mutate_at(vars(contains(\"月\")), .funs = funs(recode(., `-` = NA_character_))) %>% \n  slice(-c(1:4)) %>% \n  select(-num_range(\"X__\", 1:2)) %>% \n  fill(`国産_輸入`, `品目`, .direction = \"down\") %>% \n  drop_na(`国産_輸入`)\n\n\ndf_seika <- \n  left_join(df_1, df_2, by = c(\"国産_輸入\", \"品目\", \"種別\")) %>% \n  left_join(df_3, by = c(\"国産_輸入\", \"品目\", \"種別\")) %>% \n  filter_at(vars(`品目`, `種別`), any_vars(!is.na(.))) %>% \n  group_by_at(vars(-contains(\"主要都市の市場計\"), -contains(\"対　　前　　年　　比\"))) %>% \n  nest() %>%\n  gather(key, value, -`国産_輸入`, -`品目`, -`種別`, -data) %>% \n  separate(key, c(\"month\", \"key\"), sep = \"_\") %>% \n  type_convert()\n\n\ndf_seika %>% \n  filter(key == \"卸売数量\", month == \"１月\") %>% \n  group_by(`品目`) %>% \n  summarise(value = sum(value)) %>% \n  mutate(`品目` = forcats::fct_reorder(`品目`, value)) %>% \n  drop_na() %>% \n  ggplot(aes(`品目`, value)) +\n  geom_bar(stat = \"identity\")\n\ndf_seika %>% \n  filter(key == \"卸売数量\", `品目` == \"りんご計\", !is.na(種別)) %>% \n  ggplot(aes(month, value, color = `種別`, group = `種別`)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ `国産_輸入`)"
  },
  {
    "objectID": "locale.html",
    "href": "locale.html",
    "title": "Appendix E — コラム: ロケール",
    "section": "",
    "text": "Rのロケール設定について概要を述べる。\nRではSys.getlocale()を引数なしで実行することで現在の環境でのロケールを確認することができる。引数の値として、ロケールのカテゴリーを与えた場合には、特定のカテゴリーのロケールが返される。\nSys.getlocale()などで出力される値は実行環境によって異なる可能性がある。上記の出力は著者の環境設定のものとなっている。Sys.getlocale()で出力された値をみるとロケールはen_US.UTF-8となっている。これはロケールの設定に英語 enを指定し、その中でアメリカ合衆国での表記を採用し、文字コードとしてUTF-8を利用することがわかる。ロケールはこのように、利用する言語と国コードによって表現されている。\nなお、ロケールを指定する際のSys.setlocale()でのlocale引数に与える書式は、使用するコンピュータのオペレーションシステムによって異なるため注意が必要である。\n現在のロケールを変更するにはSys.setlocale()を利用する。ここではロケールに日本語の設定を指定して時間の表記で日本語を利用するように変更してみたい。ロケールを設定するにはSys.setlocale()関数の引数categoryとlocaleに対象のロケールカテゴリーとロケール名を指定する。Sys.setlocale()の引数categoryにはLC_COLLATE、LC_CTYPE、LC_MONETARY、LC_NUMERIC、LC_TIMEといったロケールを規定するカテゴリーを選択するが、通常はLC_ALLだけを指定することによって、これらのすべてのロケールを次のlocale引数の値に統一しておくのが良いだろう。現在設定されているロケールを確認するSys.getlocale()については前節で触れた通りであるが、現在のシステムでの時間帯はSys.timezone()によって確認できる。\n設定したロケールをデフォルトに戻すには、下記のようにSys.setlocale()の引数localeに何も指定せずに実行することでこれまで行った変更が放棄される。\n引数categoryによってロケールのカテゴリーとして用意されている項目を個々に設定することも可能であるが、異なるロケールの混在はトラブルを引き起こしかねないため、locale引数による一括指定が便利である。"
  },
  {
    "objectID": "locale.html#日本語の取り扱い",
    "href": "locale.html#日本語の取り扱い",
    "title": "Appendix E — コラム: ロケール",
    "section": "\nE.1 日本語の取り扱い",
    "text": "E.1 日本語の取り扱い\n手元のデータが文字化けを起こしている場合、iconv()やstringrのstr_conv()関数を使って改めてエンコードを実行するという方法で対処可能である。\n\nx <- \"\\x82\\xa0\\x82\\xa2\\x82\\xa4\\x82\\xa6\\x82\\xa8\"\niconv(x, from = \"CP932\", to = \"UTF-8\")\n\n[1] \"あいうえお\"\n\n# str_conv()では元のエンコードを指定するだけで自動的に現在のエンコード形式に変更する\nstringr::str_conv(x, encoding = \"cp932\")\n\n[1] \"あいうえお\""
  },
  {
    "objectID": "magrittr.html",
    "href": "magrittr.html",
    "title": "Appendix A — コラム: magrittr",
    "section": "",
    "text": "# 逐次的に処理を実行する形式\ntmp.ex1 <- c(1, 2, NA, 4)\ntmp.ex1 <- mean(tmp.ex1, na.rm = TRUE)\nround(tmp.ex1, digits = 2)\n\n[1] 2.33\n\n# 関数の中に関数を記述する形式 (入れ子構造)\nround(mean(c(1, 2, NA, 4), na.rm = TRUE), digits = 2)\n\n[1] 2.33\n\n\n処理結果を保存する方式では、途中結果を別の場面で利用する際に便利であるが、多数のオブジェクトを作ることで混乱を招く可能性がある。対して入れ子構造での表記では1行で処理内容を表現しており、オブジェクトの混乱を防ぐことはできるが、処理が複雑になると、処理内容と元の値との関係がわかりにくくなってしまう。このような問題を解消するために発案されたのが、F#やコマンドラインで用いられるパイプ処理の仕組みを取り入れたmagrittrパッケージである。\n\n# magrittrパッケージを利用できるようにする\nlibrary(magrittr)\n\n# パイプ処理を行ったもの\nc(1, 2, NA, 4) %>% \n  mean(na.rm = TRUE) %>% \n  round(digits = 2)\n\n[1] 2.33"
  },
  {
    "objectID": "character-encoding.html",
    "href": "character-encoding.html",
    "title": "Appendix B — 文字コードとエンコーディング",
    "section": "",
    "text": "RではASCIIだけでなく多種多様な文字を出力するための仕組みとして、ASCIIの拡張であり、欧米圏での文字を範囲に含めたLatin1 (ISO 8859-1)および多言語文字に対応したUTF-8 (ISO/IEC 10646)といった文字集合規格を標準でサポートしている。\n一つの例を見てみよう。アクセント記号として用いられる「Ä」や「é」といった文字はASCIIではサポートされていないが、Latin1やUTF-8で扱われる文字となっている。そこで関数Encoding()によって文字コードのエンコード方法(エンコーディング)を確認してみる。\n\n(x <- \"\\U00C4\")\n\n[1] \"Ä\"\n\nEncoding(x)\n\n[1] \"UTF-8\"\n\n(x <- \"\\U00E9\")\n\n[1] \"é\"\n\nEncoding(x)\n\n[1] \"UTF-8\"\n\n\nまたEncoding()では文字列に対してエンコーディングを指定することも可能であり、latin1、UTF-8あるいはbytesというエンコードを直接与えることができる。byteはバイト表記との互換のために利用される。\n\n(x <- \"Ä\")\n\n[1] \"Ä\"\n\nEncoding(x) <- \"bytes\" # バイト表記のエンコードを指定する\n# エンコードした文字を出力。\n#   バックスラッシュ記号を表現するためにバックスラッシュが繰り返し出力される\nx\n\n[1] \"\\\\xc3\\\\x84\"\n\n# バイト表記から文字として出力\n(x <- \"\\xc3\\x84\")\n\n[1] \"Ä\"\n\nx <- \"あ\"\nEncoding(x) <- \"latin1\" # latin1としてエンコードする\nx\n\n[1] \"ã<81>‚\"\n\n# latin1では日本語の「あ」を表現するための文字コードが存在しない\n(x <- \"ã\\u0081\\u0082\")\n\n[1] \"ã\\u0081\\u0082\"\n\n\n\nB.0.0.1 エンコードの変換\nRで利用可能な文字コードはiconvlist()関数を用いて出力することができる。ここで出力される文字集合は、ファイル読み込みの際にエンコードを指定するものと共通のものである。これらの文字集合から、関数iconv()によって指定の文字集合に対応したエンコードを実行できる。\n\n# 文字集合規格の一部と数を出力\nx <- iconvlist()\nx[392:401]\n\n [1] \"UTF-16\"    \"UTF-16BE\"  \"UTF-16LE\"  \"UTF-32\"    \"UTF-32BE\"  \"UTF-32LE\" \n [7] \"UTF-7\"     \"UTF-8\"     \"UTF-8-MAC\" \"UTF8\"     \n\nlength(x)\n\n[1] 419\n\n# 引数fromとtoそれぞれに変換前後の文字コードを指定する\niconv(\"\\x82\\xa0\\x82\\xa2\\x82\\xa4\\x82\\xa6\\x82\\xa8\", from = \"CP932\", to = \"UTF-8\")\n\n[1] \"あいうえお\"\n\n\n上記の例で利用したCP932とは日本で使われる文字コードの一種であり、主にマイクロソフト社のコンピュータで利用されていたものであった。そのため、現在でもマイクロソフト社のOSを使用したコンピュータで作成されたファイルはCP932によるエンコードが行われていることがしばしばある。その他、日本語を取り扱うために整備されている文字コードの例を以下に示す。\n\n# 日本語に関係した主な文字コードのうちRで利用可能なものを検出する\nencode.jp <- c(\"EUC-JP\", \"ISO-2022-JP\", \"SJIS\", \"SHIFT_JIS\", \"CP932\", \"UTF8\")\nis.element(encode.jp, iconvlist())\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "regular-expression.html",
    "href": "regular-expression.html",
    "title": "Appendix C — コラム: 文字クラス・POSIX文字クラス",
    "section": "",
    "text": "文字クラス\n対象\n\n\n\n[0-9]\nアラビア数字\n\n\n[a-z]\n小文字アルファベット\n\n\n[A-Z]\n大文字アルファベット\n\n\n[ぁ-ん]\nひらがな\n\n\n[ァ-ヶ]\nカタカナ\n\n\n[一-龠]\n漢字\n\n\n[\\x01-\\x7E]\n1バイト文字\n\n\n\n\n(string <- c(\"ひらがな\", \"カタカナ\", \"漢字\", \"ABC\", \"abc\", \"123\"))\n\n[1] \"ひらがな\" \"カタカナ\" \"漢字\"     \"ABC\"      \"abc\"      \"123\"     \n\n# A, B, C に含まれるパターン\ngrep(\"[A-C]\", string, value = TRUE)\n\n[1] \"ABC\"\n\n# ひらがなが該当するパターン\ngrep(\"[ぁ-ん]\", string, value = TRUE)\n\n[1] \"ひらがな\"\n\n# アンカーを利用して、ABC 以外のパターンとマッチさせる\ngrep(\"[^ABC]\", string, value = TRUE)\n\n[1] \"ひらがな\" \"カタカナ\" \"漢字\"     \"abc\"      \"123\"     \n\n# 複数の文字クラスを扱うこともできる\ngrep(\"[ぁ-んァ-ヶ一-龠]\", string, value = TRUE)\n\n[1] \"ひらがな\" \"カタカナ\" \"漢字\"    \n\ngrep(\"[0-9]{2,4}\", x = c(10, 3), value = TRUE)\n\n[1] \"10\"\n\n\nPOSIX文字クラスは一致するパターンを特定の種類にまとめ、特殊な表記を行うことで複数の文字を一度に対象パターンとして取り扱う。POSIX文字クラスは文字クラス同様、ブラケットを利用した記述を利用するが、加えてコロン (:)によってPOSIX文字クラス名を囲むという特徴がある。Rで利用可能なPOSIX文字クラスについて表XXに示した。\n\n\n\nPOSIX文字クラス\n対象\n\n\n\n[:alnum:]\nアルファベットと数値([:alpha:] + [:digit:])\n\n\n[:alpha:]\n大小文字アルファベット([:upper:] + [:lower:])\n\n\n[:upper:]\n大文字アルファベット\n\n\n[:lower:]\n小文字アルファベット\n\n\n[:digit:]\n数値\n\n\n[:blank:]\n空白文字、スペースとタブ\n\n\n[:cntrl:]\n制御文字\n\n\n[:graph:]\n空白以外の文字 ([:alnum:] + [:punct:])\n\n\n[:print:]\n印字可能な文字([:graph:] + スペース)\n\n\n[:punct:]\n補助符号を含めた句読点(! ” # $ % & ’ ( ) * + , - . /)\n\n\n[:space:]\nすべての空白文字\n\n\n[:xdigit:]\n16進数で認められている文字(0-9a-fA-F)\n\n\n\n\nx <- c(\"alphabet\", \"123456\", \"alnum789\", \"123 456\")\ngrep(\"[[:alpha:]]\", x, value = TRUE)\n\n[1] \"alphabet\" \"alnum789\"\n\ngrep(\"[[:digit:]]\", x, value = TRUE)\n\n[1] \"123456\"   \"alnum789\" \"123 456\" \n\ngrep(\"[[:space:]]\", x, value = TRUE)\n\n[1] \"123 456\"\n\n# POSIX文字クラスでは字形セットやロケールを考慮したマッチを実行可能であり、\n# Aを示す全てのバリエーションとマッチする\ngrepl(\"[[:alpha:]]\", c(\"a\", \"Ａ\", \"â\", \"Ä\"))\n\n[1] TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "datetime-format.html",
    "href": "datetime-format.html",
    "title": "Appendix D — コラム: 日付・時間の書式",
    "section": "",
    "text": "表記\n表現\n表記の例\n\n\n\n%a\n曜日名の略称\nFri\n\n\n%A\n完全な曜日名\nFriday\n\n\n%b\n月名の略称\nDec\n\n\n%B\n完全な月名\nDecember\n\n\n%c\n日付と時間 (“%a %b %e %H:%M:%S %Y”)\nFri Dec 25 14:00:00 2015\n\n\n%y\n西暦の下２桁\n15\n\n\n%Y\n西暦\n2015\n\n\n%m\n月\n12\n\n\n%d\n日(１月の範囲)\n25\n\n\n%j\n日(１年の範囲)\n359\n\n\n%H\n時間(24時間)\n14\n\n\n%I\n時間(12時間)\n02\n\n\n%M\n分\n01\n\n\n%p\n午前と午後の区分\nPM\n\n\n%S\n秒\n30\n\n\n%w\n曜日\n5\n\n\n%x\n年月日 (“%m/%d/%y”)\n12/25/2015\n\n\n%X\n時刻 (“%H:%M:%S”)\n14:01:30\n\n\n\n\n# 書式を指定して日付・時間クラスオブジェクトの表現を変更する\n# これらはすべて文字列クラスオブジェクトに変換される点に注意する\nSys.Date()\n\n[1] \"2022-05-02\"\n\nformat(Sys.Date(), \"%Y年%m月%d日\")\n\n[1] \"2022年05月02日\"\n\nformat(Sys.time(), \"%x %X\")\n\n[1] \"05/02/2022 18:25:33\"\n\n\n\n# 文字列から書式を指定した日付・時間オブジェクトを作成\nstrptime(\"2017年2月25日 14:01:30\", format = \"%Y年%m月%d日 %X\")\n\n[1] \"2017-02-25 14:01:30 JST\""
  }
]